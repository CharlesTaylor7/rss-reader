<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:wfw="http://wellformedweb.org/CommentAPI/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Yossi Kreinin</title>
    <link>https://yosefk.com/blog</link>
    <description>Worse is better</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>Yossi Kreinin's ugly publishing software</generator>
    <image>
      <url>https://yosefk.com/blog/self.jpg</url>
      <title>Yossi Kreinin</title>
      <link>https://yosefk.com/blog</link>
      <width>144</width>
      <height>144</height>
    </image>
    <language>en</language>
    <lastBuildDate>Mon, 04 Nov 2024 22:00:01 +0000</lastBuildDate>
    <item>
      <title>People can read their manager's mind</title>
      <link>https://yosefk.com/blog/people-can-read-their-managers-mind.html</link>
      <description><![CDATA[<div class="right" style="text-align: right;">
<p>The fish rots from the head down.</p>
<p><em>– A beaten saying</em></p>
</div>
<p>People generally don't do what they're told, but what they expect to be rewarded for. Managers often say they'll reward
something – perhaps they even believe it. But then they proceed to reward different things.</p>
<p>I think people are fairly good at predicting this discrepancy. The more productive they are, the better they tend to be at
predicting it. Consequently, management's stated goals will tend to go unfulfilled whenever <em>deep down</em>, management
doesn't value the sort of work that goes into achieving these goals.</p>
<p>So not only is paying lip service to these goals worthless, but so is lying to oneself and <em>genuinely convincing</em>
oneself. When time comes to reward people, it is the gut feeling of whose work is truly remarkable that matters. And what you
usually convince yourself of is that the <em>goal</em> is important – but <em>not</em> that achieving it is remarkable. In fact,
often someone pursuing what you think are <em>unimportant</em> goals in a way that you admire will impress you more than someone
doing "important grunt work" (in your eyes.)</p>
<p>You then live happily with this compartmentalization – an important goal to be achieved by unremarkable people. However,
nobody is fooled except you. The people whose compensation depends on your opinion have ample time to remember and analyze your
past words and decisions – more time than you, in fact, and a stronger incentive. And so their mental model of you is often much
better than your own. So they ignore your requests and become valued, instead of following them and sinking into obscurity.</p>
<p>Examples:</p>
<ul>
<li>A manager truly appreciates original mathematical ideas. The manager requests to rid the code of crash-causing bugs, because
customers resent crashes. The most confident people ignore him and spend time coming up with original math. The less confident
people spend time chasing bugs, are upset by the lack of recognition, and eventually leave for greener pastures. At any given
moment, the code base is ridden by crash-causing bugs.</li>
<li>A manager enjoys "software architecture", design patterns, and language lawyer type of knowledge. The manager requests to
cooperate better with neighboring teams who are upset by missing functionality in the beautifully architected software. People
will tend to keep designing more patterns into the program.</li>
<li>A highly influential figure enjoys hacking on their machine. The influential figure points out the importance of solid,
highly-available infrastructure to support development. The department responsible for said infrastructure will guarantee that
<em>he</em> gets as much bandwidth, RAM, screen pixels and other goodies as they can supply, knowing that the infrastructure he
<em>really</em> cares about is that which enables the happy hacking on his machine. The rest of the org might well remain stuck
with a turd of an infrastructure.</li>
<li>A manager loathes spending money. The manager requires to build highly-available infrastructure to support development.
People responsible for infrastructure will build a piece of shit out of yesteryear's scraps purchased at nearby failing
companies for peanuts, knowing that they'll be rewarded.</li>
<li>A manager is all about timely delivery, and he did very little code maintenance in his life. The manager nominally realizes
that a lot of code is used in multiple shipping products; that it takes some time to make a change compatible with all the
client code; and that <em>branching the entire code base</em> is a quick way to do the work for this delivery, but you'll pay
for the shortcut many times over in each of your future deliveries. People will fork the code base for every shipping product.
(I've seen it and heard about it more times than the luckier readers would believe.)</li>
</ul>
<p>And so it goes. If something is rotten in an org, the root cause is a manager who doesn't value <em>the work</em> needed to
fix it. They <em>might</em> value it being fixed, but of course no sane employee gives a shit about <em>that</em>. A sane
employee cares whether <em>they</em> are valued. Three corollaries follow:</p>
<p><strong>Corollary 1.</strong> Who can, and sometimes does, un-rot the fish from the bottom? An <em>insane</em> employee.
Someone who finds the forks, crashes, etc. a personal offense, and will repeatedly risk annoying management by fighting to stop
these things. Especially someone who spends their own political capital, hard earned doing things management truly values, on
doing work they don't truly value – such a person can keep fighting for a long time. Some people manage to make a career out of
it by persisting until management truly changes their mind and rewards them. Whatever the odds of that, the average person
cannot comprehend the motivation of someone attempting such a feat.</p>
<p><strong>Corollary 2</strong>. When does the fish un-rot from the top? When a manager is taught by experience that (1)
neglecting this thing is harmful and (2) it's actually hard to get it right (that is, the manager himself, or someone he
considers smart, tried and failed.) But that takes managers admitting mistakes and learning from them. Such managers exist; to
be called one of them would exceed my dreams.</p>
<p><strong>Corollary 3.</strong> Managers who can't make themselves value <em>all</em> important work should at least realize
this: their goals do not automatically become their employees' goals. On the contrary, much or most of a manager's job is to
align these goals – and if it were that easy, perhaps they wouldn't pay managers that much, now would they? I find it a blessing
to be able to tell a manager, "you don't really value this work so it won't get done." In fact, it's a blessing even if they
ignore me. That they can hear this sort of thing without exploding means they can be reasoned with. To be considered such a
manger is the apex of my ambitions.</p>
<p>Finally, don't expect people to enlighten you and tell you what your blind spots are. Becoming a manager means losing the
privilege of being told what's what. It's a trap to think of oneself as just the same reasonable guy and why wouldn't they want
to talk to me. The right question is, why <em>would</em> they? Is the risk worth it for them? Only if they take your org's
problem very personally, which most people quite sensibly don't. Someone telling me what's what is a thing to thank for, but not
to count on.</p>
<p>The safe assumption is, they read your mind like an open book, and perhaps they read it out loud to each other – but not to
<em>you</em>. The only way to deal with the problems I cause is an honest journey into the depths of my own rotten mind.</p>
<p>P.S. As it often happens, I wanted to write this for years (the working title was "people know their true KPIs"), but I
didn't. I was prompted to finally write it by reading Dan Luu's excellent "<a href="http://danluu.com/wat/">How Completely
Messed Up Practices Become Normal</a>", where he says, among other things, "It’s sort of funny that this ends up being a problem
about incentives. As an industry, we spend a lot of time thinking about how to incentivize consumers into doing what we want.
But then we set up incentive systems that are generally agreed upon as incentivizing us to do the wrong things..." I guess this
is my take on the incentives issue – real incentives vs stated incentives; I believe people often break rules put in place to
achieve a stated goal in order to do the kind of work that is truly valued (<em>even regardless of whether that work's goal is
valued.</em>) It's funny how I effectively comment on Dan's blog two times in a row, his blog having become easily my favorite
"tech blog", while my own is kinda fading away as I spend my free time learning to animate.</p>
<p>&nbsp;</p>]]></description>
      <comments>https://yosefk.com/cgi-bin/comments.cgi?post=blog/people-can-read-their-managers-mind#comments</comments>
      <pubDate>Thu, 31 Dec 2015 00:00:00 +0000</pubDate>
      <dc:creator>Yossi Kreinin</dc:creator>
      <wfw:commentRss>https://yosefk.com/blog/people-can-read-their-managers-mind.feed</wfw:commentRss>
    </item>
    <item>
      <title>The overblown frequency vs cost efficiency trade-off</title>
      <link>https://yosefk.com/blog/the-overblown-frequency-vs-cost-efficiency-trade-off.html</link>
      <description><![CDATA[<p>I've often&nbsp;read arguments that&nbsp;computing circuitry running at a high frequency is inefficient, power-wise or silicon
area-wise or both. So roughly, 100 MHz is more efficient, in that you get more&nbsp;work done per unit of energy or&nbsp;area spent. And
CPUs go for 1 GHz or 3 GHz because&nbsp;serial performance sells regardless of efficiency. But accelerators like GPUs or embedded
DSPs or ISPs or&nbsp;codecs implemented in hardware etc. etc. – these don't need to run at a high frequency.</p>
<p>And I think this argument is less common now when say GPUs have caught up, and an embedded GPU might run at the same
frequency as an&nbsp;embedded CPU. But still, I've&nbsp;just seen someone peddling a "neuromorphic chip" or some such, and there it was –
"you need to run conventional machines at 1 GHz and it's terribly inefficient."</p>
<p>AFAIK the real story here is pretty simple, namely:</p>
<ol>
<li>As you increase frequency, you GAIN efficiency up to point;</li>
<li>From that point on,&nbsp;you do start LOSING&nbsp;efficiency;</li>
<li>That inflection point, <em>for well-designed circuits</em>, is much higher than people think (close to a CPU's frequency in
the given manufacturing process, certainly not 10x less as people often claim);</li>
<li>...and what fueled the myth is, accelerator makers&nbsp;used to be&nbsp;much worse at designing for high frequency than CPU makers. So
marketeers together with "underdog sympathizers" have overblown the frequency vs efficiency trade-off completely out of
proportions.</li>
</ol>
<p>And below I'll detail these points;&nbsp;if you notice oversimplifications, please correct me (there are many conflicting goals in
circuit implementation, and these goals are different across markets, so my experience might be too narrow.)</p>
<h2 id="frequency-improves-efficiency-up-to-a-point">Frequency improves efficiency up to a point</h2>
<p>What's the cost of a&nbsp;circuit, and how is it affected by frequency? (This section shows the happy part of the answer – the sad
part is in the next section.)</p>
<ol>
<li>Silicon area.&nbsp;The higher the clock frequency, the more things the same circuit occupying this area does per unit of time –
so you <strong><em>win</em></strong>!</li>
<li>Leakage power – just powering up the circuit and doing nothing, not even toggling the clock signal, costs you&nbsp;a certain
amount of energy per unit of time. Here again, the higher the&nbsp;frequency, the more work gets done in exchange for the same
leakage power – again you <strong><em>win</em></strong>!</li>
<li>Switching power – every time&nbsp;the clock signal changes its value from 0 to 1 and back, this triggers a bunch of changes to
the values of other signals as dictated by the interconnection of the logic gates, flip-flops – everything making up the
circuit. All this&nbsp;switching from 0 to 1 and back costs energy (and NOT switching does not; measure the power dissipated by a
loop&nbsp;multiplying zeros vs a loop multiplying random data, and you'll see what I mean. This has implications for&nbsp;the role of
software in&nbsp;conserving energy, but this is outside our scope here.) What's the impact of frequency on cost here? It turns out
that&nbsp;frequency is <strong><em>neutral</em></strong> - the cost in energy is directly proportionate to the clock frequency, but
so is the amount of work done.</li>
</ol>
<p>Overall, higher frequency means spending less&nbsp;area and power per unit of work – the opposite of the peanut gallery's
conventional wisdom.</p>
<h2 id="frequency-degrades-efficiency-from-some-point">Frequency degrades efficiency from some point</h2>
<p>At some point, however, higher frequency does start to&nbsp;increase the cost of the circuit per unit of work.&nbsp;The reasons boil
down to&nbsp;having to build your circuit out of physically larger elements that leak more power. Even further down
the&nbsp;frequency-chasing path come other problems, such as having to break down your work to many more pipeline stages, spending
area and power on&nbsp;storage for the intermediate results of these stages; and needing expensive cooling solutions for heat
dissipation.&nbsp;So actually there are several points along the road, with&nbsp;the&nbsp;cost of extra MHz growing at each point – until you
reach the physically impossible frequency for a given manufacturing process.</p>
<p>How do you find the&nbsp;point where an&nbsp;extra MHz isn't worth it?&nbsp;For synthesizable design (one created in a high-level language
like Verilog and VHDL), you can synthesize it for different frequencies and you can measure the cost in area and power, and plot
the results.&nbsp;My confidence&nbsp;of where I think the inflection&nbsp;point should be comes from looking at these plots. Of course the plot
will depend on the design, bringing us to the next point.</p>
<h2 id="better-designed-circuits-optimal-frequency-is-higher">Better-designed circuits' optimal frequency is higher</h2>
<p>One hard part of circuit design is,&nbsp;you're basically making a&nbsp;hugely parallel&nbsp;system, where&nbsp;many parts do different things.
Each part doing the same thing would be&nbsp;easy – they all take the same time, duh, so no bottleneck. Conversely, each part doing
something else makes it really easy to create a bottleneck – and really hard to balance the parts (it's hard to tell exactly how
much time&nbsp;a&nbsp;piece of work takes&nbsp;without trying, and there are a lot of options you could try, each breaking the work into
different parts.)</p>
<p>You need to break the harder things into smaller pipeline stages (yes, a cost in itself as we've just said – but usually&nbsp;a
small cost unless you&nbsp;target really high frequencies and so have to break everything into umpteen stages.) Pipelining is hard to
get right when the pipeline stages are not truly independent, and people often recoil from&nbsp;it (a hardware bug is on average more
likely to be catastrophically costly&nbsp;than somewhat crummier performance.)&nbsp;Simpler designs also shorten schedules, which may be
better&nbsp;than reaching a higher frequency&nbsp;later.</p>
<p>So CPUs competing for a huge market on serial performance and (stupidly) advertised frequency, implementing a comparatively
stable instruction set, justified the effort to overcome these obstacles. (Sometimes to the detriment of consumers, arguably, as
say with Pentium 4 – namely, high frequency, low serial performance&nbsp;due to too much pipelining.)</p>
<p>Accelerators are different. You can to some extent compensate for poor serial performance by throwing money at the problem
-&nbsp;add more cores. Sometimes you don't care about extra performance – if you can decode video at the peak required rate and
resolution, extra performance might not&nbsp;win more business.&nbsp;Between frequency improvements and architecture
improvements/implementing a huge&nbsp;new standard, the latter might be more worthwhile. And then the budgets are generally smaller,
so you tend to design more conservatively.</p>
<p>So AFAIK this is why so many embedded accelerators had crummy frequencies when they started out (and they also had apologists
explaining why it was a good thing). And that's why some of the accelerators&nbsp;caught up – basically&nbsp;it was never a technical
limitation but an economic problem of where to spend effort, and changing circumstances caused effort to be invested into
improving frequency. And that's why if you're making&nbsp;an accelerator core which is 3 times slower than the CPU in the same chip,
my first guess is your design isn't stellar at this stage, though it might improve – if it ever has to.</p>
<p><strong>P.S.</strong> I'll say it again – my perspective can be skewed; someone with different experience might point out
some oversimplifications. Different process nodes and different implementation&nbsp;constraints mean that what's decisive in one's
experience is of marginal importance in another's experience. So please do correct me if I'm wrong in your experience.</p>
<p><strong>P.P.S.</strong> Theoretically, a design running at 1 GHz might be doing the exact same amount of work as a 2 GHz
design – if the pipeline is 2x shorter and each stage&nbsp;in the 1 GHz thing does the work of 2 stages in the 2 GHz thing. In
practice, the 1 GHz design will have stages doing less work, so they complete in less than 1 nanosecond (1/1GHz) and are idle
during much of the cycle. And this is why you want to invest some effort to up the frequency in that design – to not have
mostly-idle circuitry leaking power and using up area. But the theoretically possible perfectly balanced 1 GHz design is a valid
counter-argument to all of the above, I just don't think that's what most crummy frequencies hide behind them.</p>
<p><strong>Update:</strong>&nbsp;here's an interesting complication – Norman Yarvin's comment points to an article about
near-threshold voltage research by Intel, from which it turns out that a Pentium implementation designed to operate at
near-threshold voltage (at&nbsp;a near-2x cost in area) achieves its best energy efficiency at 100 MHz – 10x slower than its peak
frequency but spending 47x less energy. The trouble is, if you want&nbsp;that 10x performance back, you'd need 10 such cores for an
overall area increase of&nbsp;20x,&nbsp;in return for overall energy savings of 4.7x. Other points on&nbsp;the graph will be less extreme (less
area&nbsp;spent, less&nbsp;energy saved.)</p>
<p>So this makes sense when&nbsp;silicon area is tremendously cheaper than energy, or when there's a hard limit on how much energy
you can spend but a&nbsp;much laxer limit on area. This is not the case most of the time, AFAIK (silicon costs a lot and then it
simply takes physical space, which also costs), but it can be the case some of the time. NTV can also make sense if voltage is
adjusted dynamically based on workload, and you don't need high performance most of the time, and you don't care that your peak
performance is achieved at a 2x area cost as much as you're happy to be able to conserve energy tremendously when not needing
the performance.</p>
<p>Anyway, it goes to show that&nbsp;it's more complicated than I stated, even if I'm right for the average design made under today's
typical constraints.</p>]]></description>
      <comments>https://yosefk.com/cgi-bin/comments.cgi?post=blog/the-overblown-frequency-vs-cost-efficiency-trade-off#comments</comments>
      <pubDate>Sun, 31 Jan 2016 00:00:00 +0000</pubDate>
      <dc:creator>Yossi Kreinin</dc:creator>
      <wfw:commentRss>https://yosefk.com/blog/the-overblown-frequency-vs-cost-efficiency-trade-off.feed</wfw:commentRss>
    </item>
    <item>
      <title>Love thy coworker; thy work, not necessarily</title>
      <link>https://yosefk.com/blog/love-thy-coworker-thy-work-not-necessarily.html</link>
      <description><![CDATA[<p>The whole "passionate about work" attitude&nbsp;irks me out; save&nbsp;your passion for the bedroom.&nbsp;This is not to say that I'd rather
be ridiculed&nbsp;for being a nerd who works too hard.</p>
<p>In fact, personally I'm in a strange position of a certified programming nerd with a blog and code on github, who nonetheless
does it strictly for the money (I'd be&nbsp;a lawyer or an i-banker if I thought I could.) I'm thus on&nbsp;both sides of this, kinda.</p>
<p>So, in today's&nbsp;quest to change society through blogging, what am I asking society for, if neither passion nor scorn for work
please me? Well, I'd rather society neither&nbsp;encourage nor discourage&nbsp;the love of work, and leave it to the individual's
discretion.</p>
<p>From a&nbsp;moral angle, I base my belief on&nbsp;the Biblical commandment, "love thy neighbor", which I think does not dovetail into
"love thy work" for a reason. From a practical angle, again I think&nbsp;that one's&nbsp;attitude to <em>coworkers</em>
(also&nbsp;managers,&nbsp;customers and other <em>people</em>) is a&nbsp;better predictor of productivity than one's&nbsp;attitude to
<em>work.</em></p>
<p>People talk a lot about intrinsic vs extrinsic motivation – passion vs money -&nbsp;but&nbsp;I think they're actually very similar, and
<strong>the more important distinction is personal vs social motivation</strong>.</p>
<p>Why? Because whether I work for fun or&nbsp;for the money, it's a means to my own&nbsp;personal end, which&nbsp;in itself precludes neither
negligence nor fraud on my behalf. <strong>What makes you&nbsp;do the bits of work that are neither fun nor strictly necessary to get
paid is that other people need it done, and you&nbsp;don't want to fail them.</strong></p>
<p>Perhaps&nbsp;you disagree with&nbsp;my ideas on&nbsp;motivation. If so, here's an idea on boundaries that I hope is uncontroversial. Telling
me how I should feel&nbsp;about&nbsp;my job infringes on my boundaries, which is to say that it's none of your business. If however I do a
shoddy job and&nbsp;it becomes your problem, then <em>I'm</em> infringing on <em>your</em> boundaries, so&nbsp;you're absolutely entitled
to complain about it. Here again requiring respect for coworkers is sensible, while requiring this or that attitude towards&nbsp;the
work itself is not.</p>
<p>So:</p>
<ul>
<li>Someone's attitude towards work does not predict the quality of their work</li>
<li>Inquiring reports and potential hires about their attitude towards work is a minor but unpleasant&nbsp;harassment</li>
<li>A corporate culture of "we're doing this thing together" beats&nbsp;both "we're passionate to change the world by&nbsp;advancing the
state of the art&nbsp;in blah blah" and "we're laser-focused on fulfilling&nbsp;customers' requirements on time and within budget"</li>
</ul>
<p>P.S. Which kind of culture do managers typically want? Often they're&nbsp;schizophrenic on this.&nbsp;They want "passionate" workers,
hoping that they'll accept less money. On the other hand, the same person often&nbsp;doesn't care about the actual work in the worst
way (he sucks at it and not having to do it anymore is management's biggest perk to him.) But what he cares about is&nbsp;deadlines,
etc.&nbsp;- so he encourages a culture of shipping shit in the hope that it&nbsp;sorts itself out somehow (these are the people that the
term "technical debt" was&nbsp;invented&nbsp;for, of course nobody is convinced by this pseudo-businessy term if they weren't already
convinced about the underlying idea of "shipping shit is bad.") Of course a truly passionate worker is going to suffer mightily
in the kind of culture created by the same manager who thinks he wanted this worker.</p>]]></description>
      <comments>https://yosefk.com/cgi-bin/comments.cgi?post=blog/love-thy-coworker-thy-work-not-necessarily#comments</comments>
      <pubDate>Fri, 13 May 2016 00:00:00 +0000</pubDate>
      <dc:creator>Yossi Kreinin</dc:creator>
      <wfw:commentRss>https://yosefk.com/blog/love-thy-coworker-thy-work-not-necessarily.feed</wfw:commentRss>
    </item>
    <item>
      <title>Looking for a functional safety/ISO 26262 expert (anywhere on the globe)</title>
      <link>https://yosefk.com/blog/looking-for-a-functional-safetyiso-26262-expert-anywhere-on-the-globe.html</link>
      <description><![CDATA[<p>Unlike most&nbsp;positions&nbsp;mentioned here, this one includes the possibility of working remotely (certainly from Europe and I
think&nbsp;from elsewhere, too), with occasional visits to Jerusalem.</p>
<p>Functional safety experts with automotive experience are generally rare and in demand, meaning that</p>
<ul>
<li>they're probably gainfully employed, and</li>
<li>I'm not counting on one of them reading this blog.</li>
</ul>
<p>However,&nbsp;I imagine that a friend of a safety expert&nbsp;might be among my readers. If you're that reader, you can tell your
friend the&nbsp;safety expert that we're very eager to hire them, and will go a long way to make an attractive proposition.</p>
<p>We particularly value experience&nbsp;at the chip/ASIC side of things (translating ISO 26262 requirements to actionable
guidelines&nbsp;on hardening the design in question, together with a&nbsp;verification methodology and the required safety documentation.)
We also value recommendations from designers who had to&nbsp;follow the expert's guidelines, as well as experience in&nbsp;presenting
safety cases to customers.</p>]]></description>
      <comments>https://yosefk.com/cgi-bin/comments.cgi?post=blog/looking-for-a-functional-safetyiso-26262-expert-anywhere-on-the-globe#comments</comments>
      <pubDate>Thu, 19 May 2016 00:00:00 +0000</pubDate>
      <dc:creator>Yossi Kreinin</dc:creator>
      <wfw:commentRss>https://yosefk.com/blog/looking-for-a-functional-safetyiso-26262-expert-anywhere-on-the-globe.feed</wfw:commentRss>
    </item>
    <item>
      <title>Evil tip: avoid "easy" things</title>
      <link>https://yosefk.com/blog/evil-tip-avoid-easy-things.html</link>
      <description><![CDATA[<div class="right" style="text-align: right;">
<p>Now you see that evil will always triumph, because good is dumb.</p>
<p><em>– <a href="http://tvtropes.org/pmwiki/pmwiki.php/Main/GoodIsDumb">Dark Helmet</a></em></p>
<p>Evildoers&nbsp;live longer and feel better.</p>
<p><em>– <a href="https://twitter.com/YossiKreinin/status/737940963797180416">Myself</a></em></p>
</div>
<p>My&nbsp;writing has recently prompted an anonymous commenter to declare that people like&nbsp;me are&nbsp;what's wrong with the world. Oh
joy! – finally, after all these years of doing evil, some&nbsp;recognition!&nbsp;Excited, I decided to share one of my battle-tested
evil&nbsp;tips,&nbsp;which&nbsp;never ever failed evil me.</p>
<h2 id="dont-work-on-easy-things">Don't work on "easy" things</h2>
<p>An easy thing is&nbsp;a heads they win, tails you lose situation.&nbsp;Failing at easy things is shameful; succeeding is
unremarkable.&nbsp;Much better to work on hard things – heads <em>you</em> win, tails <em>they</em> lose. Failure is unfortunate but
expected; success makes you&nbsp;a hero.</p>
<p><strong>Treat this seriously, because it snowballs</strong>. The guy working on the "hard" thing gets a lot of help and
resources, making it <em>easier</em> to succeed – and easier to move on to the next "hard" thing. The guy doing&nbsp;the "easy"
tasks&nbsp;gets no help, so it's&nbsp;<em>harder</em>&nbsp;to succeed.</p>
<p>Quotation marks all over the place, because of&nbsp;course what counts is perception, not how hard or easy it&nbsp;really is. The worst
thing to work on is the hard one that's&nbsp;<em>perceived</em> as easy – the hard "easy" thing. The&nbsp;best thing is the easy "hard"
one.&nbsp;My years-long <a href="https://yosefk.com/blog/low-level-is-easy.html">preference for low-level programming</a>&nbsp;results, in
part, from its reputation of&nbsp;a very hard field, when in practice, it takes a little knowledge and a lot of discipline – but not
any outstanding skills.</p>
<p>(Why then do many&nbsp;people fear&nbsp;low-level programming? Only because of how hard it bites you the first few times. People who
felt that&nbsp;pain and recoiled respect those who've moved past it&nbsp;and reached&nbsp;productivity. Now you know why people take shit from
the likes of&nbsp;Ken Thompson and Linus Torvalds, and then&nbsp;beg&nbsp;for more.)</p>
<p>The point where&nbsp;this gets really evil is <em>not</em> when a heroic doer of hard things&nbsp;decides to behave like a Torvalds.
That's more stupid than evil. You'll&nbsp;get away&nbsp;with being a Torvalds, but it always&nbsp;costs some&nbsp;goodwill and hence, ultimately,
money. So&nbsp;the goal-oriented&nbsp;evildoer always tries to be on their best behavior.</p>
<p>No, the point where this gets really evil is when you let them fail. When they come to you <em>thinking</em> that it's easy,
and you know it's actually <em>very hard</em>, and you turn them down, and you let them fail a few times, and you wait until
they come back with a readjusted attitude -&nbsp;<em>that's</em> evil.</p>
<p>Here,&nbsp;the&nbsp;evildoer needs to strike a delicate balance, keeping in mind The Evildoer's Golden Rule:</p>
<ul>
<li><em>You</em> can only sustain that much&nbsp;do-gooding;&nbsp;however,</li>
<li><em>Your&nbsp;environment</em> can only take that much evildoing, and you need your environment.</li>
</ul>
<p>Here's the rule applied to our situation:</p>
<ul>
<li>Working on the hard "easy" thing – all trouble, no credit – is&nbsp;going to be <em>terrible</em> for you. You'll&nbsp;get a taste of
a do-gooder's short, miserable life.</li>
<li>However, if this thing is so important&nbsp;that a&nbsp;failure would endanger the org, <em>maybe</em> you should be the do-gooder and
save them from their misconceptions&nbsp;at your own expense. <em>Maybe</em>. And maybe <em>not.</em>&nbsp;Be sure to think about it.</li>
</ul>
<p>The upshot is, sometimes the&nbsp;evildoer&nbsp;gets to be the do-gooder, but&nbsp;you should know that it's hazardous to your health.</p>
<h2 id="making-easy-things-into-hard-ones-the-postponing-gambit">Making easy things into hard ones: the postponing gambit</h2>
<p>Sometimes you can't weasel out of doing something "easy." An interesting gambit for these cases is to postpone the easy thing
until it becomes urgent. This accomplishes two&nbsp;things at a time:</p>
<ul>
<li><strong>Urgent things automatically become harder</strong>, a person in a hurry more important. The later it's
done,&nbsp;the&nbsp;easier it is to get help (while retaining the status of "the" hero in the center of it all who made it happen.)</li>
<li><strong>Under time pressure, the scope shrinks</strong>, making the formerly "easy" and now officially "hard" thing
genuinely easier. This is particularly useful for&nbsp;the really disgusting, but unavoidable work.</li>
</ul>
<p>But it is a gambit, because postponing things until they become urgent is <em>openly</em> evil. (Avoiding easy things is
<em>not</em> – why, it's patriotic and heroic to look for the harder work!) To get away with postponing, you need an excuse:</p>
<ul>
<li>other supposedly urgent work;</li>
<li>whoever needing this thing not having reminded&nbsp;you;</li>
<li>or even you having sincerely underestimated the difficulty and hence, regrettably, having postponed it too much – you're
<em>so</em> sorry. (This last excuse has the drawback of you having to admit&nbsp;an error. But to the extent that urgency will make
the scope&nbsp;smaller, <em>the error will become smaller, too.</em>)</li>
</ul>
<p>One thing you want to prevent is people learning to remind you earlier. The way to accomplish it is being very nice when they
come late. If people feel punished for reminding too late, they'll come earlier next time, and in a vengeful mood, so with more
needless tasks.&nbsp;But if they're late and you eagerly "try to do the best under the circumstances", not only&nbsp;do you put yourself
under the spotlight as a patriotic hero, you move the forgetful culprit <em>out</em> of the spotlight. So they'll form a rosy
memory of the incident, and <em>not</em> learn the value of coming earlier – precisely what we want.</p>
<p>One thing making the postponing&nbsp;gambit relatively safe is that management is shocked by the very thought of people playing
it, as can be seen in the following real-life conversation:</p>
<blockquote>
<p><strong>Babbling management consultant:</strong> A lot of organizations have a problem where they only work on urgent things
at the expense of important, but less urgent ones.</p>
<p><strong>Low-ranking evildoer manager (in a momentary&nbsp;lapse of reason):</strong> Why, of course! I actually postpone things to
get priority around here.</p>
<p><strong>Higher-ranking manager (in disbelief):&nbsp;</strong>You aren't serious, of course.</p>
<p><strong>Low-ranking evildoer (apparently still out to lunch):</strong>&nbsp;I am.</p>
<p><strong>Higher-ranking manager (firmly):</strong>&nbsp;I know you aren't.</p>
<p><strong>Low-ranking evildoer</strong> finally shuts his mouth.</p>
</blockquote>
<p>See? Sometimes they won't believe it if you say it to their face. So they're unlikely to&nbsp;suspect you. (Do people reporting to
me play the postponing gambit? Sometimes they do, and I don't resent them for it; <a href="people-can-read-their-managers-mind.html">their priorities aren't mine</a>. But at the worst case, you should expect <em>a
lot</em> of resentment – it's practically high treason – so&nbsp;you should have plausible deniability.)</p>
<h2 id="conclusion">Conclusion</h2>
<p>To a very large extent, <a href="10x-more-selective.html">your productivity is a result of what you choose to work on</a>.
Keep things perceived as easy out of that list. When you can't,&nbsp;postponing an "easy"&nbsp;thing can make it both "harder" and
smaller.</p>
<p>Happy evildoing!</p>
<p>&nbsp;</p>]]></description>
      <comments>https://yosefk.com/cgi-bin/comments.cgi?post=blog/evil-tip-avoid-easy-things#comments</comments>
      <pubDate>Wed, 01 Jun 2016 00:00:00 +0000</pubDate>
      <dc:creator>Yossi Kreinin</dc:creator>
      <wfw:commentRss>https://yosefk.com/blog/evil-tip-avoid-easy-things.feed</wfw:commentRss>
    </item>
    <item>
      <title>A layman's view of the economy</title>
      <link>https://yosefk.com/blog/a-laymans-view-of-the-economy.html</link>
      <description><![CDATA[<p>First of all, I proudly present a 2-minute short that I animated!</p>
<div class="video_16_9" style="position: relative;overflow: hidden;width: 100%;padding-top: 56.25%;">
<iframe class="responsive_iframe" allowfullscreen="allowfullscreen" src="https://player.vimeo.com/video/171368757" style="position: absolute;border: 0;top: 0;left: 0;bottom: 0;right: 0;width: 100%;height: 100%;">
</iframe>
</div>
<p>...And the same thing on YouTube, in case one&nbsp;loads better&nbsp;than the other:</p>
<div class="video_16_9" style="position: relative;overflow: hidden;width: 100%;padding-top: 56.25%;">
<iframe class="responsive_iframe" allowfullscreen="allowfullscreen" src="//www.youtube.com/embed/c-cOrPOHi7E" style="position: absolute;border: 0;top: 0;left: 0;bottom: 0;right: 0;width: 100%;height: 100%;">
</iframe>
</div>
<p>One thing I learned making the film is that&nbsp;my&nbsp;Russian accent colors not only my words, but any noise coming out of my mouth.
So I'm not the most&nbsp;versatile voice actor.</p>
<p>Anyway, we certainly have a <a href="https://en.wikipedia.org/wiki/Debt_crisis">debt crisis</a>, and easy credit policies
keep&nbsp;producing still&nbsp;more debt. I don't think interest rates have ever stayed so low for so long, everywhere.</p>
<p>Economists argue both for and against debt expansion [1], as they argue&nbsp;about&nbsp;everything.</p>
<p>My own take is as simple as my sparse knowledge ought to make it:</p>
<ul>
<li>Unprecedented conditions produce unprecedented outcomes.</li>
<li>Booms are usually&nbsp;gradual, and busts&nbsp;are sudden.</li>
</ul>
<p>No unusual boom has gradually arisen&nbsp;from unusual monetary policy, and it's been a while. <strong>But something unusual ought
to happen in unusual conditions!</strong> Thus one expects a sudden, unusual bust&nbsp;down the road.</p>
<p>That's it. It's&nbsp;like a physicist's&nbsp;proof [2] that one's attractiveness peaks at some distance from the observer. At the
distances of zero and infinity, visual attractiveness is zero (you can't see anything.) Therefore, attractiveness as a function
of distance has a maximum&nbsp;<em>somewhere</em> in between. True, kinda, and it didn't take a lot of insight into the nature of
attractiveness – much like my peak debt proof doesn't require an&nbsp;understanding of the economy [3].</p>
<p>Will&nbsp;today's "Brexit" trigger&nbsp;the global&nbsp;downturn predicted by Yossi Kreinin's&nbsp;Rule of Unprecedented Conditions? Probably&nbsp;not
by itself. I think it's a symptom more than a cause [4], and&nbsp;the big bad thing comes&nbsp;later.</p>
<p>In the meanwhile, here's to hoping that my little film (started when "Grexit" was a thing, completed just in time for Brexit)
was funnier than the average <a href="https://www.reddit.com/r/forwardsfromgrandma/">forward from grandma</a>&nbsp;[5].</p>
<p>Happy Brexit! And if you follow people on Twitter, there's a strong case for <a href="https://twitter.com/YossiKreinin">following me</a> as well.</p>
<p>[1] Bibliography:&nbsp;<a href="http://www.nytimes.com/2015/02/09/opinion/paul-krugman-nobody-understands-debt.html">Nobody
Understands Debt</a>&nbsp;except Krugman;&nbsp;<a href="https://www.ced.org/blog/entry/does-krugman-understand-debt">Does Krugman
Understand Debt?</a></p>
<p>[2] I&nbsp;think a&nbsp;particular famous&nbsp;physicist said it, but I forget who.</p>
<p>[3] ...and I can't say I&nbsp;have any understanding of the economy. That said, I've owed and paid off a lot of debt, and got to
negotiate with many bankers. And I can tell you that "debt is money we owe to ourselves", Krugman's catchphrase, feels
unconvincing to&nbsp;creditors – as&nbsp;many people and whole nations have&nbsp;found out.</p>
<p>[4] In fact, I just got an email from&nbsp;an&nbsp;asset manager saying that&nbsp;it's good <em>for the UK</em> in the&nbsp;longer run, elevating
Brexit from a symptom to a cure. But he didn't say "good <em>for everyone</em>", and then I'm not sure his crystal ball is
better than yours or mine.</p>
<p>[5] I linked to /r/forwardsfromgrandma since,&nbsp;regardless of the&nbsp;politics of either its members or their grandmas, I ought to
give credit for&nbsp;the brilliant term – it's definitely funny because it's true. I've watched many relatives acquire the habit
of&nbsp;forwarding&nbsp;various wingnut stuff&nbsp;as they age. Most&nbsp;frighteningly, my own urge to email such things&nbsp;gets harder to resist
every year. I can sense&nbsp;my own ongoing grandmafication; between you and me, an animated short about debt might be a part of
"it." Scary, scary stuff.</p>]]></description>
      <comments>https://yosefk.com/cgi-bin/comments.cgi?post=blog/a-laymans-view-of-the-economy#comments</comments>
      <pubDate>Fri, 24 Jun 2016 00:00:00 +0000</pubDate>
      <dc:creator>Yossi Kreinin</dc:creator>
      <wfw:commentRss>https://yosefk.com/blog/a-laymans-view-of-the-economy.feed</wfw:commentRss>
    </item>
    <item>
      <title>Looking for senior IT/DevOps people</title>
      <link>https://yosefk.com/blog/looking-for-senior-itdevops-people.html</link>
      <description><![CDATA[<p>I wouldn't spam you with these job offers if didn't work :-) So, we're looking for senior IT people to work at our Jerusalem
offices – <strong>managers and hands-on people alike</strong>.&nbsp;We have&nbsp;rapid growth, "Big Data" (it definitely <a href="https://twitter.com/devops_borat/status/288698056470315008?lang=en">is crash Excel</a>&nbsp;-&nbsp;in fact,&nbsp;at one point it was
close to physically crashing through the floor&nbsp;due to the storage servers'&nbsp;weight, but luckily that's been handled), "HPC"
(biggish server farms, distributed build &amp; tests, etc.),&nbsp;and many other buzzwords [1].&nbsp;I don't know where IT ends and DevOps
starts but I guess a good candidate could have&nbsp;either in their CV, so there.</p>
<p>If you have qualified friends looking for a&nbsp;challenging, well-paying job at a fun place, send their CVs, the sooner the
better – we're in a hurry (rapid growth!), so early birds are&nbsp;more likely to get the can of worms. As always, "challenging" is a
downside as much as an&nbsp;upside&nbsp;(a place where IT means Exchange, SAP and little else might pay very well for a more predictable
and less demanding job.)</p>
<p>We value experience in building and maintaining non-trivial systems, and technical reasoning (X happens because of Y, Z is
most&nbsp;efficient if you use it to do W, etc.) We also value&nbsp;experience in higher-level areas such as management and purchasing,
and business reasoning (don't hook X and&nbsp;Y together since their vendors compete&nbsp;and will sabotage the project,&nbsp;Z beats W in
terms of total cost of ownership, etc.) We do kinda lean towards thinking of technical&nbsp;aptitude as a cornerstone on top of which
solid&nbsp;higher-level expertise is built. (We've seen managers snowed by vendors, reports, etc., which&nbsp;is a perennial problem in
tech at large and isn't restricted to IT.)</p>
<p>If you'd like to hear more details, please email Yossi.Kreinin@gmail.com</p>
<p>[1] what we don't have is a heavy-duty web site/application, which might make the position less relevant for some.</p>]]></description>
      <comments>https://yosefk.com/cgi-bin/comments.cgi?post=blog/looking-for-senior-itdevops-people#comments</comments>
      <pubDate>Thu, 30 Jun 2016 00:00:00 +0000</pubDate>
      <dc:creator>Yossi Kreinin</dc:creator>
      <wfw:commentRss>https://yosefk.com/blog/looking-for-senior-itdevops-people.feed</wfw:commentRss>
    </item>
    <item>
      <title>The habitat of hardware bugs</title>
      <link>https://yosefk.com/blog/the-habitat-of-hardware-bugs.html</link>
      <description><![CDATA[<p>The&nbsp;Moscow apartment which little me called home was also home to many other creatures, from smallish cockroaches to biggish
rats. But of course we rarely met&nbsp;them face to face. Evolution has weeded out those animals&nbsp;imprudent enough to crash
your&nbsp;dinner.&nbsp;However, when we moved a cupboard one time, we had the pleasure to meet a few hundreds of fabulously&nbsp;evolved
cockroaches.</p>
<p>In this sense, logical bugs aren't different from actual insects. You won't find bugs&nbsp;under the spotlight, because they get
fixed under the spotlight, crushed like a cockroach on the dinner table.&nbsp;But in darker nooks and crannies, bugs&nbsp;thrive and
multiply.</p>
<p>When hardware malfunctions in a single, specific way, software running on it usually&nbsp;fails in several different, seemingly
random ways, so it sucks to debug it. Homing in on the cause is easier if you can guess&nbsp;which parts of the system are more
likely to be buggy.</p>
<p>When hardware fails, nobody wants a programmer treating it&nbsp;as a lawyer or a mathematician (the&nbsp;hardware broke the contract!
only working hardware&nbsp;lets us&nbsp;reason about software!) Instead, the key to success is approaching it as&nbsp;a pragmatic&nbsp;entomologist
knowing where bugs live.</p>
<p>Note that I'm mostly talking about design bugs, not random&nbsp;manufacturing defects. Manufacturing defects can occur absolutely
anywhere. If you're in an industry where you can't toss a faulty&nbsp;unit into the garbage can, but instead must find the specific
manufacturing defect in every reported bad&nbsp;unit, I probably can't tell you anything new, but I can offer you my deepest
sympathy.</p>
<h2 id="cpus">CPUs</h2>
<p>CPUs are the perfect illustration of the "spotlight vs nooks and crannies" principle. In CPUs, the spotlight, where it's hard
to find&nbsp;bugs, is functionality accessible to userspace programs - data processing,&nbsp;memory access and control
flow&nbsp;instructions.</p>
<p>Bugs are more likely&nbsp;in those parts of the CPUs only accessible to operating systems and drivers - and used more by OS
kernels&nbsp;than drivers. Stuff like memory protection, interrupt handling,&nbsp;and other privileged instructions. You can sell a buggy
CPU if it doesn't break too many commercially significant, hard to patch programs - and there aren't many important OS kernels,
therefore a lot of scenarios are never triggered&nbsp;by them.</p>
<p>A new OS kernel might bump into the bug, of course, but at that point, it's&nbsp;the programmer's problem. A friend who wrote a
small real-time operating system had to&nbsp;familiarize&nbsp;himself with several errata items, and was the first to report some of these
items.</p>
<p>It should be noted that an x86 CPU should be way less buggy&nbsp;in the privileged areas&nbsp;than the average embedded CPU. That's
because it's more <em>compatible</em> in the privileged areas than almost any other CPU. AFAIK, today's x86 CPUs will still run
unmodified&nbsp;OS binaries from the 80s and 90s.</p>
<p>Other CPUs are not like that. I recall that ARM has 2 instructions, MCR and MRC (Move Register&nbsp;to/from Co-processor), and the
meaning of those instructions depends on their several constant arguments. It could flush the cache or program the memory
protection unit or do other things -&nbsp;a bit like&nbsp;a hypothetical CALC&nbsp;instruction where CALC 0&nbsp;does addition, CALC 1 subtracts,
CALC 2 multiplies, etc. My&nbsp;point isn't that MCR and MRC look cryptic in assembly code, but that <em>the meaning changes between
ARM generations</em>. MIPS is similar, except they're&nbsp;called MFC0 and MTC0, Move From/To Coprocessor 0.<br>
</p>
<p>These incompatibilities do&nbsp;not break userspace programs, which can't execute any of these instructions -&nbsp;but the OS needs to
be tweaked to support a new core. If a new core introduces a bug in a privileged instruction, <em>that doesn't break old OS code
any more than it's already broken</em> by ISA incompatibilities. Updating OS code is the perfect opportunity to also work around
fresh hardware bugs.</p>
<p>x86 chips also run more OSes than chips based on most other architectures. For instance, a now-defunct&nbsp;team making a fairly
widespread ARM-based&nbsp;application processor had to port about 3 versions of Linux (is there a chip maker who likes Linux with its
endless versions and having to port it themselves? Or do they secretly wish they could tell Linus Torvalds what he publicly said
to&nbsp;NVIDIA, namely, "fuck you"?)&nbsp;They also&nbsp;supported OS vendors in the porting of Windows and QNX. Overall, the chip probably
ever ran 5 full-blown&nbsp;OSes.&nbsp;x86 chips need to run endless OS builds - often built&nbsp;from&nbsp;very similar source code, but still.</p>
<p>The same principle applies to all hardware. <strong>It's bug-free if and only if&nbsp;they can't sell it with bugs</strong>. If
they can sell it with bugs and make it your problem, they very well&nbsp;might.</p>
<h2 id="memory">Memory</h2>
<p>$100 says your DRAM chip works. The DRAM chip is a mindless slave implementing precise&nbsp;commands by the DRAM controller on the
master chip,&nbsp;without any feedback - there are no retries, no negotiation,&nbsp;no way&nbsp;to say you're sorry. And no software will run
properly on faulty DRAM. Faulty DRAM isn't a marketable product.</p>
<p>Your board is definitely buggy. They told you they checked&nbsp;signal integrity, but they lied. If DRAM malfunctions, it's
probably&nbsp;the board, or the boot code&nbsp;programming DRAM-related components in a way that doesn't work on this board.</p>
<p>In the middle, there's the DRAM controller and the PHY. You'll only see bugs there if you're a chip maker - a chip is not
marketable unless such bugs are already worked around somehow. If you are indeed a chip maker, this is when you find out why
fabless chip companies are worth so much more than the equally fabless vendors of "IPs" such as CPUs and DRAM controllers. The
short answer is that chip makers are&nbsp;exposed to most of the risk. And in your case,&nbsp;some of this risk has&nbsp;just
been&nbsp;realized.</p>
<p>A DRAM controller bug can be very damaging&nbsp;to&nbsp;the chip maker, whose engineering samples might not work and whose production
schedule might be delayed. For the DRAM controller vendor - no big deal, "we have 3 more customers affected by this bug, we must
say you're taking it unusually passionately!" This is an actual quote. I want to add something here, something describing&nbsp;what
we chip makers think of&nbsp;these people,&nbsp;but words fail me. The point is, they fix the bug and ship the fixed version to their next
customers. You get to figure out how to make your engineering samples kinda work (often lowering the DRAM frequency helps), and
perhaps how to fix the design&nbsp;without too many changes&nbsp;to&nbsp;the wafer&nbsp;masks.</p>
<p>Bottom line is, DRAM controllers and PHYs can have bugs, usually it's the chip&nbsp;maker's problem, managing this risk is not
fun.</p>
<p>The bus interconnect&nbsp;between your processors and the&nbsp;DRAM controller probably&nbsp;doesn't have bugs - not correctness bugs, at
least. That's because today it's usually produced by a code generator, and such a code generator is really hard to market if it
has bugs, because they'll manifest in so many different ways and places. I found a bug in an interconnect once, and I was very
proud of my tests, but that&nbsp;was a preliminary version, and they found the bug independently. Real, supported versions always
worked fine.</p>
<p><em>Performance</em> bugs around memory access are legion, of course, because you can totally sell products with performance
issues, at least&nbsp;up to a point. A chip can have 2&nbsp;processors with 8-byte buses each, going to a DRAM giving you&nbsp;16 bytes per
cycle, through a shared&nbsp;8-byte-per-cycle bottleneck. This&nbsp;interconnect is the handiwork of some time-starved dude on the chip
maker's team,&nbsp;armed with an interconnect-generating tool. Even such an idiotic&nbsp;issue will manifest on some benchmarks but not
others, and might not get caught at&nbsp;design time. And if you think <em>that</em> is stupid, I've heard of&nbsp;a level 2 cache which
never actually cached anything, and this fact&nbsp;happily went&nbsp;unnoticed for a few months. (Of course, <em>this</em> not being
caught at design time is when the team should start looking for a new career.)</p>
<p>Similarly, DRAM schedulers, supposedly very clever about optimizing DRAM performance, can in practice be really stupid, etc.
In fact, performance issues&nbsp;are among the&nbsp;hardest to pinpoint, and so are found in the&nbsp;greatest abundance in hardware and
software alike. But in a way, they aren't bugs.</p>
<h2 id="peripheral-devices">Peripheral devices</h2>
<p>Expect peripheral device controllers to be pretty shitty. There really is no reason to make them particularly good. Only
device drivers access these things, so it all concerns just a handful of programmers,&nbsp;and then working around a hardware bug
here is easier than almost anywhere else.</p>
<p>A device driver has the device all to itself, nothing can touch the hardware concurrently unless the driver lets it, and the
code can fiddle with the hardware controller all it likes, perhaps emulating some of the functionality on the CPU if necessary,
and&nbsp;doing arbitrarily complex things to work around bugs. Starting with simpler things like reading memory-mapped&nbsp;registers
twice - a workaround for&nbsp;a real old&nbsp;bug from a real vendor in the automotive space, one who huffs and puffs a lot about
reliability and safety.</p>
<p>And a lot of peripheral devices also allow some room for error at the protocol level - you can drop packets, retransmit
packets, checksums tell you if ultimately all the data was transferred correctly, you can negotiate on the protocol features,
etc. etc. All that helps work around hardware bugs, reducing the pressure to ship correct hardware.</p>
<p>Also, since few people read the spec, there's no reason to make it very clear, or detailed, or up-to-date, or fully correct,
or maintain errata properly. This is not to say that nobody does it right, just that many don't, and this shit still sells.
Nobody cares that driver programmers suffer.</p>
<p>(By the way, I'm not necessarily condemning people at the&nbsp;hardware side here. Some low-level&nbsp;programmers like to complain
about how bad hardware is, but it's not obvious how much should be invested to make the driver writer's job easy, even&nbsp;from a
purely&nbsp;economic point view of optimally using society's resources, regardless of anyone's bottom line. If a chip is shipped
earlier at the cost of including a couple of peripheral controllers which are&nbsp;annoying&nbsp;to write drivers for, maybe it's the
right trade-off. I'm not saying&nbsp;that bugs should be exterminated at all cost, I'm just telling where they live.)</p>
<h2 id="miscommunication">Miscommunication</h2>
<p>As a programmer, do not expect every device to follow protocols correctly. What will work is the CPU accessing memory, in any
way the CPU can access memory -&nbsp;with or without caching (the two ways generate vastly different bus commands.)&nbsp;But if the path
between the device doing the access and the device handling the access is a less traveled one, then you might need to do the
access in a very specific way.</p>
<p>For instance, bus protocols might mandate that access to an unmapped&nbsp;address will result in an error response. But an
unmapped address might fall into a large region which the interconnect associates with a hardware module written by some bloke.
So it routes your request to the bloke's module. The&nbsp;bloke can and will write hardware description code that checks for every
address mapped within his range,&nbsp;and then&nbsp;returns a response - but the code&nbsp;does&nbsp;<em>nothing</em> when the address is
unmapped.&nbsp;Then reading from this address will cause the CPU to hang forever, and not only a debugger running on the chip,
but&nbsp;even a JTAG probe will not tell you where it's stuck.</p>
<p>There are many issues of this sort - a&nbsp;commonly unsupported thing is byte access as opposed to full word access (the hardware
bloke didn't want to look at low address bits or byte masks), etc. etc. A bus protocol lawyer might be able to prove that the
hardware is buggy in the sense of not following the&nbsp;protocol properly. A programmer must call it a feature and live with it.</p>
<p>As a chip maker, there's the additional trouble when you&nbsp;hook&nbsp;two working devices together, but lie about the protocol subset
they support, and they will not work together. For instance, a DMA engine and a cache might both "support out of order bus
responses." But the cache will return the response data interleaved at the world level, while&nbsp;the DMA might require&nbsp;responses&nbsp;to
be interleaved at the burst level, where the burst size is defined by the DMA's read commands.</p>
<p>The chip maker is rather unlikely to ship hardware with this sort of a bug, so by itself it's rarely a programmer's problem.
But they might make you set a bit in the DMA controller that disables the kind of requests producing out of order bus responses
when accessing certain addresses. Again you can argue if it's a bug or a feature, but either way, if you won't set the bit,
interesting things will transpire.</p>
<h2 id="summary">Summary</h2>
<ul>
<li>Don't trust freshly designed boards</li>
<li>Don't trust peripheral controllers</li>
<li>Trust CPUs in userspace &amp;&nbsp;DRAM chips (almost&nbsp;always), and everything&nbsp;between the two (unless the chip is new &amp;
untested)</li>
<li>Expect to bump into unsupported bus&nbsp;protocol features if you do anything except accessing memory from a CPU</li>
<li>If you write your own OS, be prepared to work around CPU bugs (except perhaps on the PC)</li>
</ul>
<p><a href="low-level-is-easy.html">I wrote a long time ago</a>, and I still believe it, that lower-level programming is made
relatively&nbsp;easy by the fact that you're less likely to have bugs in your dependencies. That's because low-level bugs both&nbsp;hurt
more users and are harder to fix, and therefore people try harder to avoid them in the first place. However, this isn't equally
true for all the&nbsp;different low-level things you depend on.</p>
<p>I've described the state of things with hardware as it is in my experience, and attempted to trace&nbsp;the differences to the
different&nbsp;costs of bugs to different&nbsp;vendors. The same reasoning applies to software components -&nbsp;for instance, compilers&nbsp;are
more likely to have bugs than&nbsp;OS kernels -&nbsp;because, by definition, compiler bugs cannot break existing binaries,&nbsp;but kernel bugs
will do that. So I think it's a generally&nbsp;useful angle to look at things from.</p>]]></description>
      <comments>https://yosefk.com/cgi-bin/comments.cgi?post=blog/the-habitat-of-hardware-bugs#comments</comments>
      <pubDate>Wed, 13 Jul 2016 00:00:00 +0000</pubDate>
      <dc:creator>Yossi Kreinin</dc:creator>
      <wfw:commentRss>https://yosefk.com/blog/the-habitat-of-hardware-bugs.feed</wfw:commentRss>
    </item>
    <item>
      <title>Fun won't get it done</title>
      <link>https://yosefk.com/blog/fun-wont-get-it-done.html</link>
      <description><![CDATA[<p>OK, published at 3:30 AM. That's a first!</p>
<p>So.&nbsp;Got something you want to do over the coarse of a year? Here's a&nbsp;motivation woefully insufficient to pull it off:</p>
<ul>
<li>It's fun!</li>
</ul>
<p>What could&nbsp;give you enough drive to finish the job? Anything with a reward <em>in the future, once you're done</em>:</p>
<ul>
<li>Millions of fans&nbsp;<strong>will</strong> adore me.</li>
<li>It <strong>will</strong> be the ugliest thing on the planet.</li>
<li>I <strong>will</strong> finally understand quantum neural rockets.</li>
<li>We <strong>will</strong> see who the loser is, Todd!</li>
<li>I <strong>will</strong> help humanity.</li>
<li>I <strong>will</strong>&nbsp;destroy humanity.</li>
</ul>
<p>It doesn't matter how noble or ignoble your&nbsp;goal is. What matters is <strong>delaying gratification</strong>. Because even
your&nbsp;favorite thing in the&nbsp;world will have&nbsp;shitty bits if you chew on&nbsp;a big enough chunk of it. A few months or years worth of
work are <em>always</em> a big enough chunk, so there <em>will</em> be shitty bits. Unfortunately, it's also the minimum-sized
chunk to do anything of significance.</p>
<p>This is&nbsp;where many brilliant talents drown. Having known the joy of true inspiration, it's hard to settle for less, which you
<em>must</em> to have any impact. Meanwhile,&nbsp;their&nbsp;thicker peers happily butcher task after task. Before you know it,&nbsp;these
tasks&nbsp;add up to an&nbsp;impactful result.</p>
<p>In hindsight, I was really&nbsp;lucky in that I chose a profession for money instead of love.&nbsp;Why? <strong>Stamina</strong>. Money
is a reward in the future that lets you ignore the shittier bits of the present.</p>
<p>Loving every moment of it, on the other hand, carries you until that moment&nbsp;which you <em>hate</em>, and then you need a new
sort of fuel. Believe me, I know. I love drawing and animation, and you won't believe how many times I started and stopped doing
it.</p>
<p>But the animation teacher who taught me 3D said he was happy to put textures on toilet seat models when he started out.
<em>That's</em> the kind of appetite you need – and very few people&nbsp;naturally feel that sort of attraction to toilet seats. You
need a&nbsp;big reward in the future, like "I'm going to become a pro," to pull it off.</p>
<p>But I don't want to become a pro. I don't want to work in the Israeli animation market where there's scarcely a feature
film&nbsp;made. I don't even want to work for a big overseas animation studio. I want to make something, erm, something beautiful
that I love, <strong>which is a piece of shit of a goal</strong>.</p>
<p>Because you know where I made most progress picking up actual skills? In an evening animation school, where I had a&nbsp;perfectly
good goal: survive. It's good because it's a simple, binary thing which doesn't give a rat's ass about your mood. You either
drop out or you don't. But "something I love" is fluid, and depends a lot on the mood. And&nbsp;when you hate this thing you're
making, as you sometimes will, it's hard to imagine loving it later.</p>
<p>Conversely, imagining how I don't drop&nbsp;out is easy. This is what I was imagining when sculpting this bust, which 90% of the
time I hated with a passion because it looked like crap. But I thought, "I'm not quitting, I'm not quitting, I'm not quitting,
hey, I&nbsp;get the point of re-topology in Mudbox, I'm not quitting, I'm not quitting, hey, I guess I see what&nbsp;the specular map
does, I'm not quitting... Guess I'm done!"</p>
<div class="video_16_9" style="position: relative;overflow: hidden;width: 100%;padding-top: 56.25%;">
<iframe class="responsive_iframe" allowfullscreen="allowfullscreen" src="https://player.vimeo.com/video/171365263" style="position: absolute;border: 0;top: 0;left: 0;bottom: 0;right: 0;width: 100%;height: 100%;">
</iframe>
</div>
<p>And now let's talk about beauty for a moment.</p>
<p>I'm a programmer. I like to think that I'm not the thickest, butcherest programmer, in that I understand the role of beauty
in it. For the trained eye, programs can be beautiful as much as&nbsp;math, physics or chess, and a beautiful program is better
<em>for business</em> than the&nbsp;needlessly uglier program. (Ever tried pitching the value of beauty to someone businessy? Loads
of fun.)</p>
<p>But you know why beauty is your enemy? Because it sucks the fun out of things. How? Because you're making this thing and
chances are, <strong>it's not beautiful according to your own standard</strong>. The trap is, your&nbsp;taste for beauty is usually
ahead of your&nbsp;creative ability. In any area, and then in any sub-area of that area, ad infinitum, you can tell ugly from
beautiful long before you can make something beautiful yourself. And&nbsp;even if&nbsp;you can satisfy your own taste,&nbsp;often&nbsp;the final
thing is beautiful, but not the states it goes through.</p>
<p>So&nbsp;the passionate, sensitive soul is hit twice:</p>
<ol>
<li>You're driven by fun and inspiration because you've once experienced it and now you covet it.</li>
<li>Your sense of beauty, frustrated by the state of your creation, kills&nbsp;all the fun – that very fun which&nbsp;you insist must be
your only fuel.</li>
</ol>
<p>Life is easier if you want a yacht. I think you can buy a&nbsp;decent&nbsp;one for $300K, and certainly for $1M. Now all you need to do
is make that money, doing doesn't matter what – imagining that yacht will help you do <em>anything</em> well! If you want
beauty, however, I do not envy you.</p>
<p>How do I cope with my desire for beauty?&nbsp;The first step is acknowledging&nbsp;the problem, which I do. The fact is that my worst
failures in programming came when I insisted on beauty the most. The second step is shunning beauty as a <em>goal</em>, and
making it&nbsp;into a <em>means</em> and a <em>side-effect</em>.</p>
<p>I need a program doing at least X, taking at most Y seconds, at a date not later than Z.&nbsp;I'll keep ugliness to a minimum
because ugly programs work badly. And if it comes out particularly nicely, that's great. But beauty is&nbsp;not a goal, and&nbsp;enjoying
the beauty of this program as I write it is not why I write it.</p>
<p>And if you think it's true for commercial work but not open source software, look at, I dunno, Linux. Read some <a href="http://www.h-online.com/open/features/Interview-Linus-Torvalds-I-don-t-read-code-any-more-1748462.html">Torvalds</a>:</p>
<blockquote>
<p>Realistically, every single release, most of it is just driver work. Which is <strong>kind of boring in the sense there is
nothing fundamentally interesting in a driver</strong>, it's just support for yet another chipset or something, and at the same
time that's kind of the bread and butter of the kernel. More than half of the kernel is just drivers, and so <strong>all the big
exciting smart things we do, in the end it pales</strong> when compared to all the work we just do to support new hardware.</p>
</blockquote>
<p>Boring bits. Boring bits that&nbsp;must be done to make something of value.</p>
<p>Does this&nbsp;transfer to art or poetry or any of those things&nbsp;whose whole point is beauty? Well, yeah, I think it does, because
no,&nbsp;beauty is not the whole point:</p>
<ul>
<li>The most important thing about a drawing is that it's done. Now it exists, and people can see it, and you can make
<em>another one</em>. Practice. They will not come out very well if they don't come out.</li>
<li>Often people like your&nbsp;subject.&nbsp;There's a continuum between "it's beautiful in a way that words cannot convey" and "I love
how this song&nbsp;expresses&nbsp;my favorite political philosophy." To the extent that a work of art tells a story, or even sets up&nbsp;a
mood, its beauty <em>does</em> become a means to an end.</li>
<li>Just because the end result is beautiful to the observer, and even if that's the only point, doesn't mean every step making
it was an orgy of beauty for whomever made it. Part of what goes into it is boring, technical work.</li>
</ul>
<p>So here, too I'm trying to make beauty a non-goal. Instead my goals are "make a point" and "keep going," and you try to add
beauty, or remove ugliness, as you go.</p>
<p>For example,&nbsp;I didn't do a graduation project in the evening school, but I&nbsp;animated a short on my own in the same timeframe,
and I published it, even though it's not the beautiful thing I always dreamed about making. And&nbsp;I'm not sure anyone gets the
joke except me. (I'm not sure I get it anymore, either.)</p>
<div class="video_16_9" style="position: relative;overflow: hidden;width: 100%;padding-top: 56.25%;">
<iframe class="responsive_iframe" allowfullscreen="allowfullscreen" src="https://player.vimeo.com/video/171368757" style="position: absolute;border: 0;top: 0;left: 0;bottom: 0;right: 0;width: 100%;height: 100%;">
</iframe>
</div>
<p>Now my goal is "make another one." It's a good goal, because it's easy to imagine making another one. It's proper&nbsp;delayed
gratification.</p>
<p>And if you've enjoyed programming 20 years ago&nbsp;and are trying to reignite the passion, I suggest that you find a goal as
worthy for you as "fun" or "beauty", but as clear and binary as a yacht.&nbsp;And you can settle for less worthy, but not for less
clear and binary. Because everything they told you about "extrinsic motivation" being inferior to "intrinsic motivation" is one
big lie. And this lie will&nbsp;fall apart the moment you sink your teeth into a bunch of shit, as will always happen if you're
trying to accomplish anything.</p>]]></description>
      <comments>https://yosefk.com/cgi-bin/comments.cgi?post=blog/fun-wont-get-it-done#comments</comments>
      <pubDate>Mon, 01 Aug 2016 00:00:00 +0000</pubDate>
      <dc:creator>Yossi Kreinin</dc:creator>
      <wfw:commentRss>https://yosefk.com/blog/fun-wont-get-it-done.feed</wfw:commentRss>
    </item>
    <item>
      <title>Hiring (self-driving algos, HLL compiler research)</title>
      <link>https://yosefk.com/blog/hiring-self-driving-algos-hll-compiler-research.html</link>
      <description><![CDATA[<p>OK, so 2 things:</p>
<p>1. If you send me a CV and they're hired to work on self-driving algos – machine vision/learning/mapping/navigation, I'll pay
you a shitton of money. (Details over email.) These teams want CS/math/physics/similar degree with great grades, and they want
programming ability. They'll hire quite a lot of people.</p>
<p>2. The position below is for my team and if you refer a CV, I cannot pay you a shitton of money. But:</p>
<p><strong>We're developing an array language that we want to efficiently compile to our in-house accelerators (multiple target
architectures, you can think of it as "compiling to a DSP/GPU/FPGA.")</strong></p>
<p>Of recent public efforts, perhaps <a href="http://halide-lang.org/">Halide</a> is the closest relative (we're compiling AOT
instead of processing a graph of C++ objects constructed at run time, but I'm guessing the work done at the back-end is somewhat
similar.) What we have now is already beating hand-optimized code in our C dialects on some programs, but it's still a "blue
sky" effort in that we're not sure exactly how far it will go (in terms of the share of production programs where it can replace
our C dialects.)</p>
<p>As usual, we aren't looking for someone with experience in exactly this sort of thing (here especially it'd be hopeless since
there are few compiler writers and most of them work on lower-level languages.) Historically, the people who enjoy this kind of
work have a background in what I broadly call (mislabel?) "discrete math" -&nbsp; formal methods,&nbsp;theory of computation, board game
AI, even cryptography, basically anywhere where you have clever algorithms in a discrete space that can be shown to work every
time. (Heavyweight counter-examples missing one of "clever", "discrete" or "every time" – OSes, rendering, or NNs. This of
course is not to say that experience in any of these is disqualifying, just that they're different.)</p>
<p>I think of it as a gig combining depth that people expect from academic work with compensation that people expect from
industry work. If you're interested, email me (Yossi.Kreinin@gmail.com).</p>
<p>All positions are in Jerusalem.</p>]]></description>
      <comments>https://yosefk.com/cgi-bin/comments.cgi?post=blog/hiring-self-driving-algos-hll-compiler-research#comments</comments>
      <pubDate>Sun, 11 Sep 2016 00:00:00 +0000</pubDate>
      <dc:creator>Yossi Kreinin</dc:creator>
      <wfw:commentRss>https://yosefk.com/blog/hiring-self-driving-algos-hll-compiler-research.feed</wfw:commentRss>
    </item>
    <item>
      <title>Things want to work, not punish errors</title>
      <link>https://yosefk.com/blog/things-want-to-work-not-punish-errors.html</link>
      <description><![CDATA[<p>For better or worse, things want to work.</p>
<p>Consider&nbsp;driving at night on unlit, curvy mountain roads, at a speed about twice the limit, zigzagging between cars,
including oncoming ones. Obviously dangerous, and yet many&nbsp;do this, and survive. How?</p>
<ul>
<li>Roads and cars are built with big safety margins</li>
<li>Other drivers don't want to die and help you get through</li>
<li>Practice makes perfect, so you get good at this bad thing</li>
</ul>
<p>The road, the car, you, other drivers, and their cars all want this to work. So for a long while, it does, until it finally
doesn't. I know 3-4 people who&nbsp;drive like this habitually. At least 2 of them totaled cars. All think they're excellent drivers.
All have high IQs, making you wonder just what this renowned benchmark of human brains really tells us.</p>
<p>Now consider a terribly managed project with an insane deadline, and a team and budget too small. All too often, this too
works out. How?</p>
<ul>
<li>Unless it physically cannot exist, a&nbsp;solution <strong>wants</strong> you to find it. You carve out a piece and the next
piece suggests itself. Even if&nbsp;management fails&nbsp;to think how the pieces&nbsp;fit together, the pieces often come out such that&nbsp;they
<em>can</em> be made to fit with modest extra effort.</li>
<li>And then the people who make the pieces <strong>want</strong> them to fit. Even if&nbsp;the process is totally mismanaged, many
people will talk to each other and find out&nbsp;what to do to make parts work together.</li>
<li>The project was approved because a customer was persuaded. At this point, the customer&nbsp;<strong>wants</strong>&nbsp;the project to
succeed. A little bit of schedule slippage will not make them change their minds, nor will a somewhat less impressive result.
More slack for you.</li>
<li>The vendor, too <strong>wants</strong> the project to succeed, and will tolerate a little bit of budget overrun. More
slack.</li>
<li>Most often, when things fail, they fail visibly. It's as if things <strong>wanted</strong> you to see that they fail, so
that you fix them.</li>
</ul>
<p>The fact is that by cutting features, having a few non-terminal bugs,&nbsp;and being somewhat late and over budget, most projects
can be salvaged. In fact, when they say that "most projects fail," the PMI <a class="footnote-ref" role="doc-noteref" href="#fn1" id="fnref1"><sup>1</sup></a> defines "failure" as being a bit late or over budget. If "failure" is defined as outright
cancellation, I conjecture that most projects "succeed."</p>
<p>Which projects are least likely to&nbsp;be canceled? In other words, where is being&nbsp;late, over budget and off the original spec
most tolerable? Obviously, <em>when the overall delivered value&nbsp;is the highest</em>, both in absolute terms and relatively to
the cost. In other words, <strong>reality punishes bad management the least&nbsp;in the most impactful cases</strong>.</p>
<p>What is the biggest problem with bad management?&nbsp;Same as crazy driving: risk.&nbsp;The problem in both cases is you risk
high-cost, low-probability events. It's terrible things that tend not to happen. And&nbsp;people are pretty bad at learning from
mistakes they&nbsp;never had to pay for.</p>
<p>Wannabe racecar drivers fail to&nbsp;learn from driving into risky situations which their own eyes tell them are risky. For
managers, learning is harder – the risks accumulated through bad management are abstract, instead of viscerally scary. In fact,
a lot of the risks are never understood by management, or even fully reported. There's just too much risk to sweep under various
rugs to make it all ingrained in institutional memory.</p>
<p>In fact, it's even worse, because risk-taking is actually <strong>rewarding</strong> as long as the downside doesn't
materialize. The crazy driver gets there 10 minutes earlier. Similarly, non-obviously hazardous management often delivers at an
obviously small cost. And while driving is&nbsp;not actually&nbsp;competitive, except in the inflamed minds of the zigzagging few, most
projects are delivered in very competitive environments indeed. And competition can make even small rewards for risk decisive –
as it can with any other smallish factor&nbsp;large enough to make a difference between victory and defeat.</p>
<p>Things want to work more than they want to punish us for our errors. The punishment may be very cruel and unusual alright,
but it's rare. It seems that the universe, at least The Universe of Deliverables, is Beckerian. It delivers optimal punishment
for rational agents correctly estimating probabilities. Sadly,&nbsp;humans are bad at probability.</p>
<p>And thus crazy drivers and bad managers alike (often the same people, BTW) march from one insane adventure to the next,
gaining more and more confidence in their brilliance.</p>

<section class="footnotes footnotes-end-of-document" role="doc-endnotes" id="footnotes">
<hr>
<ol>
<li id="fn1"><p>PMI&nbsp;(The Project Management Institute) is&nbsp;a con, where they sell you "PMBOK" (Project Management Body of
Knowledge, a thick book you can use as a monitor stand) and "PMP" (Project Management Professional, a certification required by
PMI's conscious or unwitting accomplices in dark corners of the industry.) A variety of more elaborate cons targeted at narrower
audiences incorporate PMI's core body of cargo cult practices.<a class="footnote-back" role="doc-backlink" href="#fnref1">↩︎</a></p></li>
</ol></section>]]></description>
      <comments>https://yosefk.com/cgi-bin/comments.cgi?post=blog/things-want-to-work-not-punish-errors#comments</comments>
      <pubDate>Mon, 27 Feb 2017 00:00:00 +0000</pubDate>
      <dc:creator>Yossi Kreinin</dc:creator>
      <wfw:commentRss>https://yosefk.com/blog/things-want-to-work-not-punish-errors.feed</wfw:commentRss>
    </item>
    <item>
      <title>Patents: how and why to get them</title>
      <link>https://yosefk.com/blog/patents-how-and-why-to-get-them.html</link>
      <description><![CDATA[<p>I'm going to discuss 3 very basic things about patents:</p>
<ul>
<li>Why it's good for you to get them;</li>
<li>Why it might be bad for your employer (and why they don't care);</li>
<li>How to get a patent for your idea (doesn't matter which.)</li>
</ul>
<p>Some of my points are a bit naughty. But I maintain that they're based in fact and fairly widely known. So well-known, in
fact, that I'm surprised to have never read it somewhere else.</p>
<p>My explanation is that the hatred of patents in the tech world is such that nothing except "HATE! HATE! HATE!" can be said on
the subject in polite society. In this atmosphere, "Patents: how and why to get them" reads like "Humans: how and why to cook
them."</p>
<p>If you can make yourself read this human-cooking manual, however, I think you'll find both amusing and useful things. I have
more experience with patents than I've ever asked for, having worked on this stuff with lawyers from the smallest law firms to
the largest ones, including lawyers who personally handled the most famous lawsuits for the most famous tech clients. I'm not an
authority on patents, but I have good stories.</p>
<h2 id="what-patents-give-you">What patents give you</h2>
<p>Some companies pay you money per patent. But it's rarely enough to make it worth your while, unless it's all you're doing.
Patents look good on your CV, but reactions might be negative as well (you might appear "overqualified," "an expert in an
unrelated field," etc.)</p>
<p>What's the one thing a patent undeniably buys you? <em>A right to legally and publicly discuss your work –</em> which you
often can't get in any other way. This is not a side-effect of patent law, but its whole stated point. Patent law prompts
companies to <em>publish</em> their ideas, in exchange for a time-limited monopoly right to use the ideas.</p>
<p>Note that publishing ideas in patents is easy, and the benefit <em>for the author</em> is certain. But getting and enforcing
a monopoly for said ideas is <em>not</em> easy, so the benefit <em>for the proprietor</em> is not at all certain. Here's
why.</p>
<h2 id="what-patents-give-and-dont-give-your-employer">What patents give (and don't give) your employer</h2>
<p>Some problems with patents are so obvious that even patent lawyers will honestly discuss them with their clients:</p>
<ul>
<li>When you submit a patent application, it becomes public forever, <em>even if it's rejected.</em> You will have paid legal
fees with the end result of granting competitors access to your ideas.</li>
<li>If you sue for patent infringement, your patent might be <em>invalidated</em> as a result. It's like a rejected patent
application, but with at least $1 million more in legal fees.</li>
</ul>
<p>But there's another, potentially far bigger problem, that patent lawyers will rarely mention, let alone admit its extent:</p>
<ul>
<li>You don't get monopoly rights to everything you file in the patent application. You publish a "spec" and "claims." The
monopoly is granted <em>only for the claims</em> – perhaps in a reduced form relatively to the original patent application, due
to feedback from the examiner. Yet <em>the entire spec</em>, much of it not covered by the claims, becomes public.</li>
</ul>
<p>So what's the big deal, you might ask? The spec describes some device or method. The claims describe the supposedly new ideas
used in this device or method. All you have to do is write a spec such that nothing of value is disclosed that is not covered by
the claims.</p>
<p>However, in reality, the published spec is often quite close to <em>the actual spec used by engineers</em>, with all the
details. That's simply the path of least resistance:</p>
<ul>
<li>Patent lawyers don't know which claims will be rejected by the examiner. (If they knew, you wouldn't have a heap of rejected
applications, nor patents invalidated in courts.) They file relatively broad claims, and then change the claims to address
challenges by the examiner, until a patent is granted. The catch is that <em>you can only base your new claims on details
included in the originally filed spec</em> – the spec can never be altered. Thus a detailed, complete spec maximizes the chances
to get <em>some</em> patent out of the filing – covering 90% or 10% of the spec, depending.</li>
<li>More prosaically, if we don't file the actual spec but instead write a new one tailored to the claims, who's gonna do it?
Neither the engineer nor the lawyer necessarily has the ability to do it, and surely neither has any interest in doing it. Much
better to take existing documents and do the minimal necessary translation from English to legalese.</li>
</ul>
<p><strong><em>Ultimately, there's a conflict of interest between your employer and their patent lawyer, and a surprisingly
perfect alignment of interests between the lawyer and yourself</em></strong>:</p>
<ul>
<li><strong>The lawyer wants to publish as many details as possible</strong> – to maximize the chance of getting a patent, and
to avoid extra work;</li>
<li><strong>The engineer also wants to publish as much as possible –</strong> to make his ideas known to the fullest extent, and
to avoid extra work;</li>
<li><strong>The employer/shareholder wants to publish as <em>little</em> as possible –</strong> but has no simple, reliable way
to incentivize anyone to push in this direction (though of course some are much better at this than others.)</li>
</ul>
<p>Funnily enough, this too is largely in line with the lawmaker's stated intent – prompting companies to publish ideas instead
of keeping them secret. But why do companies file patents?</p>
<p>The answer is that patents are never read – they're counted. More precisely, a company's goal is to acquire enough patents so
that they can <em>only</em> be counted – but not read and understood in a reasonable amount of time.</p>
<p>If you have too many patents to read and understand (hundreds, thousands or more), then investors and competitors alike
assume you "own your domain" – you can counter-attack if sued. You're as well-defended legally as you can possibly be. But if
you have few patents, someone might read and understand most of them – and create a narrative about some legal weakness. Such
narratives are bad for the stock price.</p>
<p>This situation must be avoided. And that's all there is to it – at least in the computing industry. And I know it might sound
too dismissive to be convincing. But the fact is that the content of patents is just too complex to drive business decisions.
The feasible thing for a decision-maker is to pick the bucket to put you in, out of "no patents, some patents, a shitton of
patents." For more information, see the seminal work "<a href="https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow">Pulling
Decisions out of One's Ass: Fast and Slow,</a>" keeping in mind that decision-makers have a lot of decisions to make, so they
must be Fast.</p>
<h2 id="why-filing-patents-isnt-a-crime-on-par-with-cannibalism">Why filing patents isn't a crime on par with cannibalism</h2>
<p>Considering the above, I don't think that <em>a product company employee</em> filing patents pollutes the tech environment as
badly as people believe.</p>
<p>Product companies file patents largely for self-defense. Some occasionally attack startups, but how many startups were
destroyed by a patent lawsuit vs the number of those destroyed by a badly managed acquisition (with the original investors doing
just fine)? And there are examples of big companies buying startups already attacked by a lawsuit filed by a bigger product
company, confident that between two big companies, the legal result will be a stalemate. Thus for a big company genuinely
fearing your product, it's much safer to buy you than sue you and have you bought by a big competitor.</p>
<p>The real trouble is patent trolls, who cannot be counter-attacked. But the only way a product company's patents will land in
a troll's hands is if the company goes bankrupt and sells the patents. Well, guess what – in these cases, other product
companies are eager to outbid the trolls. For example, when MIPS Technologies was sold to Imagination for ~$60 million many
years ago, its patents, sold separately, fetched ~$500 million from some CPU cartel involving various big name CPU companies.
Alternatively, a failing company can turn into a troll and sue a successful product company (MicroUnity comes to mind.)</p>
<p>Thus patents of failing product companies result in a weird form of socialism, where profit is spread more evenly between
investors, with losers getting a chunk of the winners' profits. I don't think this chunk is nearly large enough on average to
substantially reduce the incentive to work hard for the win, which is supposedly "the" trouble with laws subsidizing losers.</p>
<p>My point is that patent trolls and product companies seem to live in largely parallel universes. There are patents filed with
the intention to be used by a patent troll, and there are patents filed by product companies, and the latter cause far less
damage.</p>
<h2 id="how-to-get-a-patent">How to get a patent</h2>
<p>I've lost count of the number of times I've heard the words "The Black Swan." It rather aggrieves me, but you gotta hand it
to Taleb. Everyone is trying to pollute our language by needlessly coining catchphrases in a quest to be memorable, but he
succeeded more than most. Surely I wouldn't hear this nonsense as often if he called the book "The Unforeseen Event."</p>
<p>Getting patents is a lot like branding. The trick is to call old things new names.</p>
<p>Why does it take a patent lawsuit and at least $1 million in legal fees to find out if a patent <em>really</em> is a patent –
or to see it invalidated by the court? Because searching prior art is hard. "Prior art" includes everything published prior to
the patent – older patents, academic papers, and everything else, really. Strictly speaking, you never know if you're done.</p>
<p>How does the patent office examiner examine prior art, at a cost much lower than $1 million? Some equivalent of quick
googling. The input of search engines is words and short phrases. If you use words and phrases which are uncommon in your
domain, the search will come up blank, or it will find things so obviously unrelated to your work that even a patent examiner
will get it.</p>
<p>If you're extending the concept of a thread, don't call the result "extended threads", call it "hypercontexts." If you're
calculating a histogram, call it a "distribution estimator." And so on. Again, I know this sounds too dismissive of the system
to be believable. Well, try it. File a patent application full of "distribution estimators" and another one written in plain
English. See which gets approved more smoothly.</p>
<p>Note that you might be tempted to conduct a prior art search yourself before filing the patent application, as a matter of
due diligence. Yet some lawyers actually recommend against it, since if you do find prior art, you're now willfully infringing
on it, and should cease and desist. My advice is to come up with a bunch of Black Swans/Distribution Estimators describing your
idea, and pick the ones with the fewest Google results (patent search and otherwise).</p>
<p>And don't actually <em>read</em> any patent you accidentally find – don't willfully infringe, it's illegal. Just count them.
Patents are never read, only counted – sounds familiar?</p>
<p>The other very important thing – which you mostly get to worry about in smaller companies – is to get the right kind of
lawyer. Patent lawyers are fallen engineers, with engineering degrees, and sometimes actual engineering experience.
<strong><em>The underlying engineer who's morphed into a lawyer ought to have specialized in your domain. No compromise is
acceptable here</em>.</strong> If you're doing optics, don't work with a guy who did chip design, and if you do chip design,
don't work with a guy who did optics.</p>
<p>It doesn't matter if the lawyer is a Partner ($900/hour), an Associate ($450/hour), or some lesser life form in the law firm.
It doesn't matter whether the firm is the biggest name in the industry or completely unknown. What matters is engineering
knowledge. Don't expect a patent lawyer to honestly tell you he doesn't know your domain. He'll <em>always</em> accept the work,
and you'll pay $truckload/hour trying to explain the most basic things to him, and failing. You need to actively ask about his
education and experience.</p>
<h2 id="summary">Summary</h2>
<p>Like most annoying things in life, patents aren't evil as much as they're absurd. Use them to your advantage.</p>]]></description>
      <comments>https://yosefk.com/cgi-bin/comments.cgi?post=blog/patents-how-and-why-to-get-them#comments</comments>
      <pubDate>Sat, 02 Jun 2018 00:00:00 +0000</pubDate>
      <dc:creator>Yossi Kreinin</dc:creator>
      <wfw:commentRss>https://yosefk.com/blog/patents-how-and-why-to-get-them.feed</wfw:commentRss>
    </item>
    <item>
      <title>Don't ask if a monorepo is good for you – ask if you're good enough for a monorepo</title>
      <link>https://yosefk.com/blog/dont-ask-if-a-monorepo-is-good-for-you-ask-if-youre-good-enough-for-a-monorepo.html</link>
      <description><![CDATA[<p>This is inspired by <a href="https://danluu.com/monorepo/">Dan Luu's post</a> on the advantages of a single big repository
over many small ones. That post is fairly old, and I confess that I'm hardly up to date on the state of tooling, both for
managing multiple repos and for dealing with one big one. But I'm going to make an argument which I think mostly works
regardless of the state of tooling on any given day:</p>
<ul>
<li>Monorepo is great if you're really good, but absolutely terrible if you're not that good.</li>
<li>Multiple repos, on the other hand, are passable for everyone – they're never great, but they're never truly terrible,
either.</li>
</ul>
<p>If you agree with the above, the choice is up to your personal philosophy. To me, for instance, it's a no-brainer – I'll
choose the passable thing which successfully withstands contact with apathetic mediocrity over the greater thing which falls
apart upon such contact in a heartbeat.</p>
<p>You might be different – you might believe in Good – and then you'll choose a monorepo, like Google, the ultimate force for
Good in technology (which is why they safeguard your personal data; you wouldn't want someone evil to have it – luckily, Google
can do no evil.) And I'm almost not kidding: the superpower which lets Google maintain the grassroots bureaucracy which I find
necessary to make monorepos work well is actually the same trait making you sufficiently delusional to chant, or at least to
have chanted "Don't Be Evil" entirely seriously. I don't have that. I am, to a first approximation, evil. <a href="https://yosefk.com/blog/what-worse-is-better-vs-the-right-thing-is-really-about.html">Worse is Better</a>.</p>
<p>But that's me – I'm not saying <em>you/your org</em> are Not So Good, or Evil. I'm only saying that <em>you should be open to
the possibility,</em> and that I don't see the implications of being Not So Good discussed as much as they deserve.</p>
<p>Why are monorepos terrible if you're not that good? Three reasons:</p>
<ol>
<li>Branching in</li>
<li>Modularity out</li>
<li>Tooling strained</li>
</ol>
<p>Let's discuss them in some detail.</p>
<h2 id="branching-getting-forked-by-your-worst-programmer">Branching: getting forked by your worst programmer</h2>
<p>In a Good team, you don't have multiple concurrent branches from which actual product deliveries are produced, and/or where
most people get to maintain these branches simultaneously for a long time. And you certainly can't have branching due to
outright atrocities, like someone adding a feature by killing a feature – for example, making the app work on Android, but
destroying the ability to build for iOS in the process.</p>
<p>But in a not-so-good team... you get the idea.</p>
<p>What do you do when you have a branch working on Android and another branch working on iOS and you have deliveries on both
platforms? You postpone the merge, and keep the fork. For how long do you postpone the merge? For as long as is necessary for
the dumbass who caused the fork to fix their handiwork, in parallel with delivering more features (which likely results in
digging a deeper hole to climb out of afterwards.) And the dumbass might take months, years, or forever.</p>
<p>The question then becomes, <em>what was forked</em>?</p>
<p>In a multi-repo world, the repo maintained by the team with the dumbass on it got forked. In a monorepo world, <em>the entire
code base got forked, and the entire org is now held hostage by the dumbass.</em> And you might think that this will result in a
lot of pressure to fix the problem, and you'd be wrong, for the same reasons that high murder rates don't cure themselves by
people putting pressure on whomever to lower them to some equilibrium level common to all human societies.</p>
<p>Some places have higher than average murder rates, and some places have have higher than average fork rates. And I argue that
a lot of places have fork rates which combine into a complete disaster with a monorepo. And you might not even realize how bad
the fork rate is at your place, because multiple repos largely shield you from the consequences. Or, more tragically, you might
not realize how bad your fork rate is because your monorepo is in its first couple of years, and you're sowing what you'll reap
in its next couple of years, when you'll have more code, more deliveries and more dumbasses.</p>
<p>With multiple repos, if <em>you</em> have your shit under control, and <em>your</em> repos have a single release branch with
a single timeline, all you have to do is to test against both of the dumbass's branches. But with a monorepo, you need to
maintain your code in 2 branches, with a growing share of everybody else's code morphing incompatibly in those branches, simply
because they exist. And very soon it will be more than 2 because there's more than a single dumbass, and good luck to you.</p>
<h2 id="modularity-demoted-from-a-norm-to-an-ideal">Modularity: demoted from a norm to an ideal</h2>
<p>Norms are mundane, but they are what is. Ideals are lofty, but they are merely what should be (and typically isn't.) If you
want to actually <em>have</em> something, you don't want it to be an ideal, like altruism – you want it to be a norm, like
wiping one's ass. If something is demoted from ass-wiping to altruism, that something will scarcely be found in the wild.</p>
<p>With multiple repos, modularity is the norm. It's not a <em>must</em> - you technically can have a repo depending on umpteen
other repos. But your teammates <em>expect</em> to be able to work with their repo with a minimal set of dependencies. They
<em>don't like</em> to have to clone lots of other repos, and to then worry about their versions <em>(in part because the
tooling support for this might be less than great)</em>.</p>
<p>In fact, a common multi-repo failure mode is that people expect <em>too few</em> dependencies and make <em>too many repos
which are too small</em> to host a <em>useful</em> self-contained system. Note that this failure mode is not lethal. It kinda
sucks to have this over-modularity with benefits of independence which turn out to be imaginary upon a closer look, and to have
people treat what essentially are internal APIs with way too much reverence, just because two modules which are extremely
tightly coupled <em>conceptually</em> are independent <em>technically,</em> in terms of cloning/building/testing. But it doesn't
kill you.</p>
<p>With a monorepo, modularity is a mere ideal. Everybody clones the whole thing. You're not supposed to add gratuitous
dependencies, but it's very easy to add such a dependency in terms of cloning, building and versioning, and nobody objects to
the dependency being added the way they would if they needed to clone more repos.</p>
<p>Of course in a Good team, needless dependencies would be weeded out in code reviews, and a Culture would evolve over time
avoiding needless dependencies. In a not-so-good team, your monorepo will grow into a single giant ball of circular
dependencies. Note that adding dependencies is infinitely easier than untangling them, much like forking is easier than merging,
with the difference that the gut-felt urgency to merge ("I can't maintain all your damned branches any longer!!") is far greater
and far more backed by simple self-interest than the urgency to improve the dependency structure.</p>
<h2 id="tooling-is-yours-better-than-the-standard">Tooling: is yours better than the standard?</h2>
<p>This part might age worse than the others, and might not be particularly up to date even now – what "standard" tools are
capable of changes over time. But generally speaking, a growing monorepo is likely to outgrow the standard version management
tools and methods, as well as <em>other</em> tools and methods dealing with your revision controlled code.</p>
<p>Google used to have a FUSE driver to avoid copying hundreds of millions of source lines at a time, and instead getting the
files on demand, when a directory is cd'd into. Facebook used to hack on hg to make it fast on its large monorepo. Maybe already
today, or some day, a growing number of off-the-shelf tools will scale to infinite monorepos without such investments. But it
sounds reasonable that there will always be tools and workflows which you will struggle to make work with a large monorepo
(starting with some script doing find/grep.)</p>
<p>With a bunch of small monorepos, you work with a small overall number of source files in your working directory, so you don't
need to tell your tools, "don't try to deal with the whole thing – instead only search this subset, or use this index etc. etc."
And you have tools these days which kinda sorta let you manage the revisions of multiple repositories (for instance, there's
Google's Repo.) And I think the result is very, very far from a great experience <em>potentially</em> afforded by a large
monorepo. But it also <em>never</em> breaks down as badly as a large monorepo outgrowing the abilities of tools, as well as the
ability of your local toolsmiths to find creative workarounds for these growth pains.</p>
<h2 id="summary">Summary</h2>
<p>Don't ask if a monorepo is good for you – ask if you're good enough for a monorepo. Personally, I don't have the guts to bet
on the supply of Goodness in a given org to remain sufficiently large over time to consistently avert the potential disasters of
monorepos. But that's just my personal outlook; if you want to compliment me, don't call me "smart," and definitely don't call
me "good" – I know my limits in these areas, and I take far more pride in knowing these limits than in the limits themselves;
so, to compliment me, call me "pragmatic." Yet a culture worthy of a monorepo absolutely can exist – just make sure yours
actually is one of those, and don't mistake your ideals for your norms.</p>]]></description>
      <comments>https://yosefk.com/cgi-bin/comments.cgi?post=blog/dont-ask-if-a-monorepo-is-good-for-you-ask-if-youre-good-enough-for-a-monorepo#comments</comments>
      <pubDate>Tue, 30 Jul 2019 00:00:00 +0000</pubDate>
      <dc:creator>Yossi Kreinin</dc:creator>
      <wfw:commentRss>https://yosefk.com/blog/dont-ask-if-a-monorepo-is-good-for-you-ask-if-youre-good-enough-for-a-monorepo.feed</wfw:commentRss>
    </item>
    <item>
      <title>I have risen</title>
      <link>https://yosefk.com/blog/i-have-risen.html</link>
      <description><![CDATA[<p>Hello to the readers still using RSS! I've moved the blog off WordPress to my own ugly publishing software, and will be
grateful if you report any glitches you see (posts or comments look bad on device X or feed reader Y, that sort of thing.)</p>
<p>This blog slowed down a lot in 2017, when I switched from a part-time programming position to a full-time senior management
position. Between the comment spam flood and the ancient pre-mobile design, it would take some doing to get the blog back into
shape; and between work and non-work stuff I had going, I didn't find time for said doing.</p>
<p>But I've gone back to programming a couple of years ago, and back to part-time a few months ago, and now the doing is done.
And do I have things to tell you!</p>]]></description>
      <comments>https://yosefk.com/cgi-bin/comments.cgi?post=blog/i-have-risen#comments</comments>
      <pubDate>Tue, 12 Mar 2024 00:00:00 +0000</pubDate>
      <dc:creator>Yossi Kreinin</dc:creator>
      <wfw:commentRss>https://yosefk.com/blog/i-have-risen.feed</wfw:commentRss>
    </item>
    <item>
      <title>refix: fast, debuggable, reproducible builds</title>
      <link>https://yosefk.com/blog/refix-fast-debuggable-reproducible-builds.html</link>
      <description><![CDATA[<p>There's a simple way to make your builds all of the following:</p>
<ul>
<li><strong>Reproducible</strong>/deterministic - same binaries always built from the same source, so you can cache build
outputs across users</li>
<li><strong>Debuggable</strong> - gdb, sanitizers, Valgrind, KCachegrind, etc. find your source code effortlessly</li>
<li><strong>Fast</strong> - the build time overhead is negligible, even compared to a blazing fast linker like <a href="https://github.com/rui314/mold">mold</a></li>
</ul>
<p>What makes it really fast is a small Rust program called <a href="https://github.com/yosefk/refix">refix</a> that
post-processes your build outputs (if you don't want to compile from source, <a href="https://yosefk.com/refix/">here's a static
Linux binary</a>.) Both the program and this document are written for the context of C/C++ source code compiled to native
binaries. But this can work with other languages and binary formats, too, and it should be easy to support them in
<code>refix</code>. (<em>In fact, it mostly supports them already...</em> you'll see.)</p>
<p>This "one weird trick" isn't already popular, not because the solution is hard, nor because the problem isn't painful.
Rather, it's not already popular because people widely consider it impossible for builds to be both debuggable and reproducible,
and standardize on workarounds instead. Since "established practices" are sticky, and especially so in the darker corners like
build systems<a class="footnote-ref" role="doc-noteref" href="#fn1" id="fnref1"><sup>1</sup></a>, we'll need to discuss not only
how to solve the problem, but also why solve it at all.</p>
<h2 id="the-curious-case-of-the-disappearing-source-files">The curious case of the disappearing source files</h2>
<p>Why are people so willing to give up their birthright - the effortless access to the source code of a debugged program? I
mean, build a "Hello, world" cmake project, and everything just works: gdb finds your source code, <code>assert</code> prints a
path you can just open in an editor, etc. "Source path" isn't even a thing.</p>
<p>Later on, the system grows, and the build slows down. So someone implements build artifact caching, in one of several
ways:</p>
<ul>
<li>A general-purpose distributed build cache, like Bazel's</li>
<li>Something for caching specific kinds of artifacts, like ccache</li>
<li>An entirely home-grown system - like running the build of user X in a build directory left previously by user Y at the build
server's local disk (and hoping that their source code is similar enough, so most object files needn't be rebuilt<a class="footnote-ref" role="doc-noteref" href="#fn2" id="fnref2"><sup>2</sup></a>)</li>
</ul>
<p>In any case, now that you need caching, you also need reproducible builds. Otherwise, you'd cache object files built by
different users, and you'd get different file paths and other stuff depending on which user built each object file. And we can
all agree that build caches are important, and pretty much force you to put relative paths into debug information and the value
of <code>__FILE__</code> (and some meaningless garbage into <code>__TIME__</code>, etc.)</p>
<p>But we can <em>also</em> agree that the <em>final binaries</em> which users actually run should have full source paths,
right? I mean, I know there are workarounds for finding the source files. We'll talk about them later; I'd say they don't really
work. Of course, the workarounds would be tolerable if they were inevitable. But they aren't.</p>
<p><strong>Why not fix the binary coming out of the build cache, so it points to the absolute path of the source files?</strong>
(The build system made an effort to detach the binary from the full source path, so that it can be cached. But now that the
binary has left the cache, we should "refix" it back to the source path of the version where it belongs.)</p>
<p>We'll look at 3 ways of refixing the binary to the source path - a thesis, an anti-thesis and a synthesis, as it were.</p>
<h2 id="thesis-debugedit---civilized-standard-and-format-aware">Thesis: <code>debugedit</code> - civilized, standard and
format-aware</h2>
<p>A standard tool for this is <a href="https://sourceware.org/debugedit/">debugedit</a>. The man page example does exactly the
"refixing" we're looking for:</p>
<pre style="background-color: #eeeeee;color: #222;overflow: auto;margin: 0 0 1.5385em 0;padding: 0.7692em;"><code>debugedit -b `pwd` -d /usr/lib/debug files...
    Rewrites path compliled into binary
    from current directory to /usr/lib/debug.</code></pre>
<p>Some Linux distributions use <code>debugedit</code> for building source files in some arbitrary location, and then make the
debug info point to wherever source files are installed when someone downloads them to debug the program.</p>
<p>If debugedit works for you, problem solved. It works perfectly when it does. However, when I tried it on a 3GB shared object
compiled from a C++ code base<a class="footnote-ref" role="doc-noteref" href="#fn3" id="fnref3"><sup>3</sup></a>, it ran for 30
seconds, and then crashed. If you, too find debugedit either slow or buggy for your needs, read on.</p>
<h2 id="anti-thesis-sed---nasty-brutish-and-short">Anti-thesis: <code>sed</code> - nasty, brutish, and short</h2>
<p>Why is debugedit's job hard (slow and bug-prone)? Mainly because it needs to grow or shrink the space reserved for each
replaced string. When you do such things, you need to move a lot of data (slow), and adjust who-knows-which offset fields in the
file (bug-prone.)</p>
<p>But what if the strings had the same length? Then we don't need to move or adjust anything, and we could, erm, we could
replace them with <code>sed</code>.</p>
<p>Here, then, is our nasty, brutish, and short recipe:</p>
<ul>
<li>Run <code>gcc</code> with these flags:
<pre style="background-color: #eeeeee;color: #222;overflow: auto;margin: 0 0 1.5385em 0;padding: 0.7692em;">-fdebug-prefix-map==MAGIC <i># for DWARF</i>
-ffile-prefix-map==MAGIC  <i># for __FILE__</i>
</pre></li>
<li>Make MAGIC long enough for any source path prefix you're willing to support.</li>
<li>Why the <code>==</code> in the flag? This invocation assumes that file paths are relative, so it remaps <em>the empty
string</em> to MAGIC, meaning, <code>dir/file.c</code> becomes <code>MAGICdir/file.c</code>. You can also pass
<code>=/prefix/to/remap=MAGIC</code>, if your build system uses absolute paths.</li>
<li>Use <code>sed</code> to replace MAGIC with your actual source path in the binary outputted by the build system.</li>
<li>If the source path is shorter than the length of MAGIC, pad it with forward slashes: <code>/////home/user/src/</code>. If
the source path is too long, the post-link step should truncate it, warn, and eventually be changed to outright fail. You don't
<em>really</em> need to support giant paths.</li>
</ul>
<p>Our post-link step thus becomes:</p>
<pre style="background-color: #eeeeee;color: #222;overflow: auto;margin: 0 0 1.5385em 0;padding: 0.7692em;"><code>sed -i 's/MAGIC/\/\/\/...\/user\/src\//g' binary</code></pre>
<p>The downside, on top of the source path length limit, is a trace of the brutishness making it into the output file. Namely,
you're going to see these extra forward slashes in some situations. We can't pad a prefix with an invisible character...
luckily, we can pad it with a character not changing the meaning of the path.</p>
<p>On the upside, compared to <code>debugedit</code>, the method using <code>sed</code> is:</p>
<ul>
<li><strong>More widely applicable</strong> - it, erm, "supports" all executable and debug information formats, as well as
archives and object files.</li>
<li><strong>More robust</strong> - not affected by input format complexity</li>
<li><strong>Faster</strong> - 10 seconds to process the 3GB binary (about the time it takes <code>mold</code> to link that
binary... yes, it's that good!)</li>
</ul>
<p>Is this fast enough? Depends on your binary sizes. If yours are big and you don't want to effectively double the link time,
our next and last method is for you.</p>
<h2 id="synthesis-refix---nasty-brutish-and-somewhat-format-aware">Synthesis: <code>refix</code> - nasty, brutish, and somewhat
format-aware</h2>
<p>Can we go faster than <code>sed</code>? We have two reasons to hope so:</p>
<ul>
<li><code>sed</code> is unlikely to be optimized specifically for replacing strings of equal size; it's not that common a use
case.</li>
<li>We don't actually need to go through the entire file. File paths only appear in some of the sections - <code>.rodata</code>
where strings are kept, and debug info sections. If we know enough about the file format to find the sections (which takes very
little knowledge), we can avoid touching most of the bytes in the file.</li>
</ul>
<p>But wait, isn't the giant binary built from C++ mostly giant because of the debug info? <em>Yes</em>, but it turns out that
most of the debug info sections <em>don't contain file paths</em>; only <code>.debug_line</code> and <code>.debug_str</code> do
and these are only about 10% of our giant file.</p>
<p>So the <code>refix</code> program works as follows:</p>
<ul>
<li>It <code>mmap</code>s the file, since it knows it never needs to move the data and can just overwrite the strings in
place.</li>
<li>For ELF files, it finds <code>.rodata</code>, <code>.debug_line</code> and <code>.debug_str</code>, and searches &amp;
replaces only within these. This handles executables, shared libraries (<code>*.so</code>) and object files
(<code>*.o</code>).</li>
<li>For <code>ar</code> archives, it finds the ELFs within the archive, then the sections it cares about within each ELF, and
searches &amp; replaces within these. This handles <code>lib*.a</code>.</li>
<li>For files which are neither ELFs nor archives of ELFs, <code>refix</code> just replaces everywhere as <code>sed</code>
would, but still faster because it's optimized for the same-sized source &amp; destination strings case.</li>
</ul>
<p>Thus, <code>refix</code> is:</p>
<ul>
<li><strong>Very fast</strong> - 50 ms on the 3GB binary, and 250 ms on the same binary in "sed mode" (meaning, if we remove the
ELF magic number, so <code>refix</code> is forced to replace everywhere and not just in the relevant sections.)</li>
<li><strong>Widely applicable</strong> - works on any file format where the file prefix isn't compressed and is otherwise "laid
bare"</li>
<li><strong>Robust</strong> - while it knows a bit about the binary file format, it's very, very little (enough to find the
sections it's interested in); it's hundreds of lines of code vs <code>debugedit</code>'s thousands. And you can always make it
run even less code by falling back to "sed mode."</li>
</ul>
<p>...with the sole downside being that, same as with sed, you might occasionally see the leading slashes in pathnames.</p>
<p>That's it, right? We're done? Well, maybe, but it's not always how it goes. People have questions. So here we go.</p>
<h2 id="q-a">Q &amp; A</h2>
<h3 id="why-do-this-we-already-have-a-system-for-finding-the-source-code.">Why do this? We already have a system for finding the
source code.</h3>
<p>First of all, it is worth saying that you <em>shouldn't</em> have any "system" for finding source code, because the tired,
stressed developer who was sent a core dump to urgently look at is entitled to having at least <em>this</em> part work entirely
effortlessly<a class="footnote-ref" role="doc-noteref" href="#fn4" id="fnref4"><sup>4</sup></a>.</p>
<p>But also, whatever system you do have ought to have issues:</p>
<ul>
<li>If you do not modify the cacheable, reproducible binaries coming out of the build system, then by definition your way to
find source code must rely on something inherent to a given source version, independent of who built it and where. Since you're
not going to embed the entire source code into the executable, you must rely on some sort of version information. What if the
program had uncommitted changes, which happens in debugging scenarios (someone built a version to test and someone else sent a
core dump from this version?)</li>
<li>"Well you're not supposed to get core dumps from versions with uncommitted changes, unless it's your local version that you
haven't given to anyone but are testing locally, so you know which version it is. You should only release versions externally
thru CI" - so giving anything to anyone to test is now considered "releasing externally" and must necessarily go thru CI, and
having trouble finding the source code is now a punishment for straying from proper procedure? How did this discussion, which
started at how build caches <em>speed up</em> the build, deteriorate to the point where we're telling developers to change how
they work, in ways which will <em>slow them down?</em></li>
<li>But OK, let's say I didn't "release" anything - instead I have 5 local versions I'm working with and they go thru test flows
and dump core - I'm now supposed to guess which core comes from which version, or develop my own "system" to know? (Some people
actually assume this won't happen because you can't run tests outside CI anyway, so you will submit a merge request in order to
run them. And they assume that because they use some testing infra intertwined with CI infra and most of their tests technically
can't run outside CI. And perhaps they don't even have machines to run on that aren't managed by Jenkins or some such to begin
with. But that is a horror story for another time. Here I'll just assume that we agree that it's good to be able to test changes
locally and debug easily.)</li>
<li>In the cases where the version info actually enables you to find the right code, the process can be made more tolerable by
developing a <code>gdb</code> Python extension that automatically tells gdb where the source code is based on the embedded
version info. Do you have this extension and a team maintaining it together with the build system?</li>
<li>Do you also have this automated for all the other tools (sanitizers, Valgrind, KCachegrind, VTune, whatever)? Do they all
even have a way to tell them where to look for source code? Is there a team handling this for all users, for every new tool used
by developers?</li>
</ul>
<p>I realize that these pain points aren't equally relevant to all organizations, and the extent of their relevance depends a
lot on the proverbial software lifecycle. (They also aren't equally relevant to everyone in a given organization. I claim that
the people suffering the most from this are the people doing the most debugging, and they are quite often very far removed from
any team that could ameliorate their suffering by improving "the system for finding source code" - so they're bound to suffer
for a long time.)</p>
<p>My main point, however, is that you needn't have any of these pain points <em>at all</em>. There's no tradeoff or price to
pay: your build is still reproducible and fast. Just make it debuggable with this one weird trick!</p>
<p>(Wow, I've been quite composed and civil here. I'm very proud of myself. Not that it's easy. I have strong <em>feelings</em>
about this stuff, folks!)</p>
<h3 id="what-about-non-reproducible-info-other-than-source-path-time-build-host-etc">What about non-reproducible info other than
source path (time, build host, etc)?</h3>
<p>I'm glad you asked! You can put all the stuff changing with every build into a separate section, reserved at build time and
filled after link time. You make the section with:</p>
<pre style="background-color: #eeeeee;color: #222;overflow: auto;margin: 0 0 1.5385em 0;padding: 0.7692em;"><code>char ver[SIZE] __attribute__((section(".ver"))) = {1};</code></pre>
<p>This reserves <code>SIZE</code> bytes in a section called <code>.ver</code>. It's non-<code>const</code> deliberately, since
if it's <code>const</code>, the OS will exclude it from core dumps (why save data to disk when it's guaranteed to be exactly the
same as the contents of the section in the binary?) But you might actually very much want to look at the content of this section
in a core dump, perhaps before looking at anything else. <strong>For instance, the content of this section can help you find the
path of the executable that dumped this core!</strong><a class="footnote-ref" role="doc-noteref" href="#fn5" id="fnref5"><sup>5</sup></a></p>
<p>(How do you find the section in the core dump without having an executable which the debugger could use to tell you the
address of <code>ver</code>? Like so: <code>strings core | grep MagicOnlyFoundInVer</code>. Nasty, brutish, and short. The point
is, having the executable path <em>in the core dump</em> is an additional and often major improvement on top of having full
source paths <em>in the executable...</em> because you need to find the executable before you can find the source!)</p>
<p>Additionally, our <code>ver</code> variable is deliberately initialized with one <code>1</code> followed by zeros, since if
it's all zeros, then <code>.ver</code> will be a "bss" section, the kind zeroed by the loader and without space reserved for it
in the binary. So you'd have nowhere to write your actual, "non-reproducible" version info at a post-link step.</p>
<p>After the linker is done, you can use <code>objcopy</code> to replace the content of <code>.ver</code>. But if you're using
<code>refix</code>, which already mmaps the file, you can pass it more arguments to replace ELF sections:</p>
<pre style="background-color: #eeeeee;color: #222;overflow: auto;margin: 0 0 1.5385em 0;padding: 0.7692em;"><code>refix exe old-prefix new-prefix --section .ver file</code></pre>
<p><code>refix</code> will put the content of <code>file</code> into <code>.ver</code>, or fail if the file doesn't have the
right length. (We don't move stuff around in the ELF, only replace.)</p>
<h3 id="what-about-compressed-debug-sections">What about compressed debug sections?</h3>
<p>What about them? I'm not sure why people use them, to be honest. I mean, who has <em>so many</em> executable files which they
don't want to compress as a whole (because they need to run them often, I presume), but they do want to compress the debug
sections to save space? Like, in what scenario <em>this</em> is your way to save enough space to even worry about it?</p>
<p>But, they could be supported rather nicely, I think, if you really care. You wouldn't be able to just blithely
<code>mmap</code> a file and replace inside it without updating any offset field in the file, but I think you could come close,
or rather stay very far away from doing seriously heavy lifting making this slow and bug-prone. Let's chat if you're interested
in this.</p>
<p>(I think maybe one problem is that some build caches have a file size limit? Like, something Bazel-related tops out at 2GB
since it's the maximal value of the Java int type?.. Let's talk about something else, this is making me very sad.)</p>
<h3 id="its-250-ms-on-generic-data.-and-you-still-did-the-elfar-thing-to-get-to-50-ms.-are-you-insane">It's 250 ms on generic
data. And you still did the ELF/ar thing to get to 50 ms. Are you insane?</h3>
<p>Well, it's 250 ms on a fast machine with a fast SSD. Some people have files on NAS, which can slow down the file access a
lot. In such cases, accessing 10x less of the <code>mmap</code>ed data will mitigate most of the NAS slowdown. You don't really
want to produce linker output on NAS, but it can be very hard to make the build system stop doing that, and I want people stuck
in this situation to at least have debuggable binaries without waiting even more for the build. So <code>refix</code> is
optimized for a slow filesystem.</p>
<p>But also, if it's not too much work, I like things to be fast. <a href="people-can-read-their-managers-mind.html">Insane or
not</a>, the people who make fast things are usually the people who like fast things, by themselves and not due to some
compelling reason, and I'm not sure I'm ashamed of maybe going overboard a bit; better safe than sorry. Like, I don't parse most
of the ELF file, which means I don't use the <code>Elf::parse</code> method from the <code>goblin</code> library, but instead I
wrote a 30 line function to parse just what I need.</p>
<p>This saves 300-350 ms, which, is it a lot? - maybe not. Will it become much more than that on a slower file system? I don't
know, it takes less time to optimize the problem away than answer this question. Did I think of slow file systems when doing it?
- not as much as I was just annoyed that my original C++ program, which the Rust program is a "clean room" open source
implementation of, takes 150 ms and the Rust one takes about 400 ms. Am I happy now that I got it down to 50 ms? Indeed!</p>
<p>(Why is Rust faster? Not sure; I think, firstly, GNU <code>memmem</code> is slower than <code>memchr::memmem::Finder</code>,
and secondly, I didn't use TBB in C++ but did use Rayon in Rust, because the speedup is marginal - you bottleneck on I/O - and I
don't want to complicate the build for small gains, but in Rust it's not complicated - just <code>cargo add rayon</code>.)</p>
<p>It often takes less time to just do the efficient thing than it takes to argue about the amount it would save relatively to
the inefficient thing. (But it's still more time than just going ahead and doing the inefficient thing without arguing. But even
that is not always the case. But most people who make fast things will usually just go for the efficient thing when they see it
regardless if it's the case, I think. IME the people who always argue about whether optimizations are worth it make big and slow
things in the end.)</p>
<h3 id="im-as-crazy-as-you-and-i-want-this-speedup-for-non-elf-executable-formats.">I'm as crazy as you, and I want this speedup
for non-ELF executable formats.</h3>
<p>Let's chat. The <code>goblin</code> library probably supports your format - shouldn't take more than 100-150 LOC to handle
this in <code>refix</code>.</p>
<h3 id="which-binaries-should-i-run-this-stuff-on">Which binaries should I run this stuff on?</h3>
<p>Anything delivered "outside the build system" for the use of people (who run programs / load shared libraries) or other build
systems (which link code against static libraries / object files.) And nothing "inside the build system", or it will ruin
caching.</p>
<p>I hope for your sake that you have a monolithic build where you build everything from source. But I wouldn't count on it;
quite often team A builds libraries for team B, which gets them from Artifactory or something wonderful like that. In that case,
you might start out with a bug where some libraries are shipped with the MAGIC as their source prefix instead of the real thing.
This is easy to fix though, and someone might even remind you with "what's this weird MAGIC stuff?"</p>
<p>(Somehow nobody used to ask "what's <code>/local/clone-87fg12eb/src</code>", when <em>that</em> was the prefix instead of
MAGIC. Note that even if you have this bug and keep MAGIC in some library files, <em>nobody is worse off</em> than previously
when it was <code>/local/clone-87fg12eb/src</code>. And once you fix it, they'll be <em>better</em> off.)</p>
<h3 id="ci-removes-the-source-after-building-it.-what-should-the-destination-source-prefix-be..">CI removes the source after
building it. What should the destination source prefix be?..</h3>
<p>And here I was, thinking that it's the build cache not liking absolute paths that was the problem... It turns out that we
have a bigger problems: the source is just nowhere to be found! <code>/local/clone-87fg12eb/src</code> is gone forever!</p>
<p>But actually, it makes sense for CI to build on the local disk in a temporary directory. In parallel with building, CI can
export the code to a globally accessible NAS directory. And at the end of the build, CI can refix the binaries to that NAS
directory. It's not good to <em>build</em> from NAS (or to NAS) - it's not only slow, but fails in the worst ways under load -
which is why a temporary local directory makes sense. But NAS is a great place for <em>debugging tools</em> to get source from -
widely accessible with no effort for the user.</p>
<p>Many organizations decide against NAS source exports, because it would be too easy for developers. Instead you're supposed to
download the source via HTTP, which is much more scalable than NAS, thus solving an important problem you don't have; plus, you
can make yourself some coffee while the entire source code (of which you'll only need the handful of files you'll actually open
in the debugger) is downloaded and decompressed.</p>
<p>In that case, your destination source prefix should be wherever the user downloads the files to. Decide on any local path
independent of the user name, and with version information encoded in it, so multiple versions can be downloaded concurrently.
Have a nice cup of coffee!</p>
<h3 id="what-should-the-root-path-length-limit-be">What should the root path length limit be?</h3>
<p>100 bytes.</p>
<h3 id="our-ci-produces-output-in-filerkubernetesdockergitlabjenkinspre-commitdepartmentteamdeveloperbranch-nametest-suite-namerepo-which-is-110-bytes.">Our
CI produces output in
<code>/filer/kubernetes/docker/gitlab/jenkins/pre-commit/department/team/developer/branch-name/test-suite-name/repo/</code>,
which is 110 bytes.</h3>
<p>Great! Now you have a reason to ask them to shorten it. I'm sure they'll get to it in a quarter or two, if you keep
reminding.</p>
<h3 id="our-ceos-preschooler-works-as-a-developer-insists-on-a-200-byte-prefix-and-wont-tolerate-the-build-failing.">Our CEO's
preschooler works as a developer, insists on a 200 byte prefix, and won't tolerate the build failing.</h3>
<p>Then truncate the path without failing the build. He won't find the source code easily, but he can't find it <em>already
today.</em> <strong>If there's one thing fixing the problem won't do, it's making anyone worse off.</strong> It <em>can't</em>
make you worse off, since the current situation leaves it nowhere worse to take you. It could only possibly take you from
<em>never</em> being able to easily find the source to <em>sometimes</em>, if not always, being able to find it.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Use <code>refix</code>, <code>sed</code> or <code>debugedit</code> to make your fast, reproducible builds also effortlessly
debuggable, so that it's trivial to find the source given an executable - and the executable given a core dump.</p>
<p>And please don't tell me it's OK for developers to roam the Earth looking for source code instead. It hurts my feelings!</p>
<p><em>Thanks to Dan Luu for reviewing a draft of this post.</em></p>

<section class="footnotes footnotes-end-of-document" role="doc-endnotes" id="footnotes">
<hr>
<ol>
<li id="fn1"><p>I don't mean "dark corners" in a bad way. I managed a build system team for a few years and am otherwise
interested in build systems, as evidenced by my writing this whole thing up. By "dark corners" I simply mean places invisible to
most of the organization unless something bad happens, so the risk of breaking things is larger than the reward for improving
them. It's quite understandable for such circumstances to beget a somewhat conservative approach.<a class="footnote-back" role="doc-backlink" href="#fnref1">↩︎</a></p></li>
<li id="fn2"><p>I've known more than one infrastructure team that did this "cross-user build directory reuse" without ever
hearing about each other. This method, while quite terrible in terms of optimization potential left on the table, owes its
depressing popularity to its high resilience to the terribleness of everything else (eg it doesn't mind poor network bandwidth
or even network instability, or the build flow not knowing where processes get their inputs and put their outputs; thus you can
use this approach with an almost arbitrarily bad build system and IT infrastructure.)<a class="footnote-back" role="doc-backlink" href="#fnref2">↩︎</a></p></li>
<li id="fn3"><p>Yes, a 3GB shared object compiled from a C++ code base. Firstly, shame on you, it's not nice to laugh at people
with problems. Secondly, no, it's not stupid to have large binaries. It's much more stupid to split everything into many tiny
shared objects, actually. It was always <em>theoretically</em> stupid, but now mold made it <em>actually</em> stupid, because
linkage used to be theoretically very fast, and now it's actually very fast. And nothing good happens from splitting things to
umpteen tiny .so's... but that's a topic for another time.<a class="footnote-back" role="doc-backlink" href="#fnref3">↩︎</a></p></li>
<li id="fn4"><p>I've been told, in all seriousness and by an extremely capable programmer involved in a build system at the
time, that "debugging should NOT be made easy; we should incentivize more design-time effort to cut on the debugging effort." To
this I replied that Dijkstra would have been very proud of him, same as <a href="https://lispy.wordpress.com/2008/10/24/lisp50-notes-part-v-interlisp-parc-and-the-common-lisp-consolidation-wars/">he was
very angry with Warren Teitelman</a>, whom he confronted for the crime of presenting a debugger with "how many bugs are we going
to tolerate," getting the immortal reply "7." And I said that we should make debugging easy for those first and only 7 bugs
we're going to tolerate.<a class="footnote-back" role="doc-backlink" href="#fnref4">↩︎</a></p></li>
<li id="fn5"><p>But what if this info gets overwritten, seeing how it's not <code>const</code>?.. if you're really worried about
this section getting overwritten, of all things, you can align its base address and size to the OS page size, and
<code>mprotect</code> it at init time. This is exactly what <code>const</code> would achieve, but without excluding the section
from core dumps.<a class="footnote-back" role="doc-backlink" href="#fnref5">↩︎</a></p></li>
</ol></section>]]></description>
      <comments>https://yosefk.com/cgi-bin/comments.cgi?post=blog/refix-fast-debuggable-reproducible-builds#comments</comments>
      <pubDate>Tue, 19 Mar 2024 00:00:00 +0000</pubDate>
      <dc:creator>Yossi Kreinin</dc:creator>
      <wfw:commentRss>https://yosefk.com/blog/refix-fast-debuggable-reproducible-builds.feed</wfw:commentRss>
    </item>
    <item>
      <title>Managers have no human rights</title>
      <link>https://yosefk.com/blog/managers-have-no-human-rights.html</link>
      <description><![CDATA[<p>Here are some thoughts which are often basically correct:</p>
<ul>
<li>Every time I try to do the right thing here, it's like the place actively resists it. Actually, forget "the right thing" -
it's whenever I try to do pretty much <em>anything.</em></li>
<li>Yesterday's "strategic" thing I toiled over just got tossed into the dumpster. And they expect me to be all excited about
the new "strategy" they coughed up?</li>
<li>My "colleagues" attack me overtly and covertly all the time, all the while maintaining a "professional" and even cheerful
demeanor - in effect, a gaslighting tactic. And in this sewage lagoon, I'm supposed to get work done?</li>
<li>The deadline is impossible, and everybody knows it. Why are we all pretending that we're trying to meet it, when we're
actually busy rehearsing our speeches blaming the inevitable failure on each other?</li>
</ul>
<p>I'm sure you can add a few similar observations of your own, which at various times &amp; places were fairly accurate. My
point in this writeup is that <strong>a manager doesn't get to whine about any of this</strong>, any more than a boxer gets to
whine about a broken nose. A <em>normal</em> person very much <em>does</em> get to whine about a broken nose, and it isn't
whining - it's grounds for a lawsuit in any reasonable jurisdiction. But when a boxer steps into the boxing ring, he obviously
forfeits the basic human right of not getting punched in the face.</p>
<p>Similarly, a <em>normal</em> person - the so-called "individual contributor", which I guess is what we call workers in the
age where mice and keyboards replaced hammers and sickles - a normal person can reasonably expect some basics from the
workplace:</p>
<ul>
<li>The place should help me get work done, and provide various physical, informational and social infrastructure for this
purpose.</li>
<li>The place should articulate a strategy which my work meaningfully fits into, and manage changes to this strategy carefully
and thoughtfully.</li>
<li>I am entitled to healthy human relationships in the workplace, and to management fostering an environment conducive of
healthy relationships forming, rather than abusive and adversarial ones.</li>
<li>The schedule should not demand the impossible, and certainly when I work hard to meet whatever deadline was set, I should
not worry about being blamed for the team missing the deadline in the end.</li>
</ul>
<p>The condition meeting the full set of these lofty requirements is sometimes called "psychological safety." So in short, the
individual contributor is entitled to psychological safety - in hammer &amp; sickle terms, <b><em>workers</em> should be able to
focus on work</b>.</p>
<p>And by "should," I don't mean it always actually happens. I just mean that when it doesn't, you can reasonably expect to
resolve the situation by quitting <em>the team</em> you're on, without having to find <em>a different line of work.</em> That,
as opposed to a manager, who can <em>only</em> resolve this situation by finding a different line of work.</p>
<p>Why isn't the manager entitled to the same human rights as everyone else? Well, first of all, he just <em>isn't</em>,
meaning, if you see a manager who complains about said human rights of his being violated a lot, you can be certain that he's
not going to stay in management for long; he'll either have the common sense to quit or he'll be "demoted" from this position
(whether it's actually a demotion, and which way in a hierarchy is up is a question in itself; some theoreticians postulate that
there's no up, only out - but in any case, the manager will stop being one.)</p>
<p>Now, if I do try to explain this fact, which managers usually get at the gut level without needing an explanation, the
analogies that come to mind are the minister of foreign affairs and the plumber. You cannot, as a minister of foreign affairs,
be sad and shocked over countries plotting against you, and maybe even preparing to attack you - nobody wants a perpetually
shocked minister of foreign affairs. And similarly, you cannot be a plumber if you're appalled by the sight, smell and tactile
properties of shit.</p>
<p>"Individual contributors" can be fairly non-competitive, certainly in an industry like computing where demand for workers
outstrips supply, there's enough work for everyone, and where you know a ton of trivia about your area that anyone else would
need lots of time to learn if they had to step into your shoes. It's not only desirable but very possible to find a place where
no colleague is going to fight you in order to add your area of responsibility to theirs, and thus get promoted.</p>
<p>Managers, on the other hand, are <em>always</em> basically low-grade<a class="footnote-ref" role="doc-noteref" href="#fn1" id="fnref1"><sup>1</sup></a> fighting each other, in the same way as countries always have conflicting interests, even if
they maintain what looks like cordial relationships. I mean it not as a statement about the character of managers, but as a
description of their condition. This condition follows, not from their character, but from various unfortunate facts of life -
for example, the fact that managers are assumed to be fungible and are hopelessly underinformed, and it gets worse with
rank.</p>
<p>The fungibility assumption means that a manager is always under a threat of losing "territory" to another manager, a reorg
making him a report of someone undesirable, or a straightforward replacement, much more than an IC, which creates a very
competitive environment. And the theoretical impossibility of managers being truly informed on the subjects falling under their
responsibility guarantees that their never-ending competition involves a lot of so-called "<a href="https://en.wikipedia.org/wiki/Misinformation">misinformation</a>, <a href="https://en.wikipedia.org/wiki/Disinformation">disinformation</a> and <a href="https://en.wikipedia.org/wiki/Malinformation">malinformation</a>." Of course, the manager's condition of eternal
competition fought on such wonderful terms <em>does</em> filter for character - and not in a way making a manager's day spent
with colleagues particularly pleasant.</p>
<p>In fact, all the unfortunate circumstances above - like the difficulty getting things done across the proverbial
(organizational) boundaries, deranged convulsions around "strategy," schedule chicken, and of course the scheming &amp; the
gaslighting - all this shit flows first and foremost from the competition between managers as well as the organization competing
in the external world.</p>
<p>ICs are entitled to managers shielding them from this shit - rarely fully, but quite often very considerably. Managers are
not entitled to this, because even if a 2nd, 3rd or Nth level manager would <em>like</em> to shield lower-ranking managers from
this (a rare, if laudable, desire), it's <em>not possible</em>, because there's just too much of it going on at the same time.
Of course it gets worse at higher ranks, but even a 1st level manager expecting <em>a positive atmosphere</em>, the kind that
ICs take for granted - "wow, cool stuff you've made there!" - will be sorely disappointed to learn that "please," "thank you,"
and "sorry" are gone from his day, replaced by "our requirement," "your commitment" and "customer escalation."</p>
<p>"If you want to make people happy, don't be a leader, sell ice cream," said a first-rate CEO and first-rate asshole Steve
Jobs. To this we might add, "If you want <em>people</em> to make <em>you</em> happy, consider selling ice cream, too." Or it
could be any sort of work which isn't management. A manager needs to be seriously driven by something other than having a nice
day, because that's just not gonna happen - the perfect drive <em>for you and your higher management</em> is "getting promoted,"
and the perfect drive for you to have <em>from the shareholders' POV</em> is "getting shit done." But motivation is a story for
another time.</p>
<h2 id="infrequently-raised-objections">Infrequently Raised Objections</h2>
<h3 id="i-am-a-manager-and-my-days-are-nice.">I am a manager, and my days are nice.</h3>
<p>Congratulations! You're either a great liar, including to yourself (all great liars start with themselves), or you're
completely indifferent to constant struggle and maybe you even enjoy it, or you're leading a very capable organization which
overdelivers often and underdelivers never (<em>how big is it?</em> A double digit number, tops?..), so you're enjoying what's
known as "peace through strength."</p>
<p>Rest assured that this condition is not fundamentally permanent (all strength is finite and always only goes so far), but do
enjoy it while it lasts, which can be for quite a while. Watch out for large reorgs, changes in the market / technology and
wider strategy (as I'm sure you do; only a competent manager gets to enjoy any duration of peace through strength.)</p>
<p>Or you're lucky.</p>
<h3 id="i-am-a-manager-and-my-manager-shields-me-from-this.">I am a manager, and my manager shields me from this.</h3>
<p>You're either effectively an IC, like "the leader of a team of 2 people under someone who makes every 3 people into a team,"
or you're working on something self-contained nobody needs and it will be soon canceled, or you have some infernal bond sealed
in goat's blood with your manager (or your manager has it with his), and when this thing explodes under external pressure, it
will be really ugly.</p>
<p>Or you're lucky.</p>
<h3 id="there-exist-organizations-free-from-the-dysfunction-you-describe.">There exist organizations free from the dysfunction
you describe.</h3>
<p>Like I said, "...or you're lucky." Sure, they exist. They're just rare, and usually don't remain that way as they grow (ever
heard "we're only hiring the best people?" Well, when you're hiring <em>a lot</em> of people, you're hiring <em>typical</em>
people, because there aren't this many "best" people readily available. Therefore, growth tends to bring about a regression to
the mean in all areas, including this one.) And most places are dysfunctional this way from day one, which by itself doesn't
prevent them from succeeding and growing; nor does a lack of this dysfunction guarantee success.</p>
<p>Speaking of which, I never quite understood the meaning of "dysfunctional" in "dysfunctional organizations." These
organizations definitely <em>function</em>; they generate trillions worth of world GDP. That they aren't fun to work at in a
managerial role might be true, but it is <em>not</em> their function to make it fun to work there in a managerial role. It is
the function of <em>you</em> to choose roles you can enjoy, and I hope the above can help some people with this.</p>
<h3 id="but-we-foster-a-non-hierarchical-culture-of-openness-and-curiosity.">But we foster a non-hierarchical culture of
openness and curiosity.</h3>
<p>If you're looking to decorate your office space, I have a suitcase full of hammers and sickles I brought from Soviet Russia.
I kept them to remind me of the old country, but your company sounds so awesome that I'll gladly send them to you.</p>
<p>Most deliberate attempts to improve upon the baseline outcome of people being people make things worse. You'll do everyone a
favor by going straight to the standard thing without going through a tedious cult phase first.</p>
<h3 id="calmly-accepting-dysfunction-does-not-a-good-manager-make.">Calmly accepting dysfunction does not a good manager
make.</h3>
<p>I didn't mean to imply that accepting and having a strategy for handling "dysfunction," or should I say the inevitable
consequences of the job description, is sufficient for being a good manager, whatever that means. I'm only saying that it's
<em>necessary</em> for being a manager <em>at all,</em> for any reasonable period of time and with a reasonable level of job
satisfaction.</p>
<h3 id="this-acceptance-is-not-a-binary-thing.-it-depends-on-how-bad-it-gets.">This "acceptance" is not a binary thing. It
depends on how bad it gets.</h3>
<p>It's binary in some and not binary in others. There are 3 types of people: people who binarily can't accept it; people who
binarily can, regardless of the depths of depravity reached; and people on whom it starts taking a toll at a certain level. Your
type can be predicted pretty well based on what motivates you, and we'll discuss it in an upcoming, very motivational piece on
motivation.</p>
<p><em>Thanks to Dan Luu and Tim Pote for reviewing a draft of this post.</em></p>
<p><strong>P.S.</strong> There exists a breed of "individual contributor" with a fancy title, such as a Principal Engineer, a
Fellow and other such. The desirability of the existence of these titles is a subject in its own right; in our context, their
relevance is that they largely strip you of human rights as much as management titles do. One hint of why this is so is their
visibility as a status marker and the consequences of this visibility - their scarcity and the resulting competition, in many
places fiercer than the fight for management titles. An exception to the rule is if you're The Guy Who Did X for some
serious-ass value of X, and you got your fancy title thanks to that value of X, regardless of the "technical track" promotion
politics.</p>

<section class="footnotes footnotes-end-of-document" role="doc-endnotes" id="footnotes">
<hr>
<ol>
<li id="fn1"><p>Hopefully<a class="footnote-back" role="doc-backlink" href="#fnref1">↩︎</a></p></li>
</ol></section>]]></description>
      <comments>https://yosefk.com/cgi-bin/comments.cgi?post=blog/managers-have-no-human-rights#comments</comments>
      <pubDate>Sun, 31 Mar 2024 00:00:00 +0000</pubDate>
      <dc:creator>Yossi Kreinin</dc:creator>
      <wfw:commentRss>https://yosefk.com/blog/managers-have-no-human-rights.feed</wfw:commentRss>
    </item>
    <item>
      <title>The state of AI for hand-drawn animation inbetweening</title>
      <link>https://yosefk.com/blog/the-state-of-ai-for-hand-drawn-animation-inbetweening.html</link>
      <description><![CDATA[<p>There are many potential ways to use AI<a class="footnote-ref" role="doc-noteref" href="#fn1" id="fnref1"><sup>1</sup></a>
(and computers in general) for 2D animation. I’m currently interested in a seemingly conservative goal: <strong>to improve the
productivity of a traditional hand-drawn full animation workflow by AI assuming responsibilities similar to those of a human
assistant.</strong></p>
<p>As a “sub-goal” of that larger goal, we’ll take a look at two recently published papers on animation “inbetweening” – the
automatic generation of intermediate frames between given keyframes. AFAIK these papers represent the current state of the art.
We’ll see how these papers and a commercial frame interpolation tool perform on some test sequences. We’ll then briefly discuss
the future of the broad family of techniques in these papers versus some substantially different emerging approaches.</p>
<p>There’s a lot of other relevant research to look into, which I’m trying to do - this is just the start. I should say that I’m
not “an AI guy” - or rather I am if you’re building an inference chip, but not if you’re training a neural net. I’m interested
in this as a programmer who could incorporate the latest tech into an animation program, and as an animator who could use that
program. But I’m no expert on this, and so <strong>I’ll be very happy to get feedback/suggestions</strong> through <a href="mailto:Yossi.Kreinin@gmail.com">email</a> or <a href="https://yosefk.com/cgi-bin/comments.cgi?post=blog/the-state-of-ai-for-hand-drawn-animation-inbetweening#comments">comments</a>.</p>
<p>I’ve been into animation tech since forever, and what’s possible these days is exciting. Specifically with inbetweening tech,
I think we’re still “not there yet”, and I think you’ll agree after seeing the results below. But we might well get there within
a decade, and maybe much sooner.</p>
<p>I think this stuff is very, very interesting! If you think so, too, we should get in touch. Doubly so if you want to work on
this. I am going to work on exactly this!</p>
<h2 id="motivation-and-scope">Motivation and scope</h2>
<p>Why is it interesting to make AI a 2D animator’s assistant, of all the things we could have it do (text to video, image to
video, image style transfer onto a video, etc.)?</p>
<ul>
<li><strong>An animator is an actor</strong>. The motion of a character reflects the implied physical and mental state of that
character. If the motion of a character, even one designed by a human, is fully machine-generated, it means that human control
over acting is limited; the machine is now the actor, and the human’s influence is limited to “directing” at best. It is
interesting to develop AI-assisted workflows where the human is still the actor.</li>
<li><strong>To control motion, the animator needs to draw several keyframes</strong> (or perhaps edit a machine-generated draft
- but with a possibility to erase and redraw it fully.) The range of ways to do “a sad walk” or “an angry, surprised head turn”
and the range of character traits influencing the acting is too wide for acting to be controlled via cues other than actually
drawing the pose.</li>
<li><strong>If a human is to be in control, “moving line art” is the necessary basis for any innovations in the appearance of
the characters.</strong> That’s because humans use a “light table”, aka “onion skin”, to draw moving characters, where you see
several frames overlaid on top of each other (like the frames of a bouncing ball sequence below). And it’s roughly not humanly
possible to “read” a light table unless the frames have the sharp edges of line art (believe me, I spent more time trying than I
should have.) Any workflow with human animators in control of motion needs to have line art at its basis, even if the final
rendered film looks very differently from the traditional line art style.</li>
</ul>
<p><img alt="ball-light-table.png" height="237" src="https://yosefk.com/img/inbetweening/ball-light-table.png" title="several frames on a light table" width="576" style="max-width: 100%;height: auto;"></p>
<ul>
<li><strong>The above gives the human a role similar to a traditional key animator, so it’s natural to give the machine the
roles of assistants.</strong> It could be that AI can additionally do some of the key animator’s work, so that less keyframes
are provided in some cases than you’d have to give a human assistant (and one reason for this could be your ability to quickly
get the AI to complete your work in 10-20 possible ways, and choose the best option, which is impractical with a human
assistant.) But the basic role of the human as a key animator would remain, and so the first thing to explore is the machine
taking over the assistant’s role.</li>
</ul>
<p>So I’m not saying that we can’t improve productivity beyond the “machine as the assistant” arrangement, nor that we must
limit ourselves to the traditional appearance of hand-drawn animation. I’m just saying that <strong>our conservative scope is
likely the right <em>starting point</em>, even if our final goals are more ambitious - at least as long as we want the human to
remain the actor.</strong></p>
<p>What would the machine do in an assistant’s role? Traditionally, assistants’ jobs include:</p>
<ul>
<li>Inbetweening (drawing frames between the key frames)</li>
<li>Cleanup (taking rough “pencil” sketches and “inking” them)</li>
<li>Coloring (“should” be trivial with a paint bucket tool, but surprisingly annoying around small gaps in the lines)</li>
</ul>
<p><strong>Our scope here is narrowed further by focusing exclusively on inbetweening</strong>. There’s no deep reason for this
beyond having to start somewhere, and inbetweening being the most “animation-y” assistant’s job, because it’s about movement. So
focusing our search on inbetweening is most likely to give results relevant to animation and not just “still” line art.</p>
<p><strong>Finally, in this installment, we’re going to focus on papers which <em>call themselves</em> “AI for animation
inbetweening” papers. </strong>It’s <em>not</em> obvious that any relevant “killer technique” has to come from a paper focusing
on this problem explicitly. We could end up borrowing ideas from papers on video frame interpolation, or video/animation
generation not designed for inbetweening, etc. In fact, I’m looking at some things like this. But again, let’s start
somewhere.</p>
<h2 id="preamble-testing-runway">Preamble: testing Runway</h2>
<p>Before looking at papers for the latest ideas, let’s check out <a href="https://runwayml.com/ai-tools/frame-interpolation/">Runway Frame Interpolation</a>. Together with Stability AI and the
CompVis group, Runway researchers were behind <a href="https://en.wikipedia.org/wiki/Stable_Diffusion">Stable Diffusion</a>, and
Runway is at the forefront of deploying generative AI for video.</p>
<p>Let’s test frame interpolation on a sneaky cartoony rabbit sequence. It’s good as a test sequence because it has both
fast/large and slower/smaller movement (so both harder and easier parts.) It also has both “flat 2D” body movement and “3D” head
rotation - one might say too much rotation… But rotation is good to test because it’s a big reason for doing full hand-drawn
animation. Absent rotation, you can split your character into “cut-out” parts, and animate it by <a href="https://duik.rxlab.guide/Angela/index.html">moving and stretching these parts</a>.</p>
<p><img alt="rabbit.gif" height="540" src="https://yosefk.com/img/inbetweening/rabbit.gif" width="576" style="max-width: 100%;height: auto;"></p>
<p>We throw away every second frame, ask Runway to interpolate the sequence, and after some conversions and a frame rate
adjustment (don’t ask), we get something like this:</p>
<p><img alt="rabbit-runway.gif" height="540" src="https://yosefk.com/img/inbetweening/rabbit-runway.gif" width="576" style="max-width: 100%;height: auto;"></p>
<p>This tool definitely isn’t currently optimized for cartoony motion. Here’s an example inbetween:</p>
<p><img alt="rabbit-runway-inbetween.png" height="540" src="https://yosefk.com/img/inbetweening/rabbit-runway-inbetween.png" width="576" style="max-width: 100%;height: auto;"></p>
<p>Now let’s try a similar sequence with a sneaky me instead of a sneaky rabbit. Incidentally, this is one of several styles I’m
interested in - something between live action and Looney Tunes, with this self-portrait taking live action maybe 15% towards
Looney Tunes:</p>
<p><img alt="myself.gif" height="540" src="https://yosefk.com/img/inbetweening/myself.gif" width="576" style="max-width: 100%;height: auto;"></p>
<p>Frame interpolation looks somewhat better here, but it’s still more <em>morphing</em> than <em>moving</em> from pose to
pose:</p>
<p><img alt="myself-runway.gif" height="540" src="https://yosefk.com/img/inbetweening/myself-runway.gif" width="576" style="max-width: 100%;height: auto;"></p>
<p>An example inbetween:</p>
<p><img alt="myself-runway-inbetween.png" height="540" src="https://yosefk.com/img/inbetweening/myself-runway-inbetween.png" width="576" style="max-width: 100%;height: auto;"></p>
<p>While the Frame Interpolation tool currently doesn’t work for this use case, I’d bet that Runway could solve the problem
quicker and better than most if they wanted to. Whether there’s a large enough market for this is another question, and it might
depend on the exact definition of “this.” Personally, I believe that a lot of good things in life cannot be “monetized”, a lot
of art-related things are in this unfortunate category, and I’m very prepared to invest time and effort into this without clear,
or even any prospects of making money.</p>
<p>In any case, we’ve got our test sequences, and we’ve got our motivation to look for better performance in recent papers.</p>
<h2 id="raster-frame-representation">Raster frame representation</h2>
<p>There’s a lot of work on AI for image processing/computer vision. It’s natural to borrow techniques from this deeply
researched space and apply them to line art represented as raster images.</p>
<p>There are a few papers doing this; AFAIK the state of the art with this approach is currently <a href="https://arxiv.org/abs/2111.12792">Improving the Perceptual Quality of 2D Animation Interpolation</a> (2022). Their <a href="https://github.com/ShuhongChen/eisai-anime-interpolator">EISAI GitHub repo</a> points to a colab demo and a Docker image
for running locally, which I did, and things basically Just Worked.</p>
<p>That this can even happen blows my mind. I remember how things worked 25 years ago, when you rarely had the code published,
and people implementing computer vision papers would occasionally swear that the paper is outright lying, because the described
algorithms don’t do and couldn’t possibly do what the paper says.</p>
<p>The sequence below shows <em>just</em> inbetweens produced by EISAI. Meaning, frame N is produced from the original frames
N-1 and N+1; there’s not a single original frame here. So this sequence isn’t directly comparable to Runway’s output.</p>
<p><img alt="myself-eisai.gif" height="540" src="https://yosefk.com/img/inbetweening/myself-eisai.gif" width="576" style="max-width: 100%;height: auto;"></p>
<p>I couldn’t quite produce the same output with Runway as with the papers (don’t ask.) If you care, this sequence is closer to
being comparable to Runway’s, if not fully apples to apples:</p>
<p><img alt="myself-eisai-comparable-to-runway.gif" height="540" src="https://yosefk.com/img/inbetweening/myself-eisai-comparable-to-runway.gif" width="576" style="max-width: 100%;height: auto;"></p>
<p>If you look at individual inbetweens, you’ll see that EISAI and Runway have similar difficulties - big changes between
frames, occlusion and deformation, and both do their best and worst in about the same places. One of the best inbetweens by
EISAI:</p>
<p><img alt="myself-eisai-best.png" height="540" src="https://yosefk.com/img/inbetweening/myself-eisai-best.png" width="576" style="max-width: 100%;height: auto;"></p>
<p>One of the worst:</p>
<p><img alt="myself-eisai-worst.png" height="540" src="https://yosefk.com/img/inbetweening/myself-eisai-worst.png" width="576" style="max-width: 100%;height: auto;"></p>
<p>The inbetweens are produced by <strong>forward-warping based on bidirectional flow estimation</strong>. “Flow estimation”
means computing, per pixel or region in the first keyframe, its most likely corresponding location in the other keyframe -
“finding where it went to” in the other image (if you have “two images of mostly the same thing,” you can hope to find parts
from one in the other.) “Warping” means transforming pixel data - for example, scaling, translating and rotating a region.
“Forward-warping by bidirectional flow estimation” means taking regions from both keyframes and warping them to put them “where
they belong” in the inbetween - which is halfway between a region’s position in the source image, and the position in the other
image that the flow estimation says this region corresponds to.</p>
<p>Warping by flow explains the occasional 3-4 arms and legs and 2 heads (it warps a left hand from both input images into two
far-away places in the output image, since the flow estimator found a wrong match, instead of matching the hands to each other.)
This also explains “empty space” patches of various sizes in the otherwise flat background.</p>
<p>Notably, warping by flow “gives up” on cases of occlusion up front (I mean cases where something is visible in one frame and
not in the other due to rotation or any other reason.) If your problem formulation is “let’s find parts of one image in the
other image, and warp each part to the middle position between where it was in the first and where we found it in the second” -
then the <em>correct</em> answer to “where did the occluded part move?” is “I don’t know; I can’t track something that isn’t
there.”</p>
<p>(Note that the system being an “AI” has no impact on this point. You could have a “traditional,” “hardcoded” system for
warping based on optical flow, or a differentiable one with trainable parameters (“AI”.) Let’s say we believe the trainable one
is likely to achieve better results. But training does not sidestep the question the parameters <em>of what</em> are being
trained, and what the model can, or can’t possibly do once trained.)</p>
<p>When the optical flow matches “large parts” between images correctly, you still have occasional issues due to both images
being warped into the result, with “ghosting” of details of fingers or noses or what-not (meaning, you see two slightly
different drawings of a hand at roughly the same place, and you see one drawing through the other, as if that other drawing was
a semi-transparent “ghost”.) A dumb question coming to my mind is if this could be improved through brute force, by “increasing
the resolution of the image” / having a “higher-resolution flow estimation,” so you have a larger number of smaller patches
capable of representing the deformations of details, because each patch is tracked and warped separately.</p>
<p>An interesting thing in this paper is <strong>the use of <a href="https://en.wikipedia.org/wiki/Distance_transform">distance
transform</a> to “create” texture for convolutional neural networks to work with for feature extraction.</strong> The distance
transform replaces every pixel value with the distance from that pixel’s coordinates to the closest black pixel. If you
interpret distances as black &amp; white pixel values, this gives “texture” to your line art in a way. The paper cites “Optical
flow based line drawing frame interpolation using distance transform to support inbetweenings” (2019) which also used distance
transform for this purpose.</p>
<p><img alt="distance-transform.png" height="509" src="https://yosefk.com/img/inbetweening/distance-transform.png" width="576" style="max-width: 100%;height: auto;"></p>
<p>If you’re dealing with 2D animation and you’re borrowing image processing/computer vision neural networks (hyperparameters
and maybe even pretrained weights, as this paper does with a few layers of ResNet), you will have the problem of “lack of
texture” - you have these large flat-color regions, and the output of every convolution on each pixel within the region is
obviously exactly the same. Distance transform gives some texture for the convolutions to “respond” to.</p>
<p>This amuses me in a “machine learning inside joke” sort of way. “But they told me that <em>manual feature engineering</em>
was over in the era of Deep Learning!” I mean, sure, a lot of it is over - you won’t see a paper on “the next <a href="https://en.wikipedia.org/wiki/Scale-invariant_feature_transform">SIFT</a> or <a href="https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients">HOG</a>.” But, apart from the “hyperparameters” (a name
for, basically, the entire network architecture) being manually engineered, and the various manual <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a> and what-not, what’s <a href="https://github.com/kornia/kornia">Kornia</a>, if not “a tool for manual feature engineering in a differentiable
programming context”? And I’m not implying that there’s anything wrong with it - quite the contrary, my point is that people
still do this because it works, or at least makes some things work better.</p>
<p>Before we move on to other approaches, let’s check how EISAI does on the rabbit sequence. I don’t care for the rabbit
sequence; I’m selfishly interested in the me sequence. But since unlike Runway, EISAI was trained on animation data, it seems
fair to feed it something more like the training data:</p>
<p><img alt="rabbit-eisai.gif" height="540" src="https://yosefk.com/img/inbetweening/rabbit-eisai.gif" width="576" style="max-width: 100%;height: auto;"></p>
<p>Both Runway and EISAI do worse on the rabbit, which has more change in hands and ears and walks a bit faster. It seems that
large movements, deformations and rotations affect performance more than “similarity to training data,” or at least similarity
in a naive sense.</p>
<h2 id="vector-frame-representation">Vector frame representation</h2>
<p>Instead of treating the input as images, you could work on a vector representation of the lines. AFAIK the most recent paper
in this category is <a href="https://arxiv.org/abs/2309.16643">Deep Geometrized Cartoon Line Inbetweening</a> (2023). Their <a href="https://github.com/lisiyao21/AnimeInbet">AnimeInbet GitHub repo</a> lets you reproduce the paper’s results. To run on your
own data, you need to hack the code a bit (at least I didn’t manage without some code changes.) More importantly, you need to
vectorize your input data somehow.</p>
<p>The paper doesn’t come with its own input drawing vectorization system, and arguably shouldn’t, since vector drawing programs
exist, and vectorizing raster drawings is a problem in its own right and outside the paper’s scope. The code in the paper has no
trouble getting input data in a vector representation because their line art dataset is produced from their dataset of moving 3D
characters, rendered with a “toon shader” or whatever the thing rendering lines instead of shaded surfaces is called. And since
the 2D points/lines come from 3D vertices/edges, you’re basically projecting a 3D vector representation into a 2D space and it’s
still a vector representation.</p>
<p><img alt="animeinbet-dataset-characters.png" height="288" src="https://yosefk.com/img/inbetweening/animeinbet-dataset-characters.png" width="576" style="max-width: 100%;height: auto;"></p>
<p>What’s more, <strong>this data set provides a kind of ground truth that you don’t get from 2D animation data sets - namely,
detailed correspondence between the points in both input frames and the ground truth inbetween frame</strong>. If your ground
truth is a frame from an animated movie, you only know that this frame is “the inbetween you expect between the previous frame
and the next.” But here, you know where every 3D vertex ended up in every image!</p>
<p>This correspondence information is used at training time - and omitted at inference time, or it would be cheating. So if you
want to feed data into AnimeInbet, you only need to vectorize this data into points connected by straight lines, without
worrying about vertex correspondence. The paper itself cites <a href="https://github.com/MarkMoHR/virtual_sketching">Virtual
Sketching</a>, itself a deep learning based system, as the vectorization tool they used for their own experiments in one of the
“ablation studies” (I know it’s idiomatic scientific language, but can I just say that I love this expression? “Please don’t
contribute to the project during the next month. We’re performing an ablation study of individual productivity. If the study
proves successful, you shall be ablated from the company by the end of the month.”)</p>
<p>There are comments in the AnimeInbet repo about issues using Virtual Sketching; mine was that some lines partially
disappeared (could be my fault for not using it properly.) I ended up writing some neanderthal-style image processing code <a href="https://en.wikipedia.org/wiki/Topological_skeleton">skeletonizing</a> the raster lines, and then <a href="https://en.wikipedia.org/wiki/Flood_fill">flood-filling</a> the skeleton and connecting the points while flood-filling.
I’d explain this at more length if it was more than a one-off hack; for what it’s worth, I <em>think</em> it’s reasonably
correct for present purposes. (My “testing” is that when I render my vertices and the lines connecting them and eyeball the
result, no obviously stupid line connecting unrelated things appears, and no big thing from the input raster image is clearly
missing.)</p>
<p>This hacky “vectorization” code (might need more hacking to actually use) is in <a href="https://github.com/yosefk/AnimationPapers">Animation Papers GitHub repo</a>, together with other code you might use to run
AnimeInbet on your data.</p>
<p>Results on our test sequences:</p>
<p><img alt="myself-animeinbet.gif" height="576" src="https://yosefk.com/img/inbetweening/myself-animeinbet.gif" width="576" style="max-width: 100%;height: auto;"></p>
<p>The rabbit is harder for AnimeInbet, similarly to the others. For example, the ears are completely destroyed by the head
turn, as usual:</p>
<p><img alt="rabbit-animeinbet.gif" height="576" src="https://yosefk.com/img/inbetweening/rabbit-animeinbet.gif" width="576" style="max-width: 100%;height: auto;"></p>
<p>The worst and the best inbetweens occur in pretty much the same frames:</p>
<p><img alt="myself-animeinbet-worst.png" height="576" src="https://yosefk.com/img/inbetweening/myself-animeinbet-worst.png" width="576" style="max-width: 100%;height: auto;"></p>
<p><img alt="myself-animeinbet-best.png" height="576" src="https://yosefk.com/img/inbetweening/myself-animeinbet-best.png" width="576" style="max-width: 100%;height: auto;"></p>
<p>Visually notable aspects of AnimeInbet’s output compared to the previous systems we’ve seen:</p>
<ul>
<li><strong>AnimeInbet doesn’t blur lines</strong>. It might <em>shred</em> lines on occasion, but you don’t <em>blur</em>
vector lines like you blur pixels. (You very much <em>can</em> put a bunch of garbage lines into the output, and AnimeInbet is
pretty good at <em>not</em> doing that, but this capability belongs to our next item. Here we’ll just note that raster-based
systems didn’t quite “learn” to avoid line blurring, which this system avoids by design.)</li>
<li><strong>AnimeInbet seems quite good at matching small details and avoiding ghosting/copying the same thing twice from both
images.</strong> This is not something that can salvage bad inbetweens, but it makes good inbetweens better; in the one above,
the pants and the hands are examples where small detail is matched better than in the raster systems.</li>
<li><strong>For every part, AnimeInbet either finds a match or removes it from the output.</strong> The paper formulates
inbetweening as a graph matching problem (where vertices are the nodes and the lines connecting them are edges.) Parts without a
match are marked as invisible. This doesn’t “solve” occlusion or rotation, but it tends to keep you from putting stuff into the
output that the animator needs to erase and redraw afterwards. This makes good inbetweens marginally better; for bad inbetweens,
it makes them “less funny” but probably not much more usable (you get 2 legs instead of 4, but they’re often <em>not the right
legs;</em> and you can still get a head with two foreheads as in the bad inbetween above.)</li>
</ul>
<p>AnimeInbet has a comprehensive evaluation of their system vs other systems (EISAI and VFIformer as well as FILM and RIFE,
video interpolation rather than specifically animation inbetweening systems.) According to their methodology (where they use
their own test dataset), their system comes out ahead by a large margin. In my extremely small-scale and qualitative testing,
I’d say that it looks better, too, though perhaps less dramatically.</p>
<p>Here we have deep learning with a model and input data set tailored carefully to the problem - something I think you won’t
see as often as papers reusing one or several pretrained networks, and combining them with various adaptations to apply to the
problem at hand. My emotional reaction to this approach appearing to do better than ideas borrowed from “general image/video AI
research” is mixed.</p>
<p>I like “being right” (well, vaguely) about AI <em>not</em> being “general artificial intelligence” but a set of techniques
that you need to apply carefully to build a system for your needs, instead of just throwing data into some giant general-purpose
black box - this is something I like going on about, maybe more than I should given my level of understanding. As a prospective
user/implementer looking for “the next breakthrough paper,” however, it would be better for me if ideas borrowed from “general
video research” worked great, because there’s so many of them compared to the volume of “animation-focused research.”</p>
<p>I mean, Disney already fired its hand-drawn animation department years ago. If the medium is to be revived (and people even
caring about it aren’t getting any younger), it’s less likely to happen through direct investment into animation than as a
byproduct of other, more profitable things. I guess we’ll see how it goes.</p>
<h2 id="applicability-of-2d-feature-matching-techniques">Applicability of “2D feature matching” techniques</h2>
<p>No future improvement of the techniques in both papers can possibly take care of “all of inbetweening,” because occlusion and
rotation happen a lot, and do not fit these papers’ basic approach of <strong>matching 2D features</strong> in the input frames.
And even the best inbetweens aren’t quite usable as is. But they could be used with some editing, and it could be easier to edit
them than draw the whole thing from scratch.</p>
<p>An encouraging observation is that <strong>machines struggle with big changes and people struggle with small changes, so they
can complement each other well</strong>. A human is better at (and less bored by) drawing an inbetween between two keyframes
which look very different than drawing something very close to both input frames and putting every line at juuuuust the right
place. If machines can help handle the latter kind of work, even with some editing required, that’s great!</p>
<p>It’s very interesting to look into approaches that <em>can</em> in fact handle more change between input frames. For example,
check out the middle frame below, generated from the frames on its left and right:</p>
<p><img alt="yoga-mat.png" height="158" src="https://yosefk.com/img/inbetweening/yoga-mat.png" width="576" style="max-width: 100%;height: auto;"></p>
<p>This is from <a href="https://time-reversal.github.io/">Explorative Inbetweening of Time and Space</a> (2024); they say the
code is coming soon. It does have some problems with occlusion (look at the right arm in the middle image.) But it seems to only
struggle when showing something that is occluded <em>in both input frames</em> (for example, the right leg is fine, though it’s
largely occluded in the image on the left.) This is a big improvement over what we’ve seen above, or right below (this is one
frame of Runway’s output, where one right leg slowly merges into the left leg, while another right leg is growing):</p>
<p><img alt="yoga-mat-runway.png" height="321" src="https://yosefk.com/img/inbetweening/yoga-mat-runway.png" width="576" style="max-width: 100%;height: auto;"></p>
<p>But what’s even more impressive - extremely impressive - is that the system decided that <em>the body would go up before
going back down</em> between these two poses! (Which is why it’s having trouble with the right arm in the first place! A feature
matching system wouldn’t have this problem, because it wouldn’t realize that in the middle position, the body would go up, and
the right arm would have to be somewhere. Struggling with things not visible in either input keyframe is a good problem to have
- it’s evidence of knowing these things <em>exist,</em> which demonstrates quite the capabilities!)</p>
<p>This system clearly learned a lot about three-dimensional real-world movement behind the 2D images it’s asked to interpolate
between. Let’s call approaches going in this direction “<strong>3D motion reconstruction</strong>” techniques (and I apologize
if there’s better, standard terminology / taxonomy; I’d use it if I knew it.)</p>
<p>My point here, beyond eagerly waiting for the code in this paper, is that feature matching techniques might remain
interesting in the long term, <em>precisely because “they don’t understand what’s going on in the scene.”</em> Sure, they
clearly don’t learn “how a figure moves or looks like.” But this gives some hope that what they <em>can</em> do - handling small
changes - will work <em>on more kinds of inputs</em>. Meaning, a system that “learned human movement” might be less useful for
an octopus sequence than a system that “learned to match patches of pixels, or graphs of points connected by lines.” So falling
back on 2D feature matching could remain useful for a long time, even once 3D motion reconstruction works great on the kinds of
characters it was trained on.</p>
<h2 id="conclusion">Conclusion</h2>
<p>I think we can agree that animation inbetweening doesn’t quite work at the moment, though it might already be useful for
inbetweening small movements, which is otherwise a painstaking process for a human. I think we can also agree that it’s
reasonable to hope it will be production-ready quite soon, and emerging inbetweening systems which “understand and reconstruct
movement,” beyond “matching image features,” are one reason to be hopeful.</p>
<p>In future installments, I hope to look into more techniques for inbetweening, and the closely related question of what
animators need to control inbetweening, beyond just giving the system two keyframes. <strong>Human inbetweeners certainly get
more input than pairs of keyframes.</strong> This makes me believe that it’s not just the <em>plausibility</em> of the
inbetweens you produce, but their <em>controllability</em> which is going to determine “the winning technique.”</p>
<p><em>Thanks to Dan Luu for reviewing a draft of this post.</em></p>

<section class="footnotes footnotes-end-of-document" role="doc-endnotes" id="footnotes">
<hr>
<ol>
<li id="fn1"><p>I miss the time when they called it machine learning rather than artificial intelligence, and the milder, calmer
economic conditions which were a moderating influence on terminology (in the end, whether it’s called ML or AI is an investors’
preference.) But I’m giving up and calling it AI, since at this point calling it ML is more a readability issue than anything
else.<a class="footnote-back" role="doc-backlink" href="#fnref1">↩︎</a></p></li>
</ol></section>]]></description>
      <comments>https://yosefk.com/cgi-bin/comments.cgi?post=blog/the-state-of-ai-for-hand-drawn-animation-inbetweening#comments</comments>
      <pubDate>Wed, 17 Apr 2024 00:00:00 +0000</pubDate>
      <dc:creator>Yossi Kreinin</dc:creator>
      <wfw:commentRss>https://yosefk.com/blog/the-state-of-ai-for-hand-drawn-animation-inbetweening.feed</wfw:commentRss>
    </item>
    <item>
      <title>A 100x speedup with unsafe Python</title>
      <link>https://yosefk.com/blog/a-100x-speedup-with-unsafe-python.html</link>
      <description><![CDATA[<p>We're going to speed up some numpy code by 100x using "unsafe Python." Which is not quite the same as unsafe Rust, but it's a
bit similar, and I'm not sure what else to call it... you'll see. It's not something you'd use in most Python code, but it's
handy on occasion, and I think it shows "the nature of Python” from an interesting angle.</p>
<p>So let's say you use <a href="https://pyga.me/">pygame</a> to write a simple game in Python.</p>
<p>(Is pygame the way to go today? I'm not the guy to ask; to its credit, it has a very simple screen / mouse / keyboard APIs,
and is quite portable because it's built on top of <a href="https://www.libsdl.org/">SDL</a>. It runs on the major desktop
platforms, and with a bit of fiddling, you can run it on Android using <a href="https://buildozer.readthedocs.io/en/latest/">Buildozer</a>. In any case, pygame is just one real-life example where a
problem arises that "unsafe Python" can solve.)</p>
<p>Let us furthermore assume that you're resizing images a lot, so you want to optimize this. And so you discover the somewhat
unsurprising fact that <a href="https://opencv.org/">OpenCV</a>'s resizing is faster than pygame's, as measured by the following
simple microbenchmark:</p>
<pre style="background-color: #eeeeee;color: #222;overflow: auto;margin: 0 0 1.5385em 0;padding: 0.7692em;">from contextlib import contextmanager
import time

@contextmanager
def Timer(name):
    start = time.time()
    yield
    finish = time.time()
    print(<i><b>f'{name} took {finish-start:.4f} sec'</b></i>)

import pygame as pg
import numpy as np
import cv2

IW = 1920
IH = 1080
OW = IW // 2
OH = IH // 2

repeat = 100

isurf = pg.Surface((IW,IH), pg.SRCALPHA)
with Timer(<i><b>'pg.Surface with smoothscale'</b></i>):
    for i in range(repeat):
        <b>pg.transform.smoothscale</b>(isurf, (OW,OH))

def cv2_resize(image):
    return cv2.resize(image, (OH,OW), interpolation=cv2.INTER_AREA)

i1 = np.zeros((IW,IH,3), np.uint8)
with Timer(<i><b>'np.zeros with cv2'</b></i>):
    for i in range(repeat):
        o1 = <b>cv2_resize</b>(i1)
</pre>
<p>This prints:</p>
<pre style="background-color: #eeeeee;color: #222;overflow: auto;margin: 0 0 1.5385em 0;padding: 0.7692em;">pg.Surface with smoothscale took <b>0.2002</b> sec
np.zeros with cv2 took <b>0.0145</b> sec
</pre>
<p>Tempted by the nice 13x speedup reported by the mircobenchmark, you go back to your game, and use
<code>pygame.surfarray.pixels3d</code> to get zero-copy access to the pixels as a numpy array. Full of hope, you pass this array
to <code>cv2.resize</code>, and observe that everything got much <em>slower</em>. Dammit! "Caching," you think, "or something.
Never trust a mircobenchmark!"</p>
<p>Anyway, just in case, you call cv2.resize on the pixels3d array in your mircobenchmark. Perhaps the slowdown will
reproduce?..</p>
<pre style="background-color: #eeeeee;color: #222;overflow: auto;margin: 0 0 1.5385em 0;padding: 0.7692em;">i2 = <b>pg.surfarray.pixels3d</b>(isurf)
with Timer(<b><i>'pixels3d with cv2'</i></b>):
    for i in range(repeat):
        o2 = cv2_resize(i2)
</pre>
<p>Sure enough, this is very slow, just like you saw in your larger program:</p>
<pre style="background-color: #eeeeee;color: #222;overflow: auto;margin: 0 0 1.5385em 0;padding: 0.7692em;">pixels3d with cv2 took <b>1.3625</b> sec
</pre>
<p>So 7x slower than smoothscale - and more shockingly, almost <strong>100x</strong> slower than cv2.resize called with
numpy.zeros! What gives?! Like, we have two zero-initialized numpy arrays <strong>of the same shape and datatype.</strong> And
of course the resized output arrays have the same shape &amp; datatype, too:</p>
<pre style="background-color: #eeeeee;color: #222;overflow: auto;margin: 0 0 1.5385em 0;padding: 0.7692em;">print(<b><i>'i1==i2 is'</i></b>, np.all(i1==i2))
print(<b><i>'o1==o2 is'</i></b>, np.all(o1==o2))
print(<b><i>'input shapes'</i></b>, i1.shape,i2.shape)
print(<b><i>'input types'</i></b>, i1.dtype,i2.dtype)
print(<b><i>'output shapes'</i></b>, o1.shape,o2.shape)
print(<b><i>'output types'</i></b>, o1.dtype,o2.dtype)
</pre>
<p>Just like you'd expect, this prints that everything is the same:</p>
<pre style="background-color: #eeeeee;color: #222;overflow: auto;margin: 0 0 1.5385em 0;padding: 0.7692em;"><code>i1==i2 is True
o1==o2 is True
input shapes (1920, 1080, 3) (1920, 1080, 3)
input types uint8 uint8
output shapes (960, 540, 3) (960, 540, 3)
output types uint8 uint8</code></pre>
<p>How could a function run 100x more slowly on one array relatively to the other, seemingly identical array?.. I mean, you
would hope SDL <em>wouldn't</em> allocate pixels in some particularly slow-to-access RAM area - even though it theoretically
<em>could</em> do stuff like that, with a little help from the kernel (like, create a non-cachable memory area or something.) Or
is the surface stored in GPU memory and we're going thru PCI to get every pixel?!.. It doesn't work this way, <em>does it?</em>
- is there some horrible memory coherence protocol for these things that I missed?.. And if not - if it's the same kind of
memory of the same shape and size with both arrays - what's different that costs us a 100x slowdown?..</p>
<p>It turns out... And I confess that I only found out by accident, after giving up on this<a class="footnote-ref" role="doc-noteref" href="#fn1" id="fnref1"><sup>1</sup></a> and moving on to something else. Entirely incidentally, that other thing
involved passing numpy data to C code, so I had to learn what this data looks like from C. So, it turns out that the shape and
datatype aren't all there is to a numpy array:</p>
<pre style="background-color: #eeeeee;color: #222;overflow: auto;margin: 0 0 1.5385em 0;padding: 0.7692em;">print(<b><i>'input strides'</i></b>,i1.strides,i2.strides)
print(<b><i>'output strides'</i></b>,o1.strides,o2.strides)
</pre>
<p>Ah, <em>strides.</em> Same in the output arrays, but very different in the input arrays:</p>
<pre style="background-color: #eeeeee;color: #222;overflow: auto;margin: 0 0 1.5385em 0;padding: 0.7692em;">input strides <b>(3240, 3, 1) (4, 7680, -1)</b>
output strides (1620, 3, 1) (1620, 3, 1)
</pre>
<p>As we'll see, this difference between the strides does in fact fully account for the 100x slowdown. Can we fix this? We can,
but first, the post itself will need to seriously slow down to explain these strides, because they're so weird. And then we'll
snatch our 100x right back from these damned strides.</p>
<h2 id="numpy-array-memory-layout">numpy array memory layout</h2>
<p>So, what's a "stride"? A stride tells you how many bytes you have to, well, stride from one pixel to the next. For instance,
let's say we have a 3D array like an RGB image. Then given the array base pointer and the 3 strides, the address of
<code>array[x,y,z]</code> will be <code>base + x*xstride + y*ystride + z*zstride</code> (where with images, z is one of 0, 1 or
2, for the 3 channels of an RGB image.)</p>
<p>In other words, <strong>the strides define the layout of the array in memory</strong>. And for better or worse, <strong>numpy
is very flexible with respect to what this layout might be</strong>, because it supports many different stride values for a
given array shape &amp; datatype.</p>
<p>The two layouts at hand - numpy's default layout, and SDL's - are... well, I don't even know which of the two offends me
more. As you can see from the stride values, the layout numpy uses by default for a 3D array is
<code>base + x*3*height + y*3 + z</code>.</p>
<p><img alt="numpy-layout.png" height="178" src="https://yosefk.com/img/numpy-perf/numpy-layout.png" width="576" style="max-width: 100%;height: auto;"></p>
<p>This means that the RGB values of a pixel are stored in 3 adjacent bytes, and the pixels of a <em>column</em> are stored
contiguously in memory - a <a href="https://en.wikipedia.org/wiki/Row-_and_column-major_order">column-major order</a>. And I,
for one, find this <em>offensive</em>, because images are <em>traditionally</em> stored in a row-major order, in particular,
image sensors send them this way (and <em>capture</em> them this way, as you can see from the <a href="https://en.wikipedia.org/wiki/Rolling_shutter">rolling shutter</a> - every <em>row</em> is captured at a slightly
different time, not <em>column.</em>)</p>
<p>"Why, we <em>do</em> follow that respected tradition as well," say popular numpy-based image libraries. "See for yourself -
save an array of shape <code>(1920, 1080)</code> to a PNG file, and you'll get a 1080x1920 image." Which is true, and of course
makes it even worse: if you index with <code>arr[x,y]</code>, then x, aka dimension zero, actually corresponds to <em>the
vertical dimension</em> in the corresponding PNG file, and y, aka dimension one, corresponds to <em>the horizontal
dimension.</em> And thus numpy array columns correspond to PNG image rows. Which makes the numpy image layout "row-major" in
some sense, at the cost of x and y having the opposite of their usual meaning.</p>
<p>...Unless you got your numpy array from a pygame Surface object, in which case x actually <em>does</em> index into the
horizontal dimension. And so saving <code>pixels3d(surface)</code> with, say, imageio will produce a <em>transposed</em> PNG
relatively to the PNG created by <code>pygame.image.save(surface)</code>. And in case adding <em>that</em> insult to the injury
wasn't enough, cv2.resize gets a <code>(width, height)</code> tuple as the destination size, producing an output array of shape
<code>(height, width)</code>.</p>
<p>Against the backdrop of these insults and injuries, SDL has an inviting, civilized-looking layout where x is x, y is y, and
the data is stored in an honest row-major order, for all the meanings of "row." But then upon closer look, the layout just
tramples all over my feelings: <code>base + x*4 + y*4*width - z</code>.</p>
<p>Like, the part where we have 4 in the strides instead of 3 as expected for an RGB image - I can get that part. We did ask for
an <em>RGBA</em> image, with an alpha channel, when we passed <code>SRCALPHA</code> to the Surface constructor. So I guess it
keeps the alpha channel together with the RGB pixels, and the 4 in the strides is needed to skip the As in RBGA. But then why,
may I ask, are there separate <code>pixels3d</code> and <code>pixels_alpha</code> functions? It's always annoying to have to
deal with RGB and alphas separately when using numpy with pygame surfaces. Why not a single <code>pixels4d</code>
function?..</p>
<p>But OK, the 4 instead of the 3 I could live with. But a zstride of -1? MINUS ONE? You start at the address of your Red pixel,
and to get to Green, you walk back one byte?! Now you're just fucking with me.</p>
<p>It turns out that SDL supports both RGB and BGR layout (in particular, apparently surfaces loaded from files are RGB, and
those created in memory are BGR?.. or is it even hairier than this?..) And if you use pygame's APIs, you needn't worry about RGB
vs BGR, the APIs handle it transparently. If you use <code>pixels3d</code> for numpy interop, you <em>also</em> needn't worry
about RGB vs BGR, because numpy's flexibility with strides lets pygame give you an array that <em>looks</em> like RGB despite it
being BGR in memory. For that, z stride is set to -1, and the base pointer of the array points to the first pixel's red value -
two pixels ahead of where the array memory starts, which is where the first pixel's <em>blue</em> value is.</p>
<p><img alt="SDL-layout.png" height="178" src="https://yosefk.com/img/numpy-perf/SDL-layout.png" width="576" style="max-width: 100%;height: auto;"></p>
<p>Wait a minute... <strong><em>now</em> I get why we have pixels3d and pixels_alpha but no pixels4d!!</strong> Because SDL has
RGBA and BGRA images - <em>BGRA, not ABGR</em> - and you can't make BGRA data look like an RGBA numpy array, no matter what
weird values you use for strides. There's a limit to layout flexibility... or rather, there really isn't any limit beyond the
limits of computability, but thankfully numpy stops at configurable strides and doesn't let you specify a generic callback
function <code>addr(base, x, y, z)</code> for a fully programmable layout<a class="footnote-ref" role="doc-noteref" href="#fn2" id="fnref2"><sup>2</sup></a>.</p>
<p>So to support RGBA and BGRA transparently, pygame is forced to give us 2 numpy arrays - one for RGB (or BGR, depending on the
surface), and another for the alpha. And these numpy arrays have the right <em>shape</em>, and let us access the right
<em>data</em>, but their <em>layouts</em> are very different from normal arrays of their shape.</p>
<p>And <strong>different memory layout can definitely explain major differences in performance</strong>. We could try to figure
out exactly why the performance difference is almost 100x. But when possible, I prefer to just get rid of garbage, rather than
study it in detail. So instead of understanding this in depth, we'll simply show that the layout difference indeed accounts for
the 100x - and then get rid of the slowdown <em>without</em> changing the layout, which is where "unsafe Python" finally comes
in.</p>
<p>How can we show that the layout alone, and not some other property of the pygame Surface data (like the memory it's allocated
in) explains the slowdown? We can benchmark cv2.resize on a numpy array we create ourselves, with the same layout as
<code>pixels3d</code> gives us:</p>
<pre style="background-color: #eeeeee;color: #222;overflow: auto;margin: 0 0 1.5385em 0;padding: 0.7692em;"><i># create a byte array of zeros, and attach
# a numpy array with pygame-like strides
# to this byte array using the buffer argument.</i>
i3 = np.ndarray((IW, IH, 3), np.uint8,
        <b>strides=(4, IW*4, -1),</b>
        buffer=b'\0'*(IW*IH*4),
        offset=2) <i># start at the "R" of BGR</i>

with Timer(<b><i>'pixels3d-like layout with cv2'</i></b>):
    for i in range(repeat):
        o2 = <b>cv2_resize</b>(i3)
</pre>
<p>Indeed this is about as slow as we measured on pygame Surface data:</p>
<pre style="background-color: #eeeeee;color: #222;overflow: auto;margin: 0 0 1.5385em 0;padding: 0.7692em;">pixels3d-like layout with cv2 took <b>1.3829</b> sec
</pre>
<p>Out of curiosity, we can check what happens if we merely copy data between these layouts:</p>
<pre style="background-color: #eeeeee;color: #222;overflow: auto;margin: 0 0 1.5385em 0;padding: 0.7692em;">i4 = np.empty(i2.shape, i2.dtype)
with Timer(<b><i>'pixels3d-like copied to same-shape array'</i></b>):
    for i in range(repeat):
        <b>i4[:] = i2</b>

with Timer(<b><i>'pixels3d-like to same-shape array, copyto'</i></b>):
    for i in range(repeat):
        <b>np.copyto</b>(i4, i2)
</pre>
<p>Both the assignment operator and <code>copyto</code> are very slow, almost as slow as cv2.resize:</p>
<pre style="background-color: #eeeeee;color: #222;overflow: auto;margin: 0 0 1.5385em 0;padding: 0.7692em;">pixels3d-like copied to same-shape array took <b>1.2681</b> sec
pixels3d-like to same-shape array, copyto took <b>1.2702</b> sec
</pre>
<h2 id="fooling-the-code-into-running-faster">Fooling the code into running faster</h2>
<p>What can we do about this? We can't change the layout of pygame Surface data. And we <em>seriously</em> don't want to copy
the C++ code of cv2.resize, with its various platform-specific optimizations, to see if we can adapt it to the Surface layout
without losing performance. <strong>What we <em>can</em> do is feed Surface data to cv2.resize using an array <em>with numpy's
default layout</em></strong> (instead of straightforwardly passing the array object returned by pixel3d.)</p>
<p>Not that this would actually <em>work</em> with any given function, mind you. But it <em>will</em> work specifically with
resizing, because it doesn't really care about certain aspects of the data, which we're incidentally going to blatantly
misrepresent:</p>
<ul>
<li>Resizing code doesn't care if a given channel represents red or blue. (Unlike, for instance, converting RGB to greyscale,
which <em>would</em> care.) If you give it BGR data and lie that it's RGB, the code will produce the same result as it would
given actual RGB data.</li>
<li>Similarly, it doesn't matter for resizing which array dimension represents width, and which is height.</li>
</ul>
<p>Now, let's take another look at the memory representation of pygame's BGRA array of shape <code>(width, height)</code>.</p>
<p><img alt="SDL-layout.png" height="178" src="https://yosefk.com/img/numpy-perf/SDL-layout.png" width="576" style="max-width: 100%;height: auto;"></p>
<p>This representation is actually the same as an RGBA array of shape <code>(height, width)</code> with numpy's default strides!
I mean, not really - if we reinterpret this data as an RGBA array, we're treating red channel values as blue and vice versa.
Likewise, if we reinterpret this data as a <code>(height, width)</code> array with numpy's default strides, we're implicitly
transposing the image. But resizing wouldn't care!</p>
<p>And, as an added bonus, we'd get a single RGBA array, and resize it with one call to cv2.resize, instead of resizing pixels3d
and pixels_alpha separately. Yay!</p>
<p>Here's code taking a pygame surface and returning the base pointer of the underlying RGBA or BGRA array, and a flag telling
if it's BGR or RGB:</p>
<pre style="background-color: #eeeeee;color: #222;overflow: auto;margin: 0 0 1.5385em 0;padding: 0.7692em;">import ctypes

def arr_params(surface):
    pixels = pg.surfarray.pixels3d(surface)
    width, height, depth = pixels.shape
    assert depth == 3
    xstride, ystride, zstride = pixels.strides
    oft = 0
    bgr = 0
    if <b>zstride == -1</b>: <i># BGR image - walk back
        # 2 bytes to get to the first blue pixel</i>
        <b>oft = -2</b>
        zstride = 1
        bgr = 1
    <i># validate our assumptions about the data layout</i>
    assert xstride == 4
    assert zstride == 1
    assert ystride == width*4
    base = <b>pixels.ctypes.data_as</b>(ctypes.c_void_p)
    ptr = ctypes.c_void_p(base.value + oft)
    return ptr, width, height, bgr
</pre>
<p>Now that we have the underlying C pointer to the pixel data, we can wrap it in a numpy array with the default strides,
implicitly transposing the image and swapping the R &amp; B channels. <strong>And once we "attach" a numpy array with default
strides to both the input and the output data, our call to cv2.resize will run 100x faster!</strong></p>
<pre style="background-color: #eeeeee;color: #222;overflow: auto;margin: 0 0 1.5385em 0;padding: 0.7692em;">def rgba_buffer(p, w, h):
    <i># attach a WxHx4 buffer to the base pointer</i>
    type = ctypes.c_uint8 * (w * h * 4)
    return <b>ctypes.cast(p, ctypes.POINTER(type)).contents</b>

def <b>cv2_resize_surface</b>(src, dst):
    iptr, iw, ih, ibgr = arr_params(src)
    optr, ow, oh, obgr = arr_params(dst)

    <i># our trick only works if both surfaces are BGR,
    # or they're both RGB. if their layout doesn't match,
    # our code would actually swap R &amp; B channels</i>
    <b>assert ibgr == obgr</b>

    ibuf = rgba_buffer(iptr, iw, ih)

    <i># numpy's default strides are height*4, 4, 1</i>
    iarr = np.ndarray(<b>(ih,iw,4)</b>, np.uint8, <b>buffer=ibuf</b>)
    
    obuf = rgba_buffer(optr, ow, oh)

    oarr = np.ndarray(<b>(oh,ow,4)</b>, np.uint8, <b>buffer=obuf</b>)

    <b>cv2.resize</b>(iarr, (ow,oh), oarr, interpolation=cv2.INTER_AREA)
</pre>
<p>Sure enough, we finally get a speedup instead of a slowdown from using cv2.resize on Surface data, and we're as fast as
resizing an RGBA numpy.zeros array (where originally we benchmarked an <em>RGB</em> array, not RGBA):</p>
<pre style="background-color: #eeeeee;color: #222;overflow: auto;margin: 0 0 1.5385em 0;padding: 0.7692em;">osurf = pg.Surface((OW,OH), pg.SRCALPHA)
with Timer(<b><i>'attached RGBA with cv2'</i></b>):
    for i in range(repeat):
        <b>cv2_resize_surface</b>(isurf, osurf)

i6 = np.zeros((IW,IH,4), np.uint8)
with Timer(<b><i>'np.zeros RGBA with cv2'</i></b>):
    for i in range(repeat):
        o6 = <b>cv2_resize</b>(i6) 
</pre>
<p>The benchmark says we got our 100x back:</p>
<pre style="background-color: #eeeeee;color: #222;overflow: auto;margin: 0 0 1.5385em 0;padding: 0.7692em;">attached RGBA with cv2 took <b>0.0097</b> sec
np.zeros RGBA with cv2 took <b>0.0066</b> sec
</pre>
<p>All of the ugly code above is <a href="https://github.com/yosefk/BlogCodeSamples/blob/main/numpy-perf.py">on GitHub</a>.
Since this code is ugly, you can't be sure it actually resizes the image correctly, so there's some more code over there that
tests resizing on non-zero images. If you run it, you will get the following gorgeous output image:</p>
<p><img alt="resized.png" height="540" src="https://yosefk.com/img/numpy-perf/resized.png" width="960" style="max-width: 100%;height: auto;"></p>
<p>Did we really get a 100x speedup? It depends on how you count. We got cv2.resize to run 100x faster relatively to calling it
straightforwardly with the pixel3d array. But specifically for resizing, pygame has smoothscale, and our speedup relatively to
it is 13-15x. There are some more benchmarks on GitHub for functions other than resize, some of which don't have a corresponding
pygame API:</p>
<ul>
<li>Copying with <code>dst[:] = src</code>: <strong>28x</strong></li>
<li>Inverting with <code>dst[:] = 255 - src</code>: <strong>24x</strong></li>
<li><code>cv2.warpAffine</code>: <strong>12x</strong></li>
<li><code>cv2.medianBlur</code>: <strong>15x</strong></li>
<li><code>cv2.GaussianBlur</code>: <strong>200x</strong></li>
</ul>
<p>So not "exactly" 100x, though I feel it's fair enough to call it "100x" for short.</p>
<p>In any case, I'd be surprised if that many people use SDL from Python for this specific issue to be broadly relevant. But I'd
guess that numpy arrays with weird layouts come up in other places, too, so this kind of trick might be relevant elsewhere.</p>
<h2 id="unsafe-python">"Unsafe Python"</h2>
<p>The code above uses "the C kind of knowledge" to get a speedup (Python generally hides data layout from you, whereas C
proudly exposes it.) It also, unfortunately, has the memory (un)safety of C - we get a C base pointer to the pixel data, and
from that point on, if we mess up the pointer arithmetic, or use the data after it was already freed, we're going to crash or
corrupt data. And yet we wrote no C code - it's all Python.</p>
<p><a href="https://doc.rust-lang.org/book/ch19-01-unsafe-rust.html">Rust has an "unsafe" keyword</a> where the compiler forces
you to realize that you're calling an API which voids the normal safety guarantees. But the Rust compiler <em>doesn't</em> make
you mark your function as "unsafe" just because you have an unsafe block in that function. Rather, it trusts <em>you</em> to
decide whether your function is itself unsafe or not.</p>
<p>(In our example, <code>cv2_resize_surface</code> is a safe API, assuming I don't have a bug, because none of the horror
escapes into the outside world - outside, we just see that the output surface was filled with the output data. But
<code>arr_params</code> is a completely unsafe API, since it returns a C pointer that you can do anything with. And
<code>rgba_buffer</code> is <em>also</em> unsafe - although we return a numpy array, a "safe" object, nothing prevents you from
using it after the data was freed, for example. In the general case, no static analysis can tell whether you've built something
safe from unsafe building blocks or not.)</p>
<p>Python doesn't have an <code>unsafe</code> keyword - which is in character for a dynamic language with sparse static
annotation. But otherwise, Python + <code>ctypes</code> + C libraries is sort of similar in spirit to Rust with
<code>unsafe</code>. The language is safe by default, but you have your escape hatch when you need it.</p>
<p>"Unsafe Python" exemplifies a general principle: <strong>there's <em>a lot</em> of C in Python</strong>. C is Python's evil
twin, or, in chronological order, Python is C's good-natured twin. C gives you performance, and doesn't care about usability or
safety; if any of the footguns go off, tell it to your healthcare provider, C isn't interested. Python on the other hand gives
you safety, and it's based on <a href="https://en.wikipedia.org/wiki/ABC_(programming_language)">a decade's worth of
research</a> into usability for beginners. It doesn't, however, care about performance. They're both optimized aggressively for
two opposite goals, at the cost of ignoring the other's goals.</p>
<p>But on top of that, Python was built with C extensions in mind from the start. Today, from my point of view, <em>Python
functions as a packaging system</em> for popular C/C++ libraries. I have way less appetite for downloading and building OpenCV
to use it from C++ than <code>pip install</code>ing OpenCV binaries and using them from Python, because C++ doesn't have a
standard package management system, and Python does. There are a lot of high-performance libraries (for instance in scientific
computing and deep learning) with more code calling them in Python than in C/C++. And on the other hand, if you want seriously
optimized Python code and a small deployment footprint / low startup time, you'd use <a href="https://cython.org/">Cython</a> to
produce an extension "as if written in C" to spare the overhead of an otherwise "more Pythonic" JIT-based system like <a href="https://numba.pydata.org/">numba</a>.</p>
<p>Not only is there a lot of C in Python, but, being opposites of sorts, they complement each other fairly well. A good way to
make Python code fast is using C libraries in the right ways. Conversely, a good way to use C safely is to write the core in C
and a lot of the logic on top of it in Python. The Python &amp; C/C++/Rust mix - either a C program with a massive Python
extension API, or a Python program with all of the heavy lifting done in C - seems quite dominant in high-performance, numeric,
desktop / server areas. And while I'm not sure this fact is very inspiring, I think it's a fact<a class="footnote-ref" role="doc-noteref" href="#fn3" id="fnref3"><sup>3</sup></a>, and things will stay this way for a long time.</p>
<p><em>Thanks to Dan Luu for reviewing a draft of this post.</em></p>

<section class="footnotes footnotes-end-of-document" role="doc-endnotes" id="footnotes">
<hr>
<ol>
<li id="fn1"><p>This is what happens when you do stuff for fun, or just in a small team. If I was getting paid to work on this,
I'd keep looking into it until figuring it out, at least if the team was large enough to not have to worry that this would delay
more critical work too much. Makes one think, though I'm not sure <em>what</em> I think about this, all things considered.<a class="footnote-back" role="doc-backlink" href="#fnref1">↩︎</a></p></li>
<li id="fn2"><p>Thankfully, because the existing layout flexibility "only" gives us a 100x slowdown, where with a callback, it
could easily go to 10000x.<a class="footnote-back" role="doc-backlink" href="#fnref2">↩︎</a></p></li>
<li id="fn3"><p>I'm not that good in this particular area, and I'd be happy to hear the thoughts of more experienced people on
what to use these days to implement something like Krita or Blender. I sort of lean towards "a Python program with C/C++/Rust
libraries" rather than "a C++/Rust program with a Python extension API," because, funnily enough, C++ is <em>too unsafe</em> and
Rust is <em>too safe</em> for quickly iterating on a large, complicated code base - so I'd want to keep most of the code doing
lots of little things in Python, and use C/C++/Rust for optimized production code doing well-understood heavy lifting kinds of
stuff. But this way of structuring your program is at most moderately popular, and I wonder if I'm missing something.<a class="footnote-back" role="doc-backlink" href="#fnref3">↩︎</a></p></li>
</ol></section>]]></description>
      <comments>https://yosefk.com/cgi-bin/comments.cgi?post=blog/a-100x-speedup-with-unsafe-python#comments</comments>
      <pubDate>Sat, 04 May 2024 00:00:00 +0000</pubDate>
      <dc:creator>Yossi Kreinin</dc:creator>
      <wfw:commentRss>https://yosefk.com/blog/a-100x-speedup-with-unsafe-python.feed</wfw:commentRss>
    </item>
    <item>
      <title>Profiling with Ctrl-C</title>
      <link>https://yosefk.com/blog/profiling-with-ctrl-c.html</link>
      <description><![CDATA[<p>I once wrote about <a href="https://yosefk.com/blog/how-profilers-lie-the-cases-of-gprof-and-kcachegrind.html">how profiler
output can be misleading</a>. Someone <a href="https://yosefk.com/cgi-bin/comments.cgi?post=blog/how-profilers-lie-the-cases-of-gprof-and-kcachegrind#comment-1847">commented</a>
that you don’t need profilers - just Ctrl-C your program in a debugger instead, and you’ll see the call stack where your program
probably spends most of its time. I admit that I sneered at the idea at the time, because, despite those comments’ almost
aggressive enthusiasm, this method doesn’t actually work on the hard problems. But as my outlook on life worsened with age, I
came to think that Ctrl-C profiling deserves a shout-out, because it’s very effective against stupid problems encountered by
lazy people operating in unfriendly environments.</p>
<p>I mean, I’ve tended to dismiss the stupid problems and focus on the hard ones, but is this a good approach in the real world?
Today I’m quite ready to accept that most of life is stupid problems encountered by lazy people operating in unfriendly
environments. Certainly, one learning experience was becoming such a person myself, by stepping into a senior management role<a class="footnote-ref" role="doc-noteref" href="#fn1" id="fnref1"><sup>1</sup></a> and then going back to programming after a few
years. Now I’m lazy because I got used to not doing anything myself, and I’m in an environment which is unfriendly to me,
because I forgot how things work, or they no longer work the way they used to. And while I’m a bit ashamed to admit this as
someone who’s developed several profilers himself, I’m often not really in the mood to figure out how to use a profiler in a
given setting.</p>
<p>But, here’s a program taking a minute to start up. Well, only in the debug build; this must be why nobody fixed it, but we
really should, it sucks to wait for a full minute every time you rebuild &amp; rerun. So I Ctrl-C the thing, and what do you
know, there’s one billion stack frames from the <a href="https://github.com/nlohmann/json">nlohmann JSON parser</a>, I guess it
all gets inlined in the release build; must be what they call “zero-cost abstraction”<a class="footnote-ref" role="doc-noteref" href="#fn2" id="fnref2"><sup>2</sup></a>. Another Ctrl-C, another call stack, coming from a different place but again
ending up parsing JSON. And I don’t know what the fix was - a different JSON parser, or compiling some code with optimizations
even in the debug build - but someone fixed it after my Ctrl-C-based report.</p>
<p>Or let’s say I’m trying to switch to the LLD linker from gold, to speed up the linking. Why not the even faster <a href="https://github.com/rui314/mold">mold</a>? - because I’m on MIPS, and mold doesn’t support MIPS. But LLD is pretty fast,
too; the core was written by <a href="https://github.com/rui314">the same person</a>, after all. And then I open a core dump
from a binary linked with LLD, and gdb is <em>really</em> slow. Hmm. It should have been <em>faster</em>, actually, because I’ve
also added <code>--gdb-index</code>, which tells the linker to create, I guess, some index for gdb, making gdb faster than its
slow default behavior, which is reserved for the unfortunate people who don’t know the cool flags. But I’m not seeing faster,
I’m seeing slower. What gives?</p>
<p>So, I run gdb under gdb, and Ctrl-C it while it’s struggling with the core dump. There’s some callstack with
<code>dwarf_decode_macro_bytes</code>. Google quickly brings up some relevant issues, such as “<a href="https://sourceware.org/bugzilla/show_bug.cgi?id=24624">Using -ggdb3 and linking with ld.lld leads to cpu/memory hog in
gdb</a>” (Status: <strong>UNCONFIRMED</strong>) and “<a href="https://bugs.llvm.org/show_bug.cgi?id=42030">lld doesn't generate
DW_MACRO_import like ld.bfd does</a>” (Status: <strong>RESOLVED WONTFIX</strong>.)</p>
<p>Apparently gcc generates some DWARF data that gdb is slow to handle. The GNU linker fixes this data, so that gdb doesn’t end
up handling it slowly. LLD refuses to emulate this behavior of the GNU linker, because it’s gcc’s fault to have produced that
DWARF data in the first place. And gdb refuses to handle LLD’s output efficiently, because it’s LLD’s fault to not have handled
gcc’s output the way the GNU linker does. So I just remove <code>-ggdb3</code> - it gives you a bit richer debug info, but it’s
not worth the slower linking with gold instead of LLD, nor the slowdown in gdb that you get with LLD. And everyone links happily
ever after.</p>
<p>Which goes to show that <strong>Ctrl-C profiling is often enough to solve a simple problem, and it’s usually much easier than
learning how to use a profiler and how to properly read its output</strong>. You can connect a debugger to almost anything, all
the way down to some chip with nothing like a standard OS that could work with a standard profiler. You can connect a debugger
to almost anything especially if it’s slow - for example, maybe it’s hard to actually invoke the program under gdb because its
invocation is buried somewhere very deep, but if it’s slow, you can <code>gdb /proc/$pid/exe $pid</code> after it was
started.</p>
<p>A debugger also needs less to work with than a profiler. Unlike <a href="https://perf.wiki.kernel.org/index.php/Tutorial">perf</a>, gdb will give you a callstack even if <a href="https://www.brendangregg.com/blog/2024-03-17/the-return-of-the-frame-pointers.html">the program was compiled without frame
pointer support</a>. And you certainly don’t need a special build, like <a href="https://ftp.gnu.org/old-gnu/Manuals/gprof-2.9.1/html_mono/gprof.html">gprof</a>’s <code>-pg</code>, or to run on a slow
simulator, like <a href="https://valgrind.org/docs/manual/cl-manual.html">callgrind</a> / <a href="https://kcachegrind.github.io/html/Home.html">KCachegrind</a>. And then the output of a profiler might be easy to
misinterpret - and I’ve only scratched the surface <a href="https://yosefk.com/blog/how-profilers-lie-the-cases-of-gprof-and-kcachegrind.html">the last time I wrote about it</a>.
Eyeballing a few callstacks is more straightforward.</p>
<p>Why then do we need profilers at all? Here is a very partial list of reasons, in no particular order.</p>
<p>Let’s say, completely hypothetically, that you’ve switched to the LLD linker, and your program is now 2-3% slower. If you
Ctrl-C it, you’ll see the same callstacks as with the version linked with gold. But if you have a profiler running on a
simulator, similarly to callgrind, then you can find the functions with the most slowdown - and they might not be the ones
taking the most time overall, they just have the most slowdown relatively to the old version - and then <strong>you can look at
the assembly listings and see how much time was spent running each instruction</strong>. And then you’ll see that the new
version has branch-to-address-from-register instructions where the old version had branch-to-constant-offset instructions.</p>
<p>Then you will learn about MIPS “relocation relaxation” (used also in RISC-V AFAIK.) The compiler “assumes the worst” and
generates code loading a function address into a register, and then jumping to the address stored in that register. Then, if
you’re lucky, the linker realizes that it has actually placed the function close enough to the caller for that caller to branch
to the function using a constant offset. (Fixed-sized RISC branch instructions cannot encode constant offsets larger than a
certain value, so the function needs to be close enough to the caller for the distance to fit into the offset encoding.) And
then the linker “relaxes” the expensive branch-from-register instruction into a cheaper branch-to-constant-offset instruction.
And it turns out that the LLD version you’re using doesn’t implement relocation relaxation.</p>
<p>Of course you, or should I say me, wouldn’t need that very, very fancy simulator-based profiler if you weren’t the idiot
using LLD 9 when LLD 14 was already available, with relocation relaxation implemented back in LLD 10. (I wish I’d saved the
discussion in the mailing list around this patch; now I can’t find it anywhere. There was nobody confident enough in their MIPS
knowledge to review the patch, but you don’t merge patches without a review, do you? There was even a message saying “Happy
anniversary to the relocation relaxation patch!” a year after it was submitted without having been merged. Eventually someone
said something like “we have to either merge or reject it, or we’re being rude” and someone else said “well, the patch author
knows MIPS better than any of us, so let’s just merge it.”)</p>
<p>But, despite having been an idiot here, I maintain that you don’t have to be an idiot to have this sort of problem, which a
profiler will help solve, and Ctrl-C profiling will not.</p>
<p>The broader issue is that <strong>Ctrl-C is essentially a sampling profiler</strong> - one with an unusually low sampling
frequency, but a sampling profiler nonetheless. Very small changes spread across a program are obviously invisible to a sampling
profiler. Also, <strong><a href="https://danluu.com/perf-tracing/">sampling profilers are bad at tail latency</a></strong> - if
something is usually fast but occasionally slow, you won’t be there to Ctrl-C it when it’s slow. (Of course, if “slow” means 100
ms instead of the usual 25 ms, you wouldn’t manage to Ctrl-C it in time even if you were there - that low sampling frequency
comes with some downsides.)</p>
<p>Systems involving many threads, processes or machines… our esteemed “random pausing” technique, aka Ctrl-C profiling, is
often not great to use with these. And at this point I feel that the idea of replacing all of the various profilers with Ctrl-C
is too ridiculous to bother with more counterarguments.</p>
<p><strong>But, there are many various kinds of profilers, making it a question which kind to use, and how much legwork finding
the problem will take on top of using it</strong>. Simulation-based profilers don’t have the problem of losing data to a low
sampling frequency - they analyze full instruction traces - but they’re too slow for anything like a production environment. So
you might need some measurements that you can run in production, and then a way to rerun the program on the simulator using
inputs that were observed to cause a slowdown in production based on these measurements. Tracing profilers like <a href="https://www.kernel.org/doc/html/v5.0/trace/ftrace.html">ftrace</a> / <a href="https://kernelshark.org/Documentation.html">KernelShark</a> are great for looking at examples of tail latency, but they
will not reliably take you to the places in the code where the time is spent. Sampling profilers can run in production and take
you to the right place in the code, but they’re a poor match for code that runs slowly but only occasionally, and even worse for
code that occasionally gets stuck waiting for something. And most of these tools have a bunch of non-trivial prerequisites,
config knobs and likely ways to misread their output.</p>
<p>Conversely, Ctrl-C in a debugger is easy, makes you look very effective when it actually works, and costs almost nothing to
try even when it doesn’t really help in the end. What’s not to like?</p>
<p>I often find myself recommending something primitive or ugly, which <a href="https://yosefk.com/blog/refix-fast-debuggable-reproducible-builds.html#anti-thesis-sed---nasty-brutish-and-short">might
actually do better than the “proper” approach</a>, or it might have <a href="https://yosefk.com/blog/dont-ask-if-a-monorepo-is-good-for-you-ask-if-youre-good-enough-for-a-monorepo.html">less risky
failure modes in the hands of typical users</a>, or it might be <a href="https://yosefk.com/blog/how-to-make-a-heap-profiler.html">easier to tailor to your needs than a more elaborate
solution</a>. “Profile with Ctrl-C” fits right in - certainly very primitive, yet often compares surprisingly favorably with
more sophisticated alternatives. And therefore, I must give Ctrl-C profiling my warmest endorsement!</p>
<p><em>Thanks to Dan Luu for reviewing a draft of this post.</em></p>

<section class="footnotes footnotes-end-of-document" role="doc-endnotes" id="footnotes">
<hr>
<ol>
<li id="fn1"><p>In my Russian-speaking mind, “stepping into” is strongly associated with “stepping into shit.” I’m not sure
there’s an idiomatic English synonym for stepping into something strongly implying that this something is shit; there should be
- it’s a very useful thing to have in a language.<a class="footnote-back" role="doc-backlink" href="#fnref1">↩︎</a></p></li>
<li id="fn2"><p>“Zero-cost abstraction” is a figure of speech popular with people who don’t consider time spent compiling,
deciphering compiler errors, debugging, or running the debug build as a “cost.” It would be more accurate to call it “zero cost
in production machine resources,” though even that is quite often incorrect.<a class="footnote-back" role="doc-backlink" href="#fnref2">↩︎</a></p></li>
</ol></section>]]></description>
      <comments>https://yosefk.com/cgi-bin/comments.cgi?post=blog/profiling-with-ctrl-c#comments</comments>
      <pubDate>Tue, 25 Jun 2024 00:00:00 +0000</pubDate>
      <dc:creator>Yossi Kreinin</dc:creator>
      <wfw:commentRss>https://yosefk.com/blog/profiling-with-ctrl-c.feed</wfw:commentRss>
    </item>
    <item>
      <title>Advantages of incompetent management</title>
      <link>https://yosefk.com/blog/advantages-of-incompetent-management.html</link>
      <description><![CDATA[<p>What constitutes managerial competence? As a vague starting point for an answer, we could say that competent management sets
achievable objectives and then achieves them, by organizing and incentivizing the necessary work.</p>
<p>It turns out that even this near-tautological banality is enough to see why competent management puts many desirable things
out of reach. This becomes apparent when looking at examples where incompetent management does better than most well-run places
can hope for.</p>
<h2 id="efficiency">Efficiency</h2>
<p>Improving efficiency tends to be against the interest of most people in an org, because it’s equivalent to shrinking your
budget. Here’s what I’m told is a true story about how things work with actual budgets. A relatively inexperienced VP attends a
meeting where senior management is asked to shrink their budgets due to the adverse economic climate brought about by the
coronavirus pandemic. He eagerly cuts his equipment budget from $10 million to $6 million - over the loud and desperate
objections of his team (whom the VP nearly accuses of lacking patriotism, loyalty to the company and commitment to the common
good.)</p>
<p>Next year, the coronavirus mutates some more, and profits go back up. Our VP submits a $10 million equipment budget to the
finance department, where they cheerfully inform him that the extra $4 million will not go well with the CEO. Why, a 66%
increase over last year’s $6 million!</p>
<p>Wait a minute, thinks the VP, a sensation running through his whole body of rapidly gaining that invaluable experience which
he so sorely lacked. I voluntarily cut 40% of my budget - a share way larger than anyone else - due to an unforeseen,
extraordinary emergency. And now I’m rewarded with this cut becoming permanent?.. I see. Well, I’m always eager to learn.</p>
<p>This year being already lost, he quietly resubmits a $6 million budget (approved more swiftly by the CEO than any other,
thanks to zero YoY growth.) Next year, he uses some real or perceived crisis to increase this budget to $20 million. And now he
learned how to operate in a well-run company.</p>
<p>Of course you could say that this is a <em>badly</em> run company, and to avoid arguing what that means, let’s stick to the
definition of managerial competence as the ability to set and achieve objectives. <strong>Whatever objective you are expected to
achieve, a bigger budget makes it easier</strong>. And while asking for more resources gets you yelled at, the yelling is for
show, and ends once the budget increase is approved (or <em>isn’t</em> approved; but it never really hurts to try.) But if you
fail to achieve your objective, the yelling will be for realsies, go on and on, be followed by career setbacks, and continue
long afterwards, quite possibly with no way to resuscitate said career.</p>
<p>Set objectives create a simple zero-sum game over resources - you want more resources to do what they asked, and they want
you to do the same things with less. <strong>Optimization, budget cuts or relinquishing resources under any other name simply
registers as losing a round in this game</strong>. It’s awfully sweet to save company resources, but expecting it to do you any
good just means that you don’t understand the game.</p>
<p>I mean, what do you expect to happen? That we'll ask you to do less, or forgive you for doing less? No way, we asked you to
do those things because they must be done. Then maybe you expect to be given more resources? Obviously ridiculous, you just had
resources, there’s no sense in hoping to get them again as a reward for giving them back when you already had them?.. Maybe
you’d like a stock grant for being such a good citizen? No, if we do that, everyone will inflate their budget, and then cut it
to get a stock grant. Could that be what you did here?.. Like, why was the budget so large to begin with?..</p>
<p>But wait, seriously though, what’s the math here? What are we maximizing? Revenue minus cost? Revenue divided by cost? I
mean, shrinking the cost has got to be helping with these?.. Well, sure it’s helping, but it’s not helping <em>you,</em> because
you don’t bring any revenue <em>by yourself,</em> unlike cost, which you very much do incur all by yourself. The math with
<em>you</em> is, <strong>we tell you to do something if the cost is below a threshold</strong>. If you won’t commit to doing it
cheaply enough, we’ll find someone who will, and if we can’t, we won’t do the thing, or reconsider the options in some other
way. But exactly what the cost below the threshold is changes nothing in any math related to you, except for a lower cost making
your job harder, since you have the same objectives to achieve. The firm’s bottom line - sure, lower costs help there. But the
impact on the firm’s “revenue - cost” doesn’t trickle down to your “cost &lt; threshold,” because you have no revenue<a class="footnote-ref" role="doc-noteref" href="#fn1" id="fnref1"><sup>1</sup></a>.</p>
<p>Things work the same with any resource, not just actual money - it could be team size, or processor cycles and memory bytes.
If you free up 200 ms of CPU cycles and 500 megs of RAM, someone else can deploy their functionality using these newly available
resources, and then <em>you</em> won’t be able to. In fact, a mature, well-run CI system will measure everyone’s resource
footprint after each commit, and will not let you exceed your budget, which was frozen at some point based on how much you were
using at the time (hope it was a lot! - always spend like crazy before the baseline is established!) Is it any wonder that
people learn to never optimize their code - unless <em>they</em> want to deploy something new themselves, and only after asking
for more resources to deploy it and not getting any?</p>
<p>I like it when people ask “why is this code so slow? Why don't we optimize it?” And it still makes me sad when people ask
instead, “how much CPU time do I have for running this code?” when it's obviously 5-10x slower than it could be, and they're
asking to reserve 2-3x more CPU time than they're already wasting. But that's what happens when people have worked at well-run
places and aren’t stupid.</p>
<p>What happens in a badly run place? In a badly run place, management is bad at setting objectives, so you have people
aimlessly wandering about, lacking clear goals, and just doing stuff because they want to. They see an optimization opportunity
and they gladly pursue it - it’s interesting, it’s fun, it’s good for the company, it’s what they’re here for. If a patch must
be submitted to a team, that team might gladly accept it - they don’t mind shrinking their resource footprint, because nobody
monitors the resource budget properly, nor presses them to meet any targets very hard - which is also why they don’t really mind
spending some time on something not helping them achieve any such target. In fact, they might get interested enough to actively
help whoever found the problem to fix it.</p>
<p>Your legs don’t fight your heart, brain and each other for the oxygen budget; every organ only uses what it needs, and is
optimized for efficiency. The selfish corporation is yet to make its parts behave as selflessly as our body parts sharing our
supposedly selfish genes. Yet people do have a tendency to do the right thing regardless of incentives - no doubt because they
mistake their corporation for their tribe, thinking their coworkers share more of their genes than they do<a class="footnote-ref" role="doc-noteref" href="#fn2" id="fnref2"><sup>2</sup></a>. But <strong>if there’s a reliable &amp; scalable way for
vigorous, systematic management to reward the spontaneous human drive towards efficiency instead of punishing it, I am yet to
see it</strong>. Certainly honest people working for the trillion-dollar heavyweight champions of the industry testify that this
problem is far from solved.</p>
<p>It’s an exercise both fun and depressing to come up with ways to “manage for efficiency.” For example, we could reward people
for performance savings, right? Great idea - now I can commit some CPU or memory hog, then you can fix it, and we’ll split the
reward. Or, more realistically, first we all go on a crazy resource spending spree to meet a deadline. And then later on, we
optimize away the lower-hanging fruit in the crazy-inefficient system and get a reward - with not-so-low-hanging fruit from that
spending spree probably left hanging forever.</p>
<p>(Of course, we probably won’t tell ourselves that we’re deliberately overspending more than is actually helpful for meeting
the deadline to game the system. Rather, the culture will just kinda shift in that direction. People are very good at doing
fucked-up things without admitting it to themselves - which would make them sad and less energized to do the fucked-up thing
they have compelling reasons to do.)</p>
<p>Perverse incentives always appear wherever incentives are deployed, because the very notion of an incentive is fundamentally
perverse. But a competent manager is forced to use incentives, instructions, and incentives to follow instructions, because what
else could he use?</p>
<h2 id="sprawl">Sprawl</h2>
<p>“These teams are like bulldozers with no brakes,” mused my acquaintance, who’d managed a team in a poorly-run company and had
recently become a director in a much better-run one. “You only have a steering wheel, and you need to be steering all the time,
or this thing is going to dig a giant hole in the ground, or raze a building or something. If you don’t tell them exactly what
to do, they’re still always going to do <em>something</em>, and then it’ll be too late.”</p>
<p>You see, he was used to people doing pretty much nothing when left unmolested. Of course, from the employer’s point of view,
this habit is straightforwardly wasteful, because you’re still paying their salaries. To weed out such do-nothing people,
competent management sets up a performance evaluation process, so that we always know what every person has done for us every
year, and who should get outsized rewards and who should get fired.</p>
<p>This system leaves people very worried if they don’t have clear goals to work towards. However, even a competent organization
cannot set actually useful goals for everyone at all times, just like you generally need your legs, but you don’t really have a
use for them at every moment. And thus, <strong>you have people with spare bandwidth making up their own goals, so that they
have something to show in the performance review.</strong></p>
<p>If we now revisit the situation from the employer’s point of view, it is no longer <em>trivially</em> wasteful, because
everyone is always busy. However, it’s likely more wasteful than before, because people are building stuff you didn’t really
need, and yet you almost certainly need <em>now,</em> because actually productive activities are hopelessly intertwined with
this stuff.</p>
<p>This is a big reason why successful software companies end up with mountains of code. The cycle repeats and branches out
exponentially, as every team who’s built the once-needless and now-necessary thing asks for more headcount, gets it, and
inevitably ends up with some of it idle some of the time. Then these new people invent more goals to pursue, persuade everyone
that these fake goals are actual sub-goals of the real goals, and entangle existing systems with their new systems.</p>
<p>And now figuring out where the waste is will be much harder than just spotting idle people, since all the needless work was
done for no other purpose than looking very important, and people are pretty good at making the right impression when they’re
trying. And of course when people lie, they lie first and foremost to themselves - we’re all natural-born <a href="https://en.wikipedia.org/wiki/Method_acting">Method actors</a> - so if you spot a decoy and try to cancel the work on that
system, not only will the people working on it fight this with all their might, but they'll be genuinely heartbroken if you do
cancel it. And by the time you’ve actually dealt with one of these weeds, if you’re a weird manager actually trying, two more
will have sprouted in another part of the org.</p>
<p>If you’re used to such sprawl, you’d be surprised how effective sleepy HR practices are at preventing it. Suppose you always
get a standard, shitty raise at the end of the year by default, unless you bargain loudly, which works rarely and only if you’ve
really made an impression throughout the year. There is no defined budget for raises; every significant raise is hard to get,
and you never get it proactively without bargaining, but there’s no formal system to avoid spending too much on raises except
for the reluctant, reactive approach to giving them. There’s also no system for firing low performers, and it’s only very rarely
that you see anyone fired - like that crazy fuck who went on and on about how your source control sucked and should be
completely different, and then used a single dot character, “.”, as the commit message when he finally committed something.</p>
<p>A similar system is used for managing other resources: for example, every team gets to grow at some low annual rate, no
department is ever cut, and it’s very hard to grow your department faster than the base rate even if you get more
responsibilities.</p>
<p>A place like this evolves the healthy laziness that keeps animals from moving their body parts all the time, needlessly
burning calories, in order for the claws, wings and tails to get a good performance review from their head at the end of the
year. Sure, many people do nothing much of the time, and you need some effort to make them do something when it becomes
necessary; “the hedgehog is too proud a bird to fly without a kick,” as the wise Russian proverb goes. But on the upside, nobody
doing anything unless it’s really necessary means you don’t have all this unnecessary stuff.</p>
<p><strong>Healthy laziness begets agility - you have way less code, less systems, less everything, and therefore way more
ability to maneuver and actually change things with a small number of motivated people </strong>- and there’s always a small
number of motivated people in any place, and this place might even keep them, if they learn to bargain for raises. And you also
don’t need to grow as much, because you don’t need to be adding people to take care of all these sprawling systems that you
quickly come to depend on.</p>
<h2 id="bugs">Bugs</h2>
<p>Bug fixes work a lot like efficiency improvements, the main difference being that competent management makes things much
worse. You can’t make fixing bugs into a “goal,” same as you can’t make optimization into a goal, because people will just add
more bugs up front and then fix some of them. But at least with optimization, you can have teams doing it across the
organization, and it claws back some of the performance lost in the first place.</p>
<p>A team optimizing others’ systems cannot hunt down the tens of thousands of little performance hogs created by everyone else.
But it can often find tens or hundreds of relatively small changes with a fairly big performance impact. They’re probably not
“fully incentivized” for this outsized impact, because with rewards anywhere close to how much money this is worth to the
business, the incentive is quite likely to become extremely perverse. But you definitely can make “everyone else’s performance”
a team’s job description, combine it with your venerable performance evaluation &amp; promotion process, and get
<em>something</em> - often a big something.</p>
<p>Another kind of teams with some form of “someone else’s efficiency” in their job description is people working on compiler,
language runtime or kernel optimizations, custom compute or networking accelerators, and other such. They could be inefficient
<em>in their own work</em> for the same reasons mentioned above, but they might still be increasing <em>others’</em> efficiency,
because it’s legitimately an example of a goal that their competent management is good at setting and achieving.</p>
<p>The problem with bugs is that <strong>you can’t have people solve others’ bugs as much as you can have them improve others’
efficiency</strong>. It is generally much easier for a relative outsider to see where a system spends its resources than where
its bugs are. That’s because all systems spend similar kinds of resources, but what constitutes a bug varies from system to
system, and there’s almost never a machine-readable, formal, or even just a reasonably complete and written-down definition of
correctness. The few exceptions are things like programming language semantics, and indeed this is where a lot of progress has
been made - think sanitizers, <a href="https://yosefk.com/blog/checkedthreads-bug-free-shared-memory-parallelism.html">race
detectors</a>, etc.</p>
<p><strong>Another problem with bug fixes which you don’t have as much with optimizations is that it’s harder to measure the
impact</strong>. With efficiency improvements you can usually give a ballpark number of how much resources it would save -
perhaps a range of possibilities rather than one high-confidence number, but you’ll have something. With bugs, well, you could
A/B test them to try to quantify the impact on some metric management cares about, but who does that?</p>
<p>With performance, you deal in resources to begin with, and you have <em>some</em> number speaking of resource savings by
definition, or you couldn’t call it an optimization. And now there might be an argument of what multipliers to apply to this
number to arrive at a cost estimation, but at least you have a starting point. With a bug fix, you have the bug and the fix, and
you’re seriously going to suggest to A/B test the impact for no benefit to the employer except your ability to claim this impact
is worthy of a promotion? This is a great plan especially for internal systems without A/B testing infrastructure or any
preconditions for it, but it’s a great plan in general, employers love this.</p>
<p>(And also, most bugs you fix tend to come from your own team, and then all high impact proves is that you messed up big time
when you put the bug in. You’re not supposed to have bugs in the first place, punk.)</p>
<p>“I have this potential employer who says they’re interested in performance and correctness,” said another acquaintance. “I
told them that I can work on performance anywhere in the industry, so I can probably find an offer better in other respects
elsewhere. But correctness sounds interesting. I don’t know anywhere caring about correctness!”</p>
<p>Well, it’s not like they don’t care, as much as they don’t have a mechanism for caring or even registering it. Correctness is
not a goal in itself that management can set for the teams without perverse side-effects. Of course, you have to fix
“showstopper bugs” or you haven’t achieved your goal. Any further bug-fixing takes resources from achieving your nominal goals,
and is avoided - not outright, which would look bad, but through slow-walking and other acceptable forms of sabotage.</p>
<p>It’s true that Microsoft Teams (to take one example too many are familiar with) can get away with bugs because it’s bundled
with Outlook and other stuff, and because whoever pays for it doesn’t use it that much, but rather foists it upon helpless
internal users. But it’s also true that fixing those bugs would be money very well-spent for Microsoft, because it would almost
certainly improve their reputation and increase sales at the margins and more than offset the cost of the work. The problem is
that it’s hard for a well-run place to get people to fix non-showstopper bugs.</p>
<p>(One way to work on correctness, if you're into this, is to go to areas where more bugs are showstoppers, so fixing them
becomes a part of the nominal goals. If you’re a hardware developer, FPGAs, where you can fix bugs very cheaply, are a worse
context for this than ASICs, where you cannot, making you eager to find and fix them proactively. And hardware running lots of
software, which can't be patched to work around hardware bugs, like a CPU, <a href="https://yosefk.com/blog/the-habitat-of-hardware-bugs.html">will face more pressure to be correct</a> than something like a
peripheral device controller, which is only touched by comparatively little code written at the company making the hardware,
where it’s “easy” for software developers to add workarounds to this code<a class="footnote-ref" role="doc-noteref" href="#fn3" id="fnref3"><sup>3</sup></a>. If you’re a software developer, you could try an industry with high reliability
requirements, where many more bugs are defined as showstoppers.)</p>
<p>Of course, having more sprawl means having more bugs (and more performance issues, and more machine resources spent on
running all that sprawling code), and even defining what “correct” means becomes harder when the system is larger. The
unfortunate side effects of competent management compound each other.</p>
<h2 id="the-problem-with-incompetent-management">The problem with incompetent management</h2>
<p>The main disadvantage of incompetent management is its definitional inability to set and achieve key goals, which can
endanger the survival of the organization. Incompetent management can only thrive in situations where basic survival and even
growth are somehow taken care of, and any major changes in that situation create an existential risk.</p>
<p>It is theoretically possible for management to respond to an external crisis by “changing gears” from a sleepy indifference
to what’s going on in the organization to a vigorous push to get something huge done, as required by the new external situation.
The hope is to kick the suddenly awakened, terrified hedgehog into the stratosphere, and then go back to the sleepy ways of old
once it’s orbiting the Earth.</p>
<p>In practice, the risk is high for this attempt to fail - a place not used to the mobilized state of subordinating all efforts
to top-down goals will need time to learn, where “learning” might involve firing or otherwise replacing key people (which is a
big part of what “learning” means for organizations, and what people mean when they say such learning is “hard.”)</p>
<p>If the war effort does succeed, there’s quite likely no going back - the hedgehog will have been thoroughly transformed and
militarized by the ordeal. It will be the usual mix of competent management and cargo cult management from now on.</p>
<h2 id="cargo-cult-management-vs-straightforward-incompetence">Cargo cult management vs straightforward incompetence</h2>
<p>Speaking of which - a most unfortunate side effect of competent management is the widespread desire to emulate its look and
feel, which contaminates the wonderful natural incompetence of so many managers, robbing us of its many advantages.</p>
<p>Mostly incompetent management which is very bad at setting and achieving goals is perfectly capable and all too likely to
cargo-cult effective management by setting up an elaborate bureaucracy for assigning work and tracking its status, thus
preventing work from happening spontaneously. This has all the downsides of actually competent management without any of the
benefits.</p>
<p>Things work much better when incompetent managers embrace their laziness and do close to nothing. This is possible if there's
a culture where a manager gets to look good through means other than appearing to be on top of plans and status - for example,
by presenting shiny things the team is working on (regardless of their exact impact on the bottom line or even chances to be
deployed in production.)</p>
<h2 id="what-is-to-be-done">What is to be done?</h2>
<p>“What is to be done?” is <a href="https://en.wikipedia.org/wiki/What_Is_to_Be_Done%3F">a pamphlet by Lenin</a>, who proposed
some things to be done, and went on to do them and then some, with results most charitably described as mixed.</p>
<p>I don't know how it ever happened to me, but I somehow got infected with the absurd idea that there's always a good way for
things to work in an organization, and furthermore, somehow this good way always makes the org more effective than the commonly
observed not-so-good alternatives. I was brought up with natural immunity to <a href="https://en.wikipedia.org/wiki/The_Internationale#Anthem_of_the_Soviet_Union">the Soviet strain</a> of this Panglossian
optimism with respect to our ability to shape organizations in the all-around optimal way:</p>
<blockquote>
<p>We shall wholly destroy the world of oppression<br> Down to the foundations, and then<br> We'll build a new world of our
own.</p>
</blockquote>
<p>But it turned out that my Soviet antibodies don’t automatically work against <a href="https://paulgraham.com/good.html">the
Western strain</a>:</p>
<blockquote>
<p>…the most important advantage of being good is that it acts as a compass. … you have so many choices. How do you decide?</p>
<p>Here's the answer: Do whatever's best for your users. You can hold onto this like a rope in a hurricane, and it will save you
if anything can. Follow it and it will take you through everything you need to do.<a class="footnote-ref" role="doc-noteref" href="#fn4" id="fnref4"><sup>4</sup></a></p>
</blockquote>
<p>I mean, I guess I’ve always had antibodies good enough to protect against severe illness; I never imagined that companies
mostly succeeded by holding onto their goodness like a rope in a hurricane. But if you pressed me, I’d say they probably
<em>could</em>, and they would be better off if they did.</p>
<p>Which, if you think about it, why on Earth would this have to be correct? Few people would say that you can always make your
code faster without making it uglier, and those who say it tend to be a bit insane, in a professional sense. So why would making
a company more effective always make it better instead of worse, according to, well, any definition, really?.. Just because the
opposite thought is depressing? Well, the thought of <a href="https://yosefk.com/blog/efficiency-is-fundamentally-at-odds-with-elegance.html">faster code tending to be uglier</a> isn’t
a very happy one, either.</p>
<p>So, now that I have immunity strong enough to prevent infection and transmission of the effective goodness virus, I don’t
think you have to find a solution to an organizational problem just because you happen to observe it<a class="footnote-ref" role="doc-noteref" href="#fn5" id="fnref5"><sup>5</sup></a>. From an individual’s POV, the environment is nearly
impossible to change, hard to truly understand, and fairly easy to fit into for most values of “environment,” and companies are
no different. You probably can’t make most well-run places “truly care” about efficiency or correctness, but you can make a
great living optimizing stuff, and even debugging or seriously testing, if you find the right place for it.</p>
<p>Of course, if you put a gun to my head, I could add a few paragraphs on “combining the best of both worlds” and how it’s been
known to happen in small teams over short periods of time, and so on. And, not gonna lie, I almost did put a gun to my own head
to write these paragraphs - old habits die hard. But I came to my senses and deleted it. It’s more likely to make you feel sad
than happy, and most of all, it’s likely to make you bored.</p>
<p>(<strong>Update</strong> - see <a href="https://news.ycombinator.com/item?id=40893945">a comment on this write-up describing
the rise and fall of Creo</a>, a company that they say did start out combining the best of both worlds, then regressed to the
mean after acquiring another company with a lot of people and a "standard" culture, and went downhill both as a place to work
and a viable business. Like I said, these "best of both worlds" stories will make you more sad than happy.)</p>
<h2 id="conclusion">Conclusion</h2>
<p>Competent management sets goals to achieve. Whatever can’t be made into a goal cannot be achieved by definition. Whether this
sounds trivial or absurd, it has many surprising undesirable consequences which are surprisingly hard to avoid.</p>
<p>A company’s board is unlikely to raise the need for less competent management in their annual meeting, and for good reasons.
A prospective employee is another matter. If someone invites you to work for a company that’s run very badly, there might well
be a good story there - this is far from guaranteed, but you might want to hear the details. And by “a good story”, I don’t mean
“yay, here’s a place to slack off at,” but “maybe I can finally get some work done that I hardly ever get the chance to do.”</p>
<h2 id="see-also">See also</h2>
<p>It's very possible to make sure you only hire people who can answer algorithms questions in an interview. But don't expect
these carefully filtered employees to then <a href="https://danluu.com/algorithms-interviews/">actually solve, rather than
create</a>, problems equivalent to basic phone screen questions on the job, for reasons related to the above.</p>
<p><em>Thanks to Dan Luu and Tim Pote for reviewing a draft of this post.</em></p>

<section class="footnotes footnotes-end-of-document" role="doc-endnotes" id="footnotes">
<hr>
<ol>
<li id="fn1"><p>If you’re a general manager of a business unit, aka a P&amp;L (profit &amp; loss) unit, then of course you
<em>do</em> have revenue, and things are different. For the purpose of our current discussion, I treat a BU as a separate
company, and the discussion applies to its employees, not its GM who for our purposes can be treated as a CEO.<a class="footnote-back" role="doc-backlink" href="#fnref1">↩︎</a></p></li>
<li id="fn2"><p>Either that, or it’s a genetic defect in people, or something about group selection. It is a scientific fact
that everything in life is either the result of DNA molecules evolving to copy themselves more efficiently, or their occasional
failure to do so, and no other mechanism nor information encoded in any other form is of any consequence.<a class="footnote-back" role="doc-backlink" href="#fnref2">↩︎</a></p></li>
<li id="fn3"><p>It’s also easy for a device driver programmer to beat up a hardware device designer, especially when sneaking
from behind, but it’s been made artificially hard by various legal mechanisms.<a class="footnote-back" role="doc-backlink" href="#fnref3">↩︎</a></p></li>
<li id="fn4"><p>I did notice that it says “good for users” and not “good for employees” or from other points of view. But you
and I both know what the answer would have been had someone in the audience raised their hand and asked, “...it’s about being
good for users, right? - you do often have to make it terrible for employees, or terrible in some other sense, in order to
succeed?”<a class="footnote-back" role="doc-backlink" href="#fnref4">↩︎</a></p></li>
<li id="fn5"><p>The virus is bad enough to actually make you think, “since I don’t know how to solve this problem, I probably
don’t really understand it - my analysis of my observations must be incorrect,” as if thinking that the discrete knapsack
problem is NP-complete is a symptom of not understanding the discrete knapsack problem.<a class="footnote-back" role="doc-backlink" href="#fnref5">↩︎</a></p></li>
</ol></section>]]></description>
      <comments>https://yosefk.com/cgi-bin/comments.cgi?post=blog/advantages-of-incompetent-management#comments</comments>
      <pubDate>Thu, 04 Jul 2024 00:00:00 +0000</pubDate>
      <dc:creator>Yossi Kreinin</dc:creator>
      <wfw:commentRss>https://yosefk.com/blog/advantages-of-incompetent-management.feed</wfw:commentRss>
    </item>
  </channel>
</rss>
