<?xml version="1.0" encoding="utf-8"?>
<feed xml:lang="en-us" xmlns="http://www.w3.org/2005/Atom"><title>blaz.is</title><link href="https://blaz.is/blog/" rel="alternate"></link><link href="https://blaz.is/blog/feed/atom" rel="self"></link><id>https://blaz.is/blog/</id><updated>2024-08-01T16:44:05+00:00</updated><subtitle>h33p's blog on random topics</subtitle><entry><title>CGlue 0.3 Future and Beyond</title><link href="https://blaz.is/blog/post/cglue-0-3" rel="alternate"></link><published>2024-08-01T15:43:57+00:00</published><updated>2024-08-01T16:44:05+00:00</updated><author><name>auri</name></author><id>https://blaz.is/blog/post/cglue-0-3</id><summary type="html">&lt;p&gt;CGlue 0.3 now supports &lt;code&gt;Future&lt;/code&gt;, &lt;code&gt;Stream&lt;/code&gt;, and &lt;code&gt;Sink&lt;/code&gt; in &lt;a href="https://github.com/h33p/cglue/releases/tag/v0.3.4"&gt;version 0.3&lt;/a&gt;! They're not only safe, but also, relatively fast. All this took around a year to build, and I may now explain the keys to the async.&lt;/p&gt;
&lt;h2&gt;Recap&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://docs.rs/cglue"&gt;CGlue&lt;/a&gt; is an ABI safety solution, that focuses on expressing dynamic trait objects in a stable manner. A &lt;code&gt;#[cglue_trait]&lt;/code&gt; attribute is added to a trait, which triggers CGlue's code generator to spit out C-ABI wrappers. These wrappers can then be used to build plugin systems, and with &lt;code&gt;cglue-bindgen&lt;/code&gt; it is possible to generate C or C++ code for calling or implementing said plugins from foreign languages.&lt;/p&gt;
&lt;h2&gt;Associated types&lt;/h2&gt;
&lt;p&gt;To express &lt;code&gt;Future&lt;/code&gt; under CGlue, we must first be able to express associated types under CGlue. This turned out to be no easy feat.&lt;/p&gt;
&lt;p&gt;I will not bore you with excessive details that took 6 months to work out, but essentially, we have to generate additional generic parameters, for every associated type, to CGlue vtables.&lt;/p&gt;
&lt;p&gt;Effectively we get the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;#[cglue_trait]
pub trait WithAssoc&amp;lt;T&amp;gt; {
	type AssocTy: Clone;

	fn with_assoc(&amp;amp;self, assoc: &amp;amp;Self::AssocTy) -&amp;gt; T;
}

// #[cglue_trait] spits out the following (albeit more complex)

#[repr(C)]
pub struct WithAssocVtbl&amp;lt;
	'cglue_a,
	CGlueC: 'cglue_a + crate::trait_group::CGlueObjBase,
	T,
	CGlueAAssocTy: Clone,
&amp;gt; {
	with_assoc: extern &amp;quot;C&amp;quot; fn(cont: &amp;amp;CGlueC, assoc: &amp;amp;CGlueAAssocTy) -&amp;gt; T,
	/* phantoms... */
}

/// Default vtable reference creation.
impl&amp;lt;
	'cglue_a,
	CGlueC: /* ... */,
	CGlueCtx: crate::trait_group::ContextBounds,
	T,
	CGlueAAssocTy: Clone,
&amp;gt; Default for &amp;amp;'cglue_a WithAssocVtbl&amp;lt;'cglue_a, CGlueC, T, CGlueAAssocTy&amp;gt;
where
	CGlueC::ObjType: WithAssoc&amp;lt;T, AssocTy = CGlueAAssocTy&amp;gt;,
	/* ... */
&amp;gt;: crate::trait_group::CGlueBaseVtbl,
{
	/// Create a static vtable for the given type.
	fn default() -&amp;gt; Self {
		&amp;amp;WithAssocVtbl {
			with_assoc: cglue_wrapped_with_assoc,
			/* phantoms... */
		}
	}
}

// C wrapper

extern &amp;quot;C&amp;quot; fn cglue_wrapped_with_assoc&amp;lt;
	CGlueC: /* ... */,
	CGlueCtx: crate::trait_group::ContextBounds,
	T,
	CGlueAAssocTy: Clone,
&amp;gt;(cont: &amp;amp;CGlueC, assoc: &amp;amp;CGlueAAssocTy) -&amp;gt; T
where
	CGlueC::ObjType: WithAssoc&amp;lt;T, AssocTy = CGlueAAssocTy&amp;gt;,
{
	let (this, ret_tmp, cglue_ctx) = cont.cobj_ref();
	let ret = &amp;lt;CGlueC::ObjType as WithAssoc&amp;lt;T&amp;gt;&amp;gt;::with_assoc(this, assoc);
	ret
}

// Implementation on CGlue Opaque Object

impl&amp;lt;
	'cglue_a,
	CGlueO: 'cglue_a + WithAssocVtblGet&amp;lt;'cglue_a, T, CGlueAAssocTy&amp;gt;
		+ /* ... */,
	T,
	CGlueAAssocTy: Clone,
&amp;gt; WithAssoc&amp;lt;T&amp;gt; for CGlueO
where
	T: crate::trait_group::GenericTypeBounds,
	CGlueAAssocTy: crate::trait_group::GenericTypeBounds,
{
	type AssocTy = CGlueAAssocTy;
	#[inline(always)]
	fn with_assoc(&amp;amp;self, assoc: &amp;amp;Self::AssocTy) -&amp;gt; T {
		let __cglue_vfunc = self.get_vtbl().with_assoc;
		let cont = self.ccont_ref();
		let assoc = assoc;
		let mut ret = __cglue_vfunc(cont, assoc);
		ret
	}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The trick here is that CGlue vtable encodes the specific associated type used by the given opaque object, which then allows us to have multiple CGlue objects pointing to different associated types.&lt;/p&gt;
&lt;h2&gt;Task module&lt;/h2&gt;
&lt;p&gt;Once we have associated types, then we need to safely and efficiently transfer &lt;code&gt;&amp;amp;Waker&lt;/code&gt; across FFI boundary. To that end, we define &lt;code&gt;CRefWaker&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;#[repr(C)]
#[derive(Clone, Copy)]
pub struct CRefWaker&amp;lt;'a&amp;gt; {
    raw: &amp;amp;'a OpaqueRawWaker,
    clone: unsafe extern &amp;quot;C&amp;quot; fn(*const ()) -&amp;gt; CRawWaker,
    wake_by_ref: unsafe extern &amp;quot;C&amp;quot; fn(*const ()),
}

#[repr(transparent)]
#[derive(Clone, Copy)]
struct OpaqueRawWaker {
    waker: [*const (); 2],
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then create a conversion from &lt;code&gt;&amp;amp;Waker&lt;/code&gt; to &lt;code&gt;CRefWaker&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;impl&amp;lt;'a&amp;gt; CRefWaker&amp;lt;'a&amp;gt; {
    pub unsafe fn from_raw(raw: &amp;amp;'a RawWaker) -&amp;gt; Self {
        let raw: &amp;amp;'a OpaqueRawWaker = core::mem::transmute(raw);

        Self {
            raw,
            clone: waker_clone,
            wake_by_ref: waker_wake_by_ref,
        }
    }
}

impl&amp;lt;'a&amp;gt; From&amp;lt;&amp;amp;'a Waker&amp;gt; for CRefWaker&amp;lt;'a&amp;gt; {
    fn from(waker: &amp;amp;'a Waker) -&amp;gt; Self {
        const _: [(); core::mem::size_of::&amp;lt;Waker&amp;gt;()] = [(); core::mem::size_of::&amp;lt;OpaqueRawWaker&amp;gt;()];
        unsafe { Self::from_raw(core::mem::transmute(waker)) }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;waker_clone&lt;/code&gt; and &lt;code&gt;waker_wake_by_ref&lt;/code&gt; are defined as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;unsafe extern &amp;quot;C&amp;quot; fn waker_clone(waker: *const ()) -&amp;gt; CRawWaker {
    let waker: &amp;amp;Waker = &amp;amp;*(waker as *const Waker);
    let waker = core::mem::transmute(waker.clone());

    CRawWaker {
        waker,
        vtable: Default::default(),
    }
}

unsafe extern &amp;quot;C&amp;quot; fn waker_wake_by_ref(waker: *const ()) {
    let waker: &amp;amp;Waker = &amp;amp;*(waker as *const Waker);
    waker.wake_by_ref()
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And finally, a nice wrapper to go from &lt;code&gt;CRefWaker&lt;/code&gt; to &lt;code&gt;&amp;amp;Waker&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;pub fn with_waker&amp;lt;T&amp;gt;(&amp;amp;self, cb: impl FnOnce(&amp;amp;Waker) -&amp;gt; T) -&amp;gt; T {
	unsafe fn unreach(_: *const ()) {
		unreachable!()
	}
	unsafe fn noop(_: *const ()) {}
	unsafe fn clone(data: *const ()) -&amp;gt; RawWaker {
		let this = &amp;amp;*(data as *const CRefWaker);
		let waker = unsafe { (this.clone)(this.raw as *const _ as *const ()) };
		let waker = BaseArc::new(waker);
		CRawWaker::to_raw(waker)
	}
	unsafe fn wake_by_ref(data: *const ()) {
		let this = &amp;amp;*(data as *const CRefWaker);
		unsafe { (this.wake_by_ref)(this.raw as *const _ as *const ()) };
	}

	let vtbl = &amp;amp;RawWakerVTable::new(clone, unreach, wake_by_ref, noop);
	let waker = RawWaker::new(self as *const Self as *const (), vtbl);
	let waker = unsafe { Waker::from_raw(waker) };

	cb(&amp;amp;waker)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we create a stack allocated &lt;code&gt;Waker&lt;/code&gt; that creates an owned &lt;code&gt;CRawWaker&lt;/code&gt; only whenever it gets cloned. We could bypass this step, if we could rely on the calling convention of the underlying &lt;code&gt;RawWaker&lt;/code&gt;'s vtable, but we cannot, thus we must allocate. Additional optimizations could be made to improve allocation performance, but that can be done at a future point.&lt;/p&gt;
&lt;h2&gt;&lt;code&gt;Future&lt;/code&gt; wrapper&lt;/h2&gt;
&lt;p&gt;Having all the boilerplate set up, we can now create an automatic wrapper for our traits. CGlue already has provisions for this, we just need to add &lt;code&gt;future&lt;/code&gt; module to &lt;code&gt;cglue_gen::ext&lt;/code&gt;, with the following implementation:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;pub trait Future {
	type Output;

	#[custom_impl(
		// Types within the C interface other than self and additional wrappers.
		{
			cx: &amp;amp;CRefWaker,
			out: &amp;amp;mut MaybeUninit&amp;lt;Self::Output&amp;gt;,
		},
		// Unwrapped return type
		bool,
		// Conversion in trait impl to C arguments (signature names are expected).
		{
			let mut out_v = MaybeUninit::uninit();
			let out = &amp;amp;mut out_v;
			let cx = CRefWaker::from(cx.waker());
			let cx = &amp;amp;cx;
		},
		// This is the body of C impl minus the automatic wrapping.
		{
			cx.with_waker(|waker| {
				let mut cx = Context::from_waker(waker);
				match this.poll(&amp;amp;mut cx) {
					Poll::Ready(v) =&amp;gt; {
						out.write(v);
						true
					}
					_ =&amp;gt; false
				}
			})
		},
		// This part is processed in the trait impl after the call returns (impl_func_ret).
		{
			if ret {
				Poll::Ready(unsafe { out_v.assume_init() })
			} else {
				Poll::Pending
			}
		},
	)]
	fn poll(self: Pin&amp;lt;&amp;amp;mut Self&amp;gt;, cx: &amp;amp;mut Context) -&amp;gt; Poll&amp;lt;Self::Output&amp;gt;;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This allows us transparently wrap &lt;code&gt;Waker&lt;/code&gt; with &lt;code&gt;CRefWaker&lt;/code&gt; without the caller having to care about it. Now, it may seem complex, but all the above attribute does is replace default CGlue codegen with different function parameters and bodies at strategic places. Here's the expansion:&lt;/p&gt;
&lt;p&gt;First, the C wrapper:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;extern &amp;quot;C&amp;quot; fn cglue_wrapped_poll&amp;lt;
	CGlueC: /* ... */*
&amp;gt;,
	CGlueCtx: /* ... */,
	CGlueAOutput,
&amp;gt;(
	cont: ::core::pin::Pin&amp;lt;&amp;amp;mut CGlueC&amp;gt;,
	/* Note the first block being placed here */
	cx: &amp;amp;crate::task::CRefWaker,
	out: &amp;amp;mut ::core::mem::MaybeUninit&amp;lt;CGlueAOutput&amp;gt;,
) 
	/* Note the second attr param is placed here */
	-&amp;gt; bool
where
	CGlueC::ObjType: for&amp;lt;'cglue_b&amp;gt; Future&amp;lt;Output = CGlueAOutput&amp;gt;,
{
	let (this, ret_tmp, cglue_ctx) = cont.cobj_pin_mut();
	let ret = {
		/* Note the fourth block being placed here */
		cx.with_waker(|waker| {
			let mut cx = ::core::task::Context::from_waker(waker);
			match this.poll(&amp;amp;mut cx) {
				::core::task::Poll::Ready(v) =&amp;gt; {
					out.write(v);
					true
				}
				_ =&amp;gt; false,
			}
		})
	};
	ret
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And then, the Rust side:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;#[inline(always)]
fn poll(
	self: ::core::pin::Pin&amp;lt;&amp;amp;mut Self&amp;gt;,
	cx: &amp;amp;mut ::core::task::Context,
) -&amp;gt; ::core::task::Poll&amp;lt;Self::Output&amp;gt; {
	let __cglue_vfunc = self.get_vtbl().poll;
	/* Note the third block being placed here */
	let mut out_v = ::core::mem::MaybeUninit::uninit();
	let out = &amp;amp;mut out_v;
	let cx = crate::task::CRefWaker::from(cx.waker());
	let cx = &amp;amp;cx;
	/* These lines are auto generated */
	let cont = self.ccont_pin_mut();
	let cx = cx;
	let out = out;
	let mut ret = __cglue_vfunc(cont, cx, out);
	{
		/* Note the fifth block being placed here */
		if ret {
			::core::task::Poll::Ready(unsafe { out_v.assume_init() })
		} else {
			::core::task::Poll::Pending
		}
	}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Putting it all together&lt;/h2&gt;
&lt;p&gt;Having built everything, it becomes as simple as following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;// Any future
async fn hii() -&amp;gt; u64 {
	42
}

// Turn it into CGlue object
let obj = trait_obj!(hii() as Future);

// Pass it to an executor
assert_eq!(pollster::block_on(obj), 42);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In addition, we get to do the same with &lt;code&gt;futures&lt;/code&gt; streams and sinks:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;let items = [42, 43, 42];

let obj = trait_obj!(futures::stream::iter(items) as Stream);

assert_eq!(pollster::block_on(obj.collect::&amp;lt;Vec&amp;lt;_&amp;gt;&amp;gt;()), items);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, of course, there are caveats. For one, you will not be able to successfully pass a &lt;code&gt;tokio&lt;/code&gt; based future created in one shared library, back to the main executable's tokio executor, because tokio relies on thread-local storage, which is not shared across libs, but any generic future should work, and perhaps one day, the crabi gods will bless us with universal stable ABI.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/h33p/cglue/releases/tag/v0.3.4"&gt;CGlue 0.3 is out now&lt;/a&gt;, which, when opting in the &lt;code&gt;task&lt;/code&gt;, and &lt;code&gt;futures&lt;/code&gt; features, enables transparent wrappers for the aforementioned traits. Additional work would involve native support for AFIT and separating &lt;code&gt;Send&lt;/code&gt; and &lt;code&gt;!Send&lt;/code&gt; types, at which point CGlue's async story should be complete.&lt;/p&gt;
</summary><category term="Rust"></category></entry><entry><title>What If Your Body Was Just a Vessel?</title><link href="https://blaz.is/blog/post/eckhart-tolle-books" rel="alternate"></link><published>2024-04-22T17:46:20+00:00</published><updated>2024-04-22T17:46:20+00:00</updated><author><name>auri</name></author><id>https://blaz.is/blog/post/eckhart-tolle-books</id><summary type="html">&lt;p&gt;I recently read 2 of Eckhart Tolle's books - &amp;quot;The Power of Now&amp;quot; and &amp;quot;A New Earth&amp;quot;. Without a doubt, some of the most important books I've read in a really long time. Instead of trying to explain them through content, though, let me lead you through a hypothetical scenario.&lt;/p&gt;
&lt;p&gt;Imagine you are a God. The whole reality, the whole universe, everything is your creation. Every person you know, every concept you grasp, everything is a reminiscent of the world beyond, the world outside the dream you live in. In this world, your body, and everyone you know is you. You are one, just like the single lone electron in one electron theory, performing every single duty of the universe. The body and the brain you subjectively have confines everything you remember, feel, and experience into one perspective, and you can't see beyond that (something akin to privilege separation found in computers), but it's not the entire you.&lt;/p&gt;
&lt;p&gt;In classical computers we have a kernel preemptively scheduling thousands of processes to work in seemingly parallel nature. However, if you look closely, it's not many individual applications working independently - it's one kernel (ignoring multicore setups), driving the world of the computer one step at a time, by constantly switching roles, by constantly entering different process' execution state and acting on behalf of it. The processes are not independent, the processes are not acting by themselves. They are being acted upon by the one party that pulls all the strings - the kernel, and at even lower level, the CPU.&lt;/p&gt;
&lt;p&gt;What if your experience is just an experience of a single process in an infinitely vast experience of unfathomably larger Being? Every moment, the Being cycles through every single subjective experience in the universe, and experiences it. Churning through everything, femtosecond by femtosecond. What if your body is just a vessel for this larger consciousness, and your main purpose is to spread the knowledge that helps every single body shape their brains to become more self aware, and by extension - bring more power in self-expression of the Being's true nature?&lt;/p&gt;
&lt;p&gt;The books ultimately aim to make you question the nature of your reality with a few key ideas at hand - acceptance of reality leads to peace, escaping reality leads to pain and suffering. Metaphysically speaking, the scenario I described earlier is neither provable nor disprovable - it goes outside the confines of one's subjective view of reality. However, bringing your mind to capacity to at least entertain these scenarios as plausible, may allow you to be more at peace when confronted with the incomprehensible, such as death. Tolle wants you to accept that death is coming, and it's neither good nor bad, it's just reality. Tolle wants you to consider that every moment you spend ruminating about the past, or worrying about the future is a moment spent not living in the only place where you can live - the present moment. Tolle wants you to invert your view of reality, claiming that &amp;quot;life is a dancer, and you are the dance&amp;quot;. Tolle is suggesting a way to view our experience in a way that alleviates most of pain and suffering.&lt;/p&gt;
&lt;p&gt;Now, I have some points of contention with those books. They are written in an overly confident, extremely pretentious manner. That ultimately reminds me of the overly preachy way religion is taught, and is naturally repulsive to me, because the unexplainable is being explained by &amp;quot;just trust me bro&amp;quot;. However, if you are able to suspend your disbelief, and process those ideas in a manner that is more digestible by you, I think you would be able to change your perspective in a good way, and I would wholeheartedly recommend you to pick these books up.&lt;/p&gt;
</summary><category term="Personal"></category><category term="Book Club"></category></entry><entry><title>Announcing mfio - Completion I/O for Everyone</title><link href="https://blaz.is/blog/post/mfio-release" rel="alternate"></link><published>2023-12-07T17:00:00+00:00</published><updated>2023-12-07T17:00:00+00:00</updated><author><name>auri</name></author><id>https://blaz.is/blog/post/mfio-release</id><summary type="html">&lt;p&gt;I'm extremely proud to announce the first release of &lt;code&gt;mfio&lt;/code&gt;, &lt;code&gt;mfio-rt&lt;/code&gt;, and &lt;code&gt;mfio-netfs&lt;/code&gt;! This is the most flexible Rust I/O framework, period. Let's get into it, but before that, warning: this is going to be a dense post, so if you want a higher level overview, &lt;a href="https://youtu.be/EBAC1KcjR28"&gt;check out the release video&lt;/a&gt; :)&lt;/p&gt;
&lt;p&gt;&lt;code&gt;mfio&lt;/code&gt; is an ambitious project that builds on the ideas of &lt;a href="https://blaz.is/blog/post/no-compromises-io/"&gt;No compromises I/O&lt;/a&gt;. In the YouTube video, I say &lt;code&gt;mfio&lt;/code&gt; is a framework defined by 2 traits, so, let's have a look at them:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;#[cglue_trait]
pub trait PacketIo&amp;lt;Perms: PacketPerms, Param&amp;gt;: Sized {
    fn send_io(&amp;amp;self, param: Param, view: BoundPacketView&amp;lt;Perms&amp;gt;);
}

pub trait IoBackend&amp;lt;Handle: Pollable = DefaultHandle&amp;gt; {
    type Backend: Future&amp;lt;Output = ()&amp;gt; + Send + ?Sized;

    fn polling_handle(&amp;amp;self) -&amp;gt; Option&amp;lt;PollingHandle&amp;gt;;
    fn get_backend(&amp;amp;self) -&amp;gt; BackendHandle&amp;lt;Self::Backend&amp;gt;;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;PacketIo&lt;/code&gt; allows the client to submit requests to I/O backends, while &lt;code&gt;IoBackend&lt;/code&gt; allows the client to cooperatively drive the backend. Of course, there is way more to the story than meets the eye.&lt;/p&gt;
&lt;p&gt;The aforementioned traits are the 2 nuclei of the framework, however, users will almost never want to use them - the traits are too low level! A higher level set of traits is provided to make the system not so alien to the user:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;PacketIo&lt;/code&gt; is abstracted through the likes of &lt;code&gt;PacketIoExt&lt;/code&gt;, &lt;code&gt;IoRead&lt;/code&gt;, &lt;code&gt;IoWrite&lt;/code&gt;, &lt;code&gt;SyncIoRead&lt;/code&gt;, &lt;code&gt;SyncIoWrite&lt;/code&gt;, &lt;code&gt;AsyncRead&lt;/code&gt;, &lt;code&gt;AsyncWrite&lt;/code&gt;, &lt;code&gt;std::io::Read&lt;/code&gt;, &lt;code&gt;std::io::Write&lt;/code&gt;.&lt;ul&gt;
&lt;li&gt;Yes, there are synchronous APIs, but they only work when &lt;code&gt;T: PacketIo + IoBackend&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;IoBackend&lt;/code&gt; is abstracted through &lt;code&gt;IoBackendExt&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Completion I/O&lt;/h2&gt;
&lt;p&gt;Let's address the elephant in the room. This crate does not use readiness poll approach, instead, it centers itself around completion I/O. But first, what is completion I/O anyways?&lt;/p&gt;
&lt;h3&gt;Differences from readiness approach&lt;/h3&gt;
&lt;p&gt;Rust typically uses readiness based I/O. In this model, user polls the operating system &amp;quot;hey, I want to read N bytes right here, do you have them ready?&amp;quot; If OS has the data ready, it reads it and returns success, otherwise it returns a WouldBlock error. Then, the runtime registers that file as &amp;quot;waiting for readable&amp;quot; and only attempts to read again when OS signals &amp;quot;hey, this file is available for reading now&amp;quot;.&lt;/p&gt;
&lt;p&gt;In completion I/O, you hand your byte buffer to the I/O system, and it owns it until the I/O request is complete, or is cancelled. For instance, in io_uring, you submit an I/O request to a ring, which is then fulfilled by the operating system. Once you submit the buffer, you have to assume the buffer is borrowed until it's complete.&lt;/p&gt;
&lt;p&gt;The primary difference between these 2 approaches is as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In readiness based I/O, you can typically do 1 simultaneous I/O operation at a time. This is because readiness notifications just indicate whether a file contains data you previously requested, or not - it cannot differentiate which request it is ready for. This is actually great for streamed I/O, like TCP sockets.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In completion I/O, you pay a little extra upfront, but what you get is ability to submit multiple I/O operations at a time. Then, the backend (operating system), can process them in the most efficient order without having to wait for a new requests from the user.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the end, completion I/O can achieve higher performance, because the operating system gets saturated more than in readiness based approach, however, it is typically more expensive at individual request level, which means it typically performs best in scenarios where a single file has multiple readers at different positions at the file, like databases, while not being eye shattering in sequential processing scenarios.&lt;/p&gt;
&lt;h3&gt;Our take&lt;/h3&gt;
&lt;p&gt;We built the system in a way that enables completely safe completion I/O, even when the I/O buffers are being borrowed. We do this through a relatively clunky synchronization system at the start and end of each I/O request - for borrowed data an intermediate buffer is created. However, if you feel wild, and do not want intermediate buffers, at the cost of safety, feel free to build your project with &lt;code&gt;--cfg mfio_assume_linear_types&lt;/code&gt;, which will then assume futures will not get cancelled and give you a performance gain. However, the performance gains are only going to matter in situations where I/O is already incredibly fast, and by fast I mean ramdisk level fast. So don't bother with the switch if what you're doing is processing files - a better way to go about it is reusing heap allocated buffers, which will not allocate intermediates.&lt;/p&gt;
&lt;h2&gt;Integrated async runtime&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;mfio&lt;/code&gt;'s &lt;code&gt;IoBackend&lt;/code&gt; can be thought as a provider of asynchronous processing. Note that we are not aiming to replace &lt;code&gt;tokio&lt;/code&gt; and alike, because we are only targeting the I/O aspect of the runtime, without touching scheduling. This means, there's no task spawning, or built-in time synchronization (by default) - we are only dealing with I/O. You can think of &lt;code&gt;IoBackend&lt;/code&gt; as something slightly more powerful than &lt;code&gt;pollster&lt;/code&gt;, but not by much - unlike &lt;code&gt;pollster&lt;/code&gt;, we enable the backend to cooperate scheduling with the operating system, without the need for high-latency thread signaling, but nothing more.&lt;/p&gt;
&lt;h3&gt;Efficient completion polling&lt;/h3&gt;
&lt;p&gt;It's probably worth to go in a greater detail into this one. To make your I/O have low latency, you can't offload processing to a separate thread. But then, how do process I/O asynchronously, without blocking user's code, or wastefully polling the OS for completion without rest? We instruct the operating system to signal a handle (file descriptor on Unix) whenever I/O operations complete, and then we await for that handle. On Unix platforms, this corresponds to &lt;a href="https://www.man7.org/linux/man-pages/man2/poll.2.html"&gt;&lt;code&gt;poll(2)&lt;/code&gt;&lt;/a&gt;, while on Windows, it is &lt;a href="https://learn.microsoft.com/en-us/windows/win32/api/synchapi/nf-synchapi-waitforsingleobject"&gt;&lt;code&gt;WaitForSingleObject&lt;/code&gt;&lt;/a&gt;. The key enabler of this approach is the discovery that most I/O backends, like &lt;code&gt;iocp&lt;/code&gt;, &lt;code&gt;io_uring&lt;/code&gt;, &lt;code&gt;epoll&lt;/code&gt;, or &lt;code&gt;kqueue&lt;/code&gt; do not have to be polled for using the backend-specific functions, such as &lt;a href="https://man.archlinux.org/man/io_uring_wait_cqe.3.en"&gt;&lt;code&gt;io_uring_wait_cqe(3)&lt;/code&gt;&lt;/a&gt;. Speculation (would need to test): you may receive single-digit performance improvements from using the backend functions, however, as benchmarks show, using &lt;code&gt;poll&lt;/code&gt; and &lt;code&gt;WaitForSingleObject&lt;/code&gt; are sufficiently fast not to be deal-breakers.&lt;/p&gt;
&lt;h3&gt;Integration with other runtimes&lt;/h3&gt;
&lt;p&gt;If &lt;code&gt;mfio&lt;/code&gt; does not have the feature set of &lt;code&gt;tokio&lt;/code&gt; or &lt;code&gt;async_std&lt;/code&gt;, then it is rather useless for real software. Plus, let's be real, nobody is going to switch to an unproven system just because it's fast. That's okay, because on Unix platforms we are able to seamlessly integrate with the biggest async runtimes! We do this by taking the exact same handle we normally poll for, and ask &lt;code&gt;tokio&lt;/code&gt; (or &lt;code&gt;async_std&lt;/code&gt;/&lt;code&gt;smol&lt;/code&gt;) to do it instead. It's that simple! Then, instead of calling &lt;code&gt;backend.block_on&lt;/code&gt;, we do the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;Tokio::run_with_mut(&amp;amp;mut rt, |rt| async move {
	// code goes here
}).await
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Windows could in theory be supported in a similar manner, however, handles are currently not exposed to the same extent in async runtimes, therefore it's just not possible to do at the moment (although, &lt;a href="https://github.com/smol-rs/async-io/pull/152"&gt;this will soon change on async-io end&lt;/a&gt;!) In addition, there appears to be some complications with asynchronously waiting for objects, so it may also be a question whether waiting for said handle would even be possible, without changing &lt;code&gt;mio&lt;/code&gt;/&lt;code&gt;polling&lt;/code&gt; implementations to wait on handles, instead of IOCPs. There appears to be a solution using
&lt;code&gt;NtCreateWaitCompletionPacket&lt;/code&gt; / &lt;code&gt;NtAssociateWaitCompletionPacket&lt;/code&gt; and friends, however, these functions are not well documented, and &lt;a href="https://github.com/j00ru/windows-syscalls/blob/master/x64/csv/nt.csv#L129"&gt;only available since Windows Server 2012&lt;/a&gt;. Basically, the path is there, but it's not as pretty as on Unix.&lt;/p&gt;
&lt;p&gt;Additional aspect worth mentioning is that the system is best used in thread-per-core scenarios, or, in Tokio's case, mfio-backend-per-task. It would work in other scenarios too, however, you would likely run into some inefficiencies. In addition, this recommendation is not yet firm - multithreading story is not solved, but should be worked out over the next year.&lt;/p&gt;
&lt;h2&gt;Colorless system&lt;/h2&gt;
&lt;p&gt;I make a claim that &lt;code&gt;mfio&lt;/code&gt; does not have color, which means, it doesn't matter whether you use it from sync or async runtime. To be fair, depending on how you interpret the color problem, the claim may or may not be true.&lt;/p&gt;
&lt;p&gt;What I mean by lack of color is that the system makes it trivial to create synchronous wrappers of the asynchronous functions. You can see that in the &lt;code&gt;std::io&lt;/code&gt; trait implementations, and &lt;code&gt;SyncIoRead&lt;/code&gt;/&lt;code&gt;SyncIoWrite&lt;/code&gt;. So long as the object you wish to make synchronous wrappers for has both &lt;code&gt;PacketIo&lt;/code&gt; and &lt;code&gt;IoBackend&lt;/code&gt;, you should be trivially able to make it happen. This effectively makes it possible for the user to not care how the backend is implemented. Meanwhile, the backend is always async.&lt;/p&gt;
&lt;h2&gt;Cutting edge backends for OS interfacing&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;mfio-rt&lt;/code&gt; attempts to define a standard async runtime, which can then be implemented in various ways. The &lt;code&gt;native&lt;/code&gt; feature enables built-in backends that interface directly with the OS through various APIs, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;io_uring&lt;/li&gt;
&lt;li&gt;iocp&lt;/li&gt;
&lt;li&gt;epoll/kqueue (leveraging &lt;code&gt;mio&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Threaded standard library fallback&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;iocp&lt;/code&gt; and &lt;code&gt;io_uring&lt;/code&gt; backends enable for far greater random access performance than the likes of &lt;code&gt;std&lt;/code&gt; or &lt;code&gt;tokio&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blaz.is/media/mfio-linux-random0151.jpg" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;mfio-rt&lt;/code&gt; is still at its infancy - we currently have &lt;code&gt;Fs&lt;/code&gt; and &lt;code&gt;Tcp&lt;/code&gt; traits defined that allow the user to perform standard OS related operations, however, the usage differs wildly from typical async runtimes. Ideally, what I'd want to expose is a &lt;code&gt;global&lt;/code&gt; feature flag that allows the user to install a global runtime, that can then be used from regular function calls, like so:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;use mfio_rt::fs;
use mfio::prelude::v1::*;

#[tokio::main]
#[mfio_rt::init(integration = 'Tokio')]
async fn main() {
	// We don't need mutable file with mfio
	let file = fs::File::open(&amp;quot;test.txt&amp;quot;).await.unwrap();

	let mut buf = vec![];
	file.read_to_end(0, &amp;amp;mut buf).await.unwrap();

	println!(&amp;quot;{}&amp;quot;, buf.len());
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Additional thing worth adding is &lt;code&gt;Time&lt;/code&gt; trait so that more applications could be built in a runtime-agnostic way, however, that is not the highest priority, since you can already do sleeps with &lt;code&gt;tokio&lt;/code&gt; or &lt;code&gt;smol&lt;/code&gt;, and also, how many &amp;quot;small&amp;quot; features away are we from being a fully fledged runtime, capable of completely displacing &lt;code&gt;tokio&lt;/code&gt;?&lt;/p&gt;
&lt;h2&gt;Network filesystem&lt;/h2&gt;
&lt;p&gt;As a bonus, we have an &lt;strong&gt;experimental&lt;/strong&gt; network filesystem implementation, going by the name of &lt;code&gt;mfio-netfs&lt;/code&gt;. It was built purely as a test, and is to be considered as a toy, because:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;I have personally encountered hard to reproduce hangs.&lt;/li&gt;
&lt;li&gt;There is absolutely zero encryption, checksumming, or request validation.&lt;/li&gt;
&lt;li&gt;The communication protocol is built on interpreting C struct as bytes, and going the other way around&lt;ul&gt;
&lt;li&gt;It was not a fun 5 hours spent in &lt;code&gt;rr&lt;/code&gt;, trying to figure out why my code would segfault with a nonsensical call stack. However, it was also a good experience, because it let me track down a bug in &lt;code&gt;io_uring&lt;/code&gt; backend implementation. This crash is the type of scary stuff you shouldn't expect from Rust codebase, and yet &lt;code&gt;mfio-netfs&lt;/code&gt; has the potential!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As a result, upon running it, you will see a big warning advising against using it in production, and please listen to it! I have plans to write a proper network filesystem, built on quic/h3, which should potentially achieve much better performance than it is getting now, because the nature of &lt;code&gt;mfio&lt;/code&gt; favors almost-non-sequential message passing that is unlike TCP (which we currently use). However, it is a good sub-2000 LoC example to dig into and see how one would implement an efficient I/O proxy.&lt;/p&gt;
&lt;h2&gt;Exhaustive benchmarks&lt;/h2&gt;
&lt;p&gt;I say &lt;code&gt;mfio&lt;/code&gt; is fast, but how fast? If you wish to see an overview of the results, please see the release video. However, if you wish to look at raw data, check out the reports:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://blaz.is/media/mfio-criterion-windows/report/index.html"&gt;Windows&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blaz.is/media/mfio-criterion-linux/report/index.html"&gt;Linux&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Testing methodology&lt;/h3&gt;
&lt;p&gt;The exact setup is available in &lt;a href="https://github.com/memflow/mfio-bench"&gt;mfio-bench&lt;/a&gt; repo.&lt;/p&gt;
&lt;p&gt;The reports are not the clearest, because I originally planned to only use them in the video, but figured not all data will fit there. Here are some important points:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All graphs are log scale. This allows one to compare 2 backends percentage wise.&lt;/li&gt;
&lt;li&gt;You will find 1ms latency attached to all local results. This is false, there is no latency - files are local.&lt;ul&gt;
&lt;li&gt;However, all benchmarks also include &lt;code&gt;mfio-netfs&lt;/code&gt; results, which go to a remote node. Read size mode results too have no added latency - the remote node is a VM running locally.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Latency mode results are going to a local VM running SMB server, and artificial latency is setup.&lt;ul&gt;
&lt;li&gt;However, &lt;code&gt;mfio-netfs&lt;/code&gt; bypasses SMB - it goes to a &lt;code&gt;mfio-netfs&lt;/code&gt; server running on the same SMB node - identical latency as SMB.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;In latency mode, X axis is the latency (ms), not bytes/s.&lt;/li&gt;
&lt;li&gt;For completion I/O, we set up multiple concurrent I/O requests - up to 64MB of data being in flight, to be precise.&lt;/li&gt;
&lt;li&gt;For &lt;code&gt;glommio&lt;/code&gt;, we did not use &lt;code&gt;DmaFile&lt;/code&gt;, because our reads are unaligned. It is possible &lt;code&gt;glommio&lt;/code&gt; could achieve better performance if it were not using buffered I/O. Same with &lt;code&gt;mfio&lt;/code&gt;, we could achieve better performance, if we used unbuffered I/O, however, that makes us unable to easily perform unaligned reads, so for now, we are sticking with buffered.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Results&lt;/h3&gt;
&lt;p&gt;From the results we can see that &lt;code&gt;mfio&lt;/code&gt; backends, especially &lt;code&gt;io_uring&lt;/code&gt; and &lt;code&gt;iocp&lt;/code&gt; achieve astonishing performance in random tests. In addition, in latency comparison, &lt;code&gt;mfio-netfs&lt;/code&gt; achieves better results than going through SMB on Linux, while on Windows, we have similar results.&lt;/p&gt;
&lt;p&gt;Sequential performance is not the best - OSs can perform read-ahead rather well, making &lt;code&gt;std&lt;/code&gt; perform much better than any completion I/O system. That is the tradeoff - with completion I/O, you have much more complex software architecture, that incurs bigger constant overhead, however, once you start using it its fullest, then that architecture starts to pay off big time.&lt;/p&gt;
&lt;h2&gt;Learnings from No compromises I/O&lt;/h2&gt;
&lt;p&gt;For &lt;code&gt;mfio&lt;/code&gt; core, I attempted to use the model detailed in the &lt;a href="https://blaz.is/blog/post/no-compromises-io/"&gt;No compromises I/O&lt;/a&gt; post, however, I soon realized that attaching a backend to every I/O request future is undesirable and too complicated to handle. Instead, I opted for a different approach where each top level future is combined with the I/O backend, and both are then processed sequentially. This model makes it not possible to efficiently share the I/O backend across threads, however, the method's simplicity outweighs the potential benefits.&lt;/p&gt;
&lt;p&gt;In addition, I made the base I/O objects be simple and do away with the streams and callbacks. The reason for that is there's more performance to be gained from implementing the most commonly used packets as standard to the system. The flexibility mentioned in the post is still there, however, it is now opt-in rather than forcing you to take performance loss.&lt;/p&gt;
&lt;p&gt;It is natural to have the design change over the months, and I'm glad to have messed around with the original design, because I learnt quite a bit about making better software architecture decisions.&lt;/p&gt;
&lt;h2&gt;Closing words&lt;/h2&gt;
&lt;p&gt;This is a big release, and it has been long coming. I do have a few important shaping changes to do, but the overall spirit of the library should stay the same. In addition, I have quite a few project ideas building on top of &lt;code&gt;mfio&lt;/code&gt;, and of course, migrating &lt;code&gt;memflow&lt;/code&gt; to async. Should you want to try the system out, head to the &lt;a href="https://github.com/memflow/mfio"&gt;mfio repo&lt;/a&gt;, and do set it up! If you have any questions or feedback, feel free to get in touch, it is greatly appreciated.&lt;/p&gt;
</summary><category term="memflow"></category><category term="Design Choices"></category><category term="Rust"></category></entry><entry><title>What If We Pretended That a Task = Thread?</title><link href="https://blaz.is/blog/post/lets-pretend-that-task-equals-thread" rel="alternate"></link><published>2023-05-22T21:12:20+00:00</published><updated>2023-05-22T21:12:20+00:00</updated><author><name>auri</name></author><id>https://blaz.is/blog/post/lets-pretend-that-task-equals-thread</id><summary type="html">&lt;p&gt;In my &lt;a href="https://blaz.is/blog/post/future-send-was-unavoidable/"&gt;previous post&lt;/a&gt; I made a fairly inaccurate attempt at describing one of the primary problems plaguing current async ecosystem and an idea of solving it. Redditors criticized me, and rightfully so - the post was written in one go without giving it time to rest and the solution was not optimal. So I thought about it more, and I think I am on to something.&lt;/p&gt;
&lt;h2&gt;Recap&lt;/h2&gt;
&lt;p&gt;The core premise of my original post was stating that &lt;code&gt;Send&lt;/code&gt;+&lt;code&gt;Sync&lt;/code&gt; are constructs that work at thread boundaries and don't map well to async tasks (top level futures). I also stated that while these traits are core to memory safety, they introduce arbitrary limitations in async context, such as being unable to use fast single-threaded abstractions like &lt;code&gt;Rc&lt;/code&gt; and &lt;code&gt;RefCell&lt;/code&gt; in multi-threaded runtimes, even though no actual safety problems are present.&lt;/p&gt;
&lt;p&gt;The core link was equating threads in Rust async context to CPU cores in OS scheduling context - it makes as much sense for an async task to be aware of the underlying thread executing it, as it is for an OS thread to be aware of which CPU core it's executing on. You may want to know and control the behavior in performance critical scenarios, but for a vast majority of applications it doesn't matter, and you certainly don't want to associate data to a particular lower-level execution unit (Thread for tasks and CPU core for threads). So my questions were: why &lt;code&gt;tokio::spawn&lt;/code&gt; needs a &lt;code&gt;Send&lt;/code&gt; future? Why would you need to specify &lt;code&gt;Send&lt;/code&gt;-iness in async traits? Why is this complexity forced on the end user? And finally, can we hide this complexity at the runtime level?&lt;/p&gt;
&lt;p&gt;The main problem why a runtime can't move &lt;code&gt;!Send + !Sync&lt;/code&gt; tasks across multiple threads is because they may access Thread Local Storage (TLS) in their code, allowing one to clone &lt;code&gt;Rc&lt;/code&gt; objects into potentially different threads and breaking the safety guarantees. The idea for solving this was to create a new &lt;code&gt;AsyncSend&lt;/code&gt; trait and have a future implement it, unless there are raw pointers involved or some code of it accesses TLS. The problem with that approach is that it introduces an additional trait, and requires developing a whole new function effect system that would be incredibly complicated to pull off. Can we do simpler and better?&lt;/p&gt;
&lt;h2&gt;Let's Pretend&lt;/h2&gt;
&lt;p&gt;Let's pretend a task is a thread! It's that simple! While a Thread and a Task have some key differences, they both define a unit of execution, and it makes little sense for an async task to concern itself with the lower level unit of execution - a thread.&lt;/p&gt;
&lt;p&gt;In my original idea the async runtime would have been responsible for promising the compiler that they were holding top level futures (tasks), meaning a runtime would probably have to use unsafe to move &lt;code&gt;!Send&lt;/code&gt; futures across threads. What if instead of that, an async runtime was responsible for setting up a fake &lt;code&gt;Thread&lt;/code&gt; for each task? This part wouldn't even need to be unsafe, the only unsafe part would be moving the tasks across threads, using a wrapper structure, like so:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;struct Task&amp;lt;F&amp;gt; {
    future: F,
    thread: TaskThread,
}

unsafe impl&amp;lt;F: Future&amp;gt; Send for Task {}
unsafe impl&amp;lt;F: Future&amp;gt; Sync for Task {}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So long as the &lt;code&gt;future&lt;/code&gt; and the &lt;code&gt;thread&lt;/code&gt; are linked to each other and there is proper synchronization transfering ownership across actual OS threads, the task should be &lt;code&gt;Send&lt;/code&gt;. And even this &lt;code&gt;Task&lt;/code&gt; definition could live within the standard library!&lt;/p&gt;
&lt;p&gt;Now, you're probably confused by the meaning of a fake thread. A fake thread corresponds to a unique per-task &lt;code&gt;ThreadID&lt;/code&gt; that is loaded in by the async runtime. It masks &lt;code&gt;thread::current&lt;/code&gt; and is used by the &lt;code&gt;LocalKey&lt;/code&gt; (TLS) to give per-task static data. Code-wise, you'd end up with a distinction between OS threads and Task threads, like so:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;// Fake thread data for an async task
pub struct TaskThread {
    inner: TaskInner,
    // This may contain actual TLS data.
    context: Pin&amp;lt;Box&amp;lt;/* ... */&amp;gt;&amp;gt;,
}

// This is how a `Thread` is currently defined in STD
pub struct Thread {
    inner: Pin&amp;lt;Arc&amp;lt;Inner&amp;gt;&amp;gt;,
}

// Inner is a struct in STD, but we may want to have an enum-based distinction here
enum Inner {
    Os(OsInner),
    Task(TaskInner),
}

enum OsInner {
    name: Option&amp;lt;CString&amp;gt;, // Guaranteed to be UTF-8
    id: ThreadId,
    parker: Parker,
}

enum TaskInner {
    name: Option&amp;lt;String&amp;gt;,
    id: ThreadId,
    // Thread::unpark would simply call Waker::wake_by_ref on a task.
    waker: Waker,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To &amp;quot;enter&amp;quot; the fake task thread, async runtime would call &lt;code&gt;thread::enter_task&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;// Runtime gets a task to run. Internally it may steal the task from another thread - it's a runtime implementation detail.
let task = self.tasks.get()?;

// Create a future `Context` that attaches the internally stored waker.
let mut cx = task.task_context();

// Enters the task - panics if called from within task, although may not even be necessary.
thread::enter_task(&amp;amp;mut task, |future| {
    match future.poll(&amp;amp;mut cx) {
        /* ... */
    }
});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this, you no longer need a &lt;code&gt;Send&lt;/code&gt; bound in async spawn function, you no longer need to specify &lt;code&gt;AsyncTrait::some_function(): Send&lt;/code&gt;, and generally speaking as a user, your life becomes much easier! However, it also brings Rust further away from bare metal. This can be solved relatively easily with the following 2 APIs:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;// Get the underlying OS thread
let os_thread = thread::os_current();

// Access `LocalKey` in the OS thread context
GLOBAL_DATA.os_with(|data| {
    // The function is virtually the same as `LocalKey::with`, except for the following differences:
    // - The closure must be `Send`. This prevents leaking `!Send` data outside the closure with assignment.
    // - The closure must return `Send` data. This prevents leaking `!Send` data outside the closure through return.
    // !Send argument is allowed, because that is the point of TLS, and poses no risk if it's not being leaked.
});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Given that &lt;code&gt;Thread&lt;/code&gt; would not necessarily refer to an OS thread anymore, we may also want to rename the structure to &lt;code&gt;Task&lt;/code&gt;, which I believe should be doable through an edition change. The &lt;code&gt;os_with&lt;/code&gt; function in &lt;code&gt;LocalKey&lt;/code&gt; would also be faster than the regular task-local &lt;code&gt;with&lt;/code&gt; function. Users may choose one over the other, but the beautiful thing is that neither of them would prevent async runtimes from moving threads around. We may also invert this completely, and have the following TLS API:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;GLOBAL_DATA.with(|data| {
    // This closure needs to be `Send`, and return data must also be `Send`.
    // If the conditions mismatch, compiler directs users to use `LocalKey::task_with`.
});

GLOBAL_DATA.task_with(|data| {
    // This is the same as current `LocalKey::with` - no restrictions on sendability, but
    // the data would be per-task, and the implementation may be slower than `LocalKey::with`.
});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Third alternative would be to keep threads and async part separate - have both &lt;code&gt;Task&lt;/code&gt; and &lt;code&gt;Thread&lt;/code&gt;, with a higher level abstraction linking async tasks and OS threads together. This would keep Threads very explicitly linked to OS backed threads, while TLS could be redefined to be both Task and Thread Local Storage. These details are certainly important to get correct, and I don't have all the answers, but whichever way we go, it won't prevent async runtimes from transfering the tasks around.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Sending thread-unsafe futures poses no obvious risk for far majority of cases, however, combining them with TLS is risky, because TLS may allow one to leak data across multiple futures, and consequently, multiple threads. This can be solved by redefining TLS as &amp;quot;Task Local Storage&amp;quot; and &lt;code&gt;Thread&lt;/code&gt; as a &lt;code&gt;Task&lt;/code&gt;. With this, async runtimes would be able to move &lt;code&gt;!Send&lt;/code&gt; futures across different threads, thus removing the need from distinguishing between &lt;code&gt;Send&lt;/code&gt; and &lt;code&gt;!Send&lt;/code&gt; spawn functions.&lt;/p&gt;
&lt;p&gt;I asked GPT-4 for their thoughts, and this is what they had to say:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;... while your proposed solution sounds promising and could potentially simplify async programming in Rust, it would be prudent to seek further reviews and perhaps implement a proof-of-concept to further validate your ideas.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Thus, I'd really like to hear your thoughts!&lt;/p&gt;
</summary><category term="Design Choices"></category><category term="Rust"></category></entry><entry><title>`Future + Send` Was (Not) Unavoidable</title><link href="https://blaz.is/blog/post/future-send-was-unavoidable" rel="alternate"></link><published>2023-05-09T22:02:35+00:00</published><updated>2023-05-09T22:02:35+00:00</updated><author><name>auri</name></author><id>https://blaz.is/blog/post/future-send-was-unavoidable</id><summary type="html">&lt;p&gt;If you've done any async programming in Rust, you will have encountered the problem, where you can't run &lt;code&gt;tokio::spawn&lt;/code&gt;, because the &lt;code&gt;future cannot be sent between threads safely&lt;/code&gt;. This is a tale about why this problem exists, how it could be solved for good, and why it's not trivial to do without breaking existing code.&lt;/p&gt;
&lt;h2&gt;Rust Thread Safety&lt;/h2&gt;
&lt;p&gt;Rust ensures thread safety through 2 really ingenious mechanisms - lifetimes and &lt;code&gt;Send&lt;/code&gt;/&lt;code&gt;Sync&lt;/code&gt; traits. We won't talk about lifetimes, but we will instead focus on those 2 traits. Here are the core rules surrounding them:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If &lt;code&gt;T: Send&lt;/code&gt;, then you can send &lt;code&gt;T&lt;/code&gt; to another thread.&lt;/li&gt;
&lt;li&gt;If &lt;code&gt;T: Sync&lt;/code&gt;, then you can have &lt;code&gt;T&lt;/code&gt; in multiple threads.&lt;/li&gt;
&lt;li&gt;If &lt;code&gt;&amp;amp;T: Send&lt;/code&gt;, then &lt;code&gt;T: Sync&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Everything is &lt;code&gt;Send&lt;/code&gt;, except for pointers and &lt;code&gt;Rc&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Everything is &lt;code&gt;Sync&lt;/code&gt;, except for pointers, &lt;code&gt;Rc&lt;/code&gt;, and &lt;code&gt;UnsafeCell&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If everything in &lt;code&gt;T&lt;/code&gt; is &lt;code&gt;Send&lt;/code&gt;, then &lt;code&gt;T: Send&lt;/code&gt; (if something is &lt;code&gt;!Send&lt;/code&gt;, then &lt;code&gt;T: !Send&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;You may manually implement &lt;code&gt;Send&lt;/code&gt; and &lt;code&gt;Sync&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Everything is governed by these 7 points. And all you need to do to ensure thread safety is require a &lt;code&gt;Send&lt;/code&gt; bound in your arguments, like here:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;pub fn spawn&amp;lt;F, T&amp;gt;(f: F) -&amp;gt; JoinHandle&amp;lt;T&amp;gt;where
    F: FnOnce() -&amp;gt; T + Send + 'static,
    T: Send + 'static,
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is incredibly simple yet genius! Where does the problem arise?&lt;/p&gt;
&lt;h2&gt;Threads Meet Tasks&lt;/h2&gt;
&lt;p&gt;Rust thread safety hinges upon one assumption - the core unit of execution is a thread. Everything that happens in a thread happens sequentially, and if &lt;code&gt;!Send&lt;/code&gt; data is on the thread, it's okay, because we know it's contained within the thread. But what is a thread anyways?&lt;/p&gt;
&lt;p&gt;A thread is an OS backed unit of execution. It's how you get CPUs doing multiple things, seemingly at the same time. Everything is actually happening once (or twice, if you count SMT) at a time on a CPU core. However, to build up an illusion of concurrency, the CPU, in conjunction with the OS kernel, are switching those threads around like crazy. Each thread has its own state, its own unique &amp;quot;global&amp;quot; view (to specific processes), and are isolated thanks to the OS and CPU architecture mechanisms. A thread also has per-thread local storage. Great! Now, how do we compare with tasks?&lt;/p&gt;
&lt;p&gt;A task in async context is a runtime backed unit of execution. It's how you get threads doing multiple things, seemingly at the same time. Everything is actually happening once at a time on a program's thread. However, to build up an illusion of concurrency, the thread, in conjunction with the async runtime, are switching those tasks around like crazy. Each task has its own state, the same global view to the process, and are isolated merely by compiler safety mechanisms. A task may also have per-task local storage, but it also has access to Thread Local Storage (TLS). Looks fairly similar to a thread, doesn't it? And here's the kicker, I don't have to worry about threads being moved from one CPU core to another! So, why do I have to care about tasks moving from one thread to another?&lt;/p&gt;
&lt;h3&gt;Why the &lt;code&gt;Send&lt;/code&gt;?&lt;/h3&gt;
&lt;p&gt;We have to remember, that async runtimes still run within the safety rules, that means if entire &lt;code&gt;Future&lt;/code&gt; is &lt;code&gt;!Send&lt;/code&gt;, then the runtime must stay within the Rust rules, and not move tasks across multiple threads. But, what does make a task &lt;code&gt;!Send&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;This results in &lt;code&gt;Send&lt;/code&gt; future:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;let value = *mutex.lock().await;
let foo = Rc::new(RefCell::new(1));
*foo.borrow_mut() = value;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This results in &lt;code&gt;!Send&lt;/code&gt; future:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;let foo = Rc::new(RefCell::new(1));
let value = *mutex.lock().await;
*foo.borrow_mut() = value;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The difference is simple - &lt;code&gt;foo&lt;/code&gt; is being held across the &lt;code&gt;await&lt;/code&gt; point. Very subtle! The idea is that, a multithreaded runtime &lt;em&gt;may&lt;/em&gt; move the task to another thread, therefore the only way to ensure that doesn't happen at the wrong time at the typesystem level is to disallow it altogether, therefore the future becomes &lt;code&gt;!Send&lt;/code&gt;. This is not good, because it effectively forces you to add &lt;code&gt;Send&lt;/code&gt; bounds to your types, and especially in async traits this becomes incredibly verbose and cumbersome to work with. However, it doesn't have to be this way!&lt;/p&gt;
&lt;h2&gt;&lt;code&gt;AsyncSend&lt;/code&gt; - the Lord and Savior&lt;/h2&gt;
&lt;p&gt;Why don't we extend the core &lt;code&gt;Send&lt;/code&gt;/&lt;code&gt;Sync&lt;/code&gt; traits with one more - &lt;code&gt;AsyncSend&lt;/code&gt; (or &lt;code&gt;TaskSend&lt;/code&gt;)? The idea is simple - define &lt;code&gt;AsyncSend&lt;/code&gt; as a strict superset of &lt;code&gt;Send&lt;/code&gt;, and here are the full rules:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If &lt;code&gt;T: AsyncSend&lt;/code&gt;, then you can send &lt;code&gt;T&lt;/code&gt; to another task.&lt;/li&gt;
&lt;li&gt;If &lt;code&gt;T: Send&lt;/code&gt;, then &lt;code&gt;T: AsyncSend&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If &lt;code&gt;T: AsyncSend&lt;/code&gt;, then &lt;code&gt;Rc&amp;lt;T&amp;gt;: AsyncSend&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If everything in &lt;code&gt;T&lt;/code&gt; is &lt;code&gt;AsyncSend&lt;/code&gt;, then &lt;code&gt;T: AsyncSend&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then, async runtimes can change their signatures to require &lt;code&gt;AsyncSend&lt;/code&gt;, instead of &lt;code&gt;Send&lt;/code&gt;, and with slight bit of &lt;code&gt;unsafe&lt;/code&gt;, make a promise to the compiler that they hold top level futures and pass them across different threads. Sounds perfect, right?&lt;/p&gt;
&lt;h3&gt;The Baddie &lt;code&gt;thread_local!&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The one thing breaking this idea apart is thread local variables. Consider the following code:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;thread_local! {
	pub static FOO: Rc&amp;lt;RefCell&amp;lt;u32&amp;gt;&amp;gt; = Rc::new(RefCell::new(1));
}

async fn do_locked(lock: Mutex&amp;lt;u32&amp;gt;) {
	// Load `FOO` from TLS
	let foo = FOO.with(|foo| foo.clone());
	// Wait for lock, potentially switching threads
	let mut guard = lock.lock().await;
	// Borrow, on another thread already
	*foo.borrow_mut() = *guard;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If &lt;code&gt;do_locked&lt;/code&gt; was &lt;code&gt;AsyncSend&lt;/code&gt;, you could break the safety guarantees, and have 2 of these tasks borrow &lt;code&gt;FOO&lt;/code&gt; in different threads. Essentially, &lt;code&gt;thread_local!&lt;/code&gt; would wrongfully allow you to leak &lt;code&gt;!Send&lt;/code&gt; types across task boundaries. This is a problem, and before going further I strongly encourage you to think of an equivalent to usage of &lt;code&gt;thread_local!&lt;/code&gt; in async context, but for regular threads. &lt;em&gt;Take your time...&lt;/em&gt; The equivalent would be allowing a thread to access CPU-local data. And here's a handy table showing the equivalents:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Task&lt;/em&gt;          | &lt;em&gt;Thread Equivalent&lt;/em&gt;
:-------------  | :----------------
&lt;code&gt;task_local!&lt;/code&gt;   | &lt;code&gt;thread_local!&lt;/code&gt;
&lt;code&gt;thread_local!&lt;/code&gt; | &lt;a href="https://0xax.gitbooks.io/linux-insides/content/Concepts/linux-cpu-1.html"&gt;Per-CPU vars&lt;/a&gt;/static vars&lt;/p&gt;
&lt;p&gt;Tell me, have you ever thought of accessing per-CPU variables on your program? I guess you can't do that! Okay, if you can't do that, another alternative would be static vars. Does Rust allow you to put &lt;code&gt;!Send&lt;/code&gt; types there? No! So does it make sense for an async task to do the equivalent of that? No! So, can we solve it somehow?&lt;/p&gt;
&lt;h2&gt;Is There Any Hope?&lt;/h2&gt;
&lt;p&gt;I have a question for you - are you satisfied with the current state of async bounds? If you could break Rust, would you change &lt;code&gt;thread_local!&lt;/code&gt; to not work inside async environment? Would you pull the trigger? I doubt we can actually do that, but also, do you think adding &lt;code&gt;where Trait::func(): Send&lt;/code&gt; to every async function of the trait is sustainable for the next 10 years? Is there a way around it?&lt;/p&gt;
&lt;h3&gt;Function Properties&lt;/h3&gt;
&lt;p&gt;This is a rather complicated idea I have, and it may not be the best solution (I'd actually love to hear your ideas), but it's to have function properties that you can specify - something in-between calling conventions and traits. Here's a hypothetical example:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;pub fn func1() !tls + !panic -&amp;gt; u32 {
	// ...
	val
}

fn inferred() -&amp;gt; u32 {
	func1()
}

pub fn func2() !tls -&amp;gt; u32 {
	// ...
	inferred()
}

pub mut FUNC: Option&amp;lt;fn() !tls -&amp;gt; usize&amp;gt; = None;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These properties signify what can and can't happen in a function. If you want to say that the function doesn't panic, you'd add &lt;code&gt;!panic&lt;/code&gt;. If you want to say that the function doesn't use TLS, you'd add &lt;code&gt;!tls&lt;/code&gt;. These properties would be inferred at the crate level - only public APIs, function pointers, and traits would need to specify this (because changes in properties are breaking from semver perspective). Even then it's optional - these properties are opt-out, meaning if you don't specify them, the compiler assumes the effects can happen. Finally, bridging this to async code, a future that only consists of &lt;code&gt;!tls&lt;/code&gt; code and &lt;code&gt;AsyncSend&lt;/code&gt; types automatically becomes &lt;code&gt;AsyncSend&lt;/code&gt;, and to make things even simpler, one could imply &lt;code&gt;!tls&lt;/code&gt; in async traits. This would make async usage very concise, and also trivially buy us an ability to prove lack of panics, which is another topic of interest for many people! In the future, you could even relax the requirements based on the type of TLS var being accessed, in different stages of flexibility:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Start with any &lt;code&gt;thread_local!&lt;/code&gt; access emitting &lt;code&gt;tls&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Do not emit &lt;code&gt;tls&lt;/code&gt;, if &lt;code&gt;T: Send&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Do not emit &lt;code&gt;tls&lt;/code&gt;, if &lt;code&gt;T: !AsyncSend | Send&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Naturally, the third step is very complex to check, so it's quite far off, but the second one could be fairly trivial to specialize!&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Rust is a language that got many things right, and it's the reason why many people love it so much! However, with new additions cracks inevitably start to form, and it's not very pleasant to see where async is headed. However, the cracks are small and there ultimately are ways to walk around these problems. The biggest obstacle so far for keeping usability of async code tidy is Thread Local Storage. I propose a possible solution to the problem, in the form of function properties. These properties allow one to prove that a &lt;code&gt;Future&lt;/code&gt; is truly thread-independent, which would be instrumental for moving &lt;code&gt;!Send&lt;/code&gt; futures across threads.&lt;/p&gt;
</summary><category term="Design Choices"></category><category term="Rust"></category></entry><entry><title>We Need Type Information, Not Stable ABI</title><link href="https://blaz.is/blog/post/we-dont-need-a-stable-abi" rel="alternate"></link><published>2023-01-15T17:42:26+00:00</published><updated>2023-01-15T17:42:26+00:00</updated><author><name>auri</name></author><id>https://blaz.is/blog/post/we-dont-need-a-stable-abi</id><summary type="html">&lt;p&gt;I've always been a strong proponent of dynamic linking, and last month a new Rust proposal dropped - &lt;a href="https://github.com/rust-lang/rust/pull/105586"&gt;interoperable_abi&lt;/a&gt;. It sparked discussions, plans forward, and my excitement was immeasurable. Then, one member said something that got me thinking. And after much thinking, I don't think ABI is what we need today. What we need is type information. Let me explain.&lt;/p&gt;
&lt;h2&gt;Preface&lt;/h2&gt;
&lt;p&gt;Rust has been a black hole of programming for many years - everything is being rewritten in it, and for a good reason - Rust is an incredibly powerful, fast, productive, yet safe language that is a joy to write in. One of the biggest shortcomings of the language is its bloat - compile times are rather slow, and binaries are relatively large. There's a reason for it - Rust prefers to compile everything from source, and code usually makes extensive use of its powerful type system.&lt;/p&gt;
&lt;p&gt;Dynamic linking is a possible solution - unchanged libraries don't need to be recompiled, there is potentially stronger (compiled) code reuse across projects, and you gain the ability to load features incrementally. This sounds like a dream! So what's stopping us from doing it?&lt;/p&gt;
&lt;h2&gt;The Problem&lt;/h2&gt;
&lt;p&gt;One of the universally accepted truths, or an axiom if I may, is that for dynamic linking you need to stabilize the language ABI. And whenever such idea floats up, people are quick to point out, just how serious of a commitment it is, and how catastrophic it is to get it wrong. The consensus is that the default Rust repr, &lt;code&gt;#[repr(Rust)]&lt;/code&gt;, will not get stabilized any time soon, to not push Rust into a corner 10 years in the future.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;interoperable_abi&lt;/code&gt; instead proposes a new repr, &lt;code&gt;#[repr(interop)]&lt;/code&gt;, that bases itself on the already stable C ABI, and extends it with features that make sense for Rust today. Some potential examples:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Well defined fat pointers - slices, strings, trait objects.&lt;/li&gt;
&lt;li&gt;Optimizing enums to use less space.&lt;/li&gt;
&lt;li&gt;Support zero-sized fields in structs, without increasing the struct size.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The idea is great in principle, but all of this can be worked around and is not really the main problem blocking Rust from becoming a language that interops with other languages.&lt;/p&gt;
&lt;h2&gt;The Blind Spot&lt;/h2&gt;
&lt;p&gt;Manishearth, creator of &lt;a href="https://github.com/rust-diplomat/diplomat/"&gt;&lt;code&gt;diplomat&lt;/code&gt;&lt;/a&gt;, commented the following on the issue:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;One thing that bothers me a bit is that I'm not actually sure if extern &amp;quot;interop&amp;quot; has
taken a bite at the right chunk of the problem here, when looking from the POV of interop.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;You have a tool like Diplomat or CXX that translates idiomatic Rust APIs to idiomatic FooLang
APIs. It's going to find extern &amp;quot;interop&amp;quot; useful, but it's still going to have to generate
some Rust wrappers around it. Not a huge change from the status quo. However, a lot of the
things involved in making extern &amp;quot;interop&amp;quot; work will be very important.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;In the cases most addressing the motivation (Diplomat, uniffi), work still needs to be done
to translate to extern &amp;quot;interop&amp;quot;, so I'm not sure if it actually buys much in those cases.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is the comment that got me thinking. I've written a crate that makes traits shareable across dynamic libraries. Was &lt;code&gt;#[repr(Rust)]&lt;/code&gt; the biggest problem in my efforts? No, it wasn't perfect, but I could walk around it by writing C structures for slices and alike. Are zero-sized fields key in the grand scheme of things? It wakes my OCD up, makes me very irritated, and I walked around it by writing my own custom bindings generator that ignores these fields, but in majority of cases, extra 1-8 bytes in a struct mean nothing. The main problem lies in bridging different languages together, and ensuring type safety across 2 libraries. That's the big fish.&lt;/p&gt;
&lt;h2&gt;The Root of All Solutions&lt;/h2&gt;
&lt;p&gt;Bridges between multiple languages and binaries, is a problem that has been in the works to be tackled by multiple parties. There are multiple approaches to this, but a compiler has the potential to do it better. So how would we solve the interoperability problem?&lt;/p&gt;
&lt;h3&gt;Prior art&lt;/h3&gt;
&lt;p&gt;Let's take a look at what different interoparability solutions do, and what their limitations are. Note that the list is not exhaustive and we will mostly focus on structures here.&lt;/p&gt;
&lt;h4&gt;cbindgen&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;cbindgen&lt;/code&gt; is an external tool that generates C/C++ headers by parsing your crate at source level. It's pretty amazing that it can do it, it can even parse dependency crates, but it suffers from a key limitation - it's buggy. Types often get misinterpreted, macros do not get automatically expanded, and so on. I don't have an example at hand, so the source is &amp;quot;trust me bro&amp;quot;, but it's the actual reality of the tool.&lt;/p&gt;
&lt;h4&gt;diplomat&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;diplomat&lt;/code&gt; is a framework for allowing foreign languages to call into Rust. I've not used it, so I don't want to misrepresent it, but it has &lt;a href="https://rust-diplomat.github.io/book/"&gt;very extensive documentation&lt;/a&gt;. What it seems to boil down to is parsing a portion of Rust code, collecting types within, and then emitting a C layer, as well as other language implementations. They do define both ast and hir in &lt;a href="https://docs.rs/diplomat_core/latest/diplomat_core/"&gt;&lt;code&gt;diplomat_core&lt;/code&gt;&lt;/a&gt;. It seems to be something like a more reliable &lt;code&gt;cbindgen&lt;/code&gt; with a limitation (or a feature, depending how you look) of not parsing your whole crate.&lt;/p&gt;
&lt;h4&gt;abi_stable&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;abi_stable&lt;/code&gt; is one of the oldest dynamic linking solutions - it provides runtime type checking when loading objects from multiple dynamic Rust libraries. It explicitly doesn't involve foreign languages, but I think it's still worth to mention, as it is the closest to what Rust people need today.&lt;/p&gt;
&lt;p&gt;The core of it is the &lt;code&gt;StableAbi&lt;/code&gt; trait. Let's take a look at it:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;pub unsafe trait StableAbi: GetStaticEquivalent_ {
    type IsNonZeroType: Boolean;

    const LAYOUT: &amp;amp;'static TypeLayout;
    const ABI_CONSTS: AbiConsts = _;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;const LAYOUT&lt;/code&gt; is the meat of the crate - it contains type information accessible at runtime/compile-time. Then, there's a &lt;a href="https://docs.rs/abi_stable/latest/abi_stable/abi_stability/fn.check_layout_compatibility.html"&gt;&lt;code&gt;check_layout_compatibility&lt;/code&gt;&lt;/a&gt; function that takes 2 type layouts, and returns whether they are compatible. Mind you, it checks for compatibility, not equality. This implies assymetric relationship. For instance, if between library versions &lt;code&gt;0.1.0&lt;/code&gt; and &lt;code&gt;0.1.1&lt;/code&gt;, struct &lt;code&gt;Foo&lt;/code&gt; had an extra field appended to the end of it, &lt;code&gt;&amp;amp;Foo&lt;/code&gt; from library &lt;code&gt;0.1.1&lt;/code&gt; should be compatible for use by the old &lt;code&gt;0.1.0&lt;/code&gt; version, because it simply doesn't care about the field. However, it would be invalid in the opposite case, because the real struct does not allocate or set the field expected by the new library, and that's what the function checks for.&lt;/p&gt;
&lt;h4&gt;safer-ffi&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;safer-ffi&lt;/code&gt; is a framework for building FFI in a safe manner. It's a really elegant design - unlike most FFI solutions, it doesn't attempt to perfectly parse and keep track of types. Instead, it works at type information level and derives special &lt;code&gt;ReprC&lt;/code&gt; and &lt;code&gt;CType&lt;/code&gt; traits that specify how to expand that type, using the information of the member types.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;pub unsafe trait ReprC: Sized {
    type CLayout: CType;

    fn is_valid(it: &amp;amp;Self::CLayout) -&amp;gt; bool;
}

pub unsafe trait CType: Sized + Copy {
    type OPAQUE_KIND: __;

    fn c_short_name_fmt(fmt: &amp;amp;mut Formatter&amp;lt;'_&amp;gt;) -&amp;gt; Result;
    fn c_var_fmt(fmt: &amp;amp;mut Formatter&amp;lt;'_&amp;gt;, var_name: &amp;amp;str) -&amp;gt; Result;

    fn c_short_name() -&amp;gt; ImplDisplay&amp;lt;Self&amp;gt; { ... }
    fn c_define_self(definer: &amp;amp;mut dyn Definer) -&amp;gt; Result&amp;lt;()&amp;gt; { ... }
    fn c_var(var_name: &amp;amp;str) -&amp;gt; ImplDisplay&amp;lt;'_, Self&amp;gt; { ... }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It doesn't go as far as storing actual type information as a constant, but it uses Rust's type system to provide a well structured way to generate interoperable headers.&lt;/p&gt;
&lt;h3&gt;The Common Thread&lt;/h3&gt;
&lt;p&gt;All of the examples share one key feature - they leverage type information one way or the other. I think &lt;code&gt;abi_stable&lt;/code&gt; and &lt;code&gt;safer-ffi&lt;/code&gt; do it better, because using the type system itself makes a lot of sense - you are directly accessing the source of truth. Yet, they both suffer from one key limitation - implementing general information on types is a manual task requiring annotations for all user defined types, and manual implementations for the commonly used stdlib types. &lt;code&gt;cbindgen&lt;/code&gt; attempts to remove the manual labor part, but suffers greatly from not being able to separate types clearly. Meanwhile, rustc clearly has all the necessary information at hand. If we were to expose it in some structured way, everyone could benefit.&lt;/p&gt;
&lt;h3&gt;Type Information is Enough&lt;/h3&gt;
&lt;p&gt;I built an example crate - &lt;a href="https://crates.io/crates/ctti"&gt;&lt;code&gt;ctti&lt;/code&gt;&lt;/a&gt; that provides just that - type information at compile-time. It's not complete - all primitives expose the information, user defined structures can implement it using provided macro, but unions, enums, and stdlib are out of luck. It's not ready for daily use, and it's not the point of it. The point of the crate is to show that &lt;em&gt;type information is enough&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Mara Bos put the following image up on the GitHub issue:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blaz.is/media/rust-abi.jpg" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;It's a potential plan of the Rust ABI project. Notice how features are built incrementally. Stage B suggests &lt;code&gt;extern dyn crate&lt;/code&gt;, where a crate is loaded dynamically. Stage C aims to provide stable TypeIds. Stage E aims to store type layout in the binaries. I think it's a reasonable approach to not have scope creep, but how are B and C parts supposed to be implemented?&lt;/p&gt;
&lt;h4&gt;extern dyn crate&lt;/h4&gt;
&lt;p&gt;To load a crate dynamically, you'd need a mechanism in place to verify compatibility of the libraries. This is exactly what &lt;code&gt;abi_stable&lt;/code&gt; does by utilising type layouts. Of course, &lt;code&gt;extern dyn crate&lt;/code&gt; is a more elegant solution, but that seems like something that could be built easier after type layouts exist.&lt;/p&gt;
&lt;h4&gt;Stable TypeIds&lt;/h4&gt;
&lt;p&gt;There's an example &lt;a href="https://docs.rs/ctti/latest/ctti/type_id/index.html"&gt;&lt;code&gt;type_id&lt;/code&gt;&lt;/a&gt; module in my crate that implements consistent type ID through a const fn. With type layouts, you don't need a special compiler-supported implementation of stable TypeIds - it can be outsourced to an external crate.&lt;/p&gt;
&lt;h3&gt;Change the Angle&lt;/h3&gt;
&lt;p&gt;My suggestion here is to change the angle interop is attacked from. If the endpoint is to &lt;em&gt;store type information and documentation in binaries&lt;/em&gt;, as uncertain as things are, I think type information is the first thing needed to be built before other features are built, because it's the only foundation needed for the rest. Then Rust can do what it's always done - outsource experimentation of higher level features to external crates, and if there's demand - incorporate the winner to the standard library.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Is &lt;code&gt;interoperable_abi&lt;/code&gt; a bad idea? Absolutely not - it would fix some C limitations imposed at a time when compile time computation based on types was unthinkable. It would be good, and it is needed at some point, but to truly make language interop possible, I think we need type information to build all the other necessary pieces of the interoperability ecosystem project.&lt;/p&gt;
</summary><category term="Design Choices"></category><category term="Rust"></category></entry><entry><title>No compromises I/O</title><link href="https://blaz.is/blog/post/no-compromises-io" rel="alternate"></link><published>2022-12-26T10:22:58+00:00</published><updated>2022-12-26T10:23:00+00:00</updated><author><name>auri</name></author><id>https://blaz.is/blog/post/no-compromises-io</id><summary type="html">&lt;p&gt;&amp;quot;Mr. h33p, what kind of substance are you on? All I want is 2 simple read/write functions!&amp;quot;, said everyone in the room. Well, if you just let me get into the weeds, I'll tell you why that's not enough, and why we can do much, much better.&lt;/p&gt;
&lt;h2&gt;Preface&lt;/h2&gt;
&lt;p&gt;Happy holidays, everyone! I/O is at the heart of memflow - it is the path from read/write invokation on a virtual process all the way to low level LeechCore calls. That path may cross several translation steps, it may even cross machines through networking, and every step adds overhead. The question is, how much of it is necessary? That's exactly what I want to solve - making that path as efficient as possible, without sacrificing API simplicity.&lt;/p&gt;
&lt;h2&gt;Layers of complexity&lt;/h2&gt;
&lt;p&gt;What is the simplest read function you can write? In what ways can we make it more complex? What wins do these layers of complexity provide?&lt;/p&gt;
&lt;h3&gt;Failure-free operation&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Basic single-byte read&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;fn read(address: usize) -&amp;gt; u8;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can't get any simpler than this. The smallest addressable piece of data gets read at the smallest granularity address possible. This is the core upon which everything else builds on.&lt;/p&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Buffer read (memcpy)&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;fn read(address: usize, buffer: &amp;amp;mut [u8]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is equivalent to &lt;code&gt;memcpy(dest, src, size)&lt;/code&gt;, except &lt;code&gt;address&lt;/code&gt; is src, and &lt;code&gt;dest&lt;/code&gt; with &lt;code&gt;size&lt;/code&gt; are encoded within &lt;code&gt;buffer&lt;/code&gt;. Say it's a more rusty version of memcpy.&lt;/p&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Multi-buffer read&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;struct Read&amp;lt;'a&amp;gt; {
    address: usize,
    buffer: &amp;amp;'a mut [u8]
}

fn read(ops: &amp;amp;mut [Read]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is merely putting all the read instances to one function call. Every function call incurs a latency penalty, and switching to kernel side incurs an especially heavy penalty. Having all reads happen in one go minimizes that overhead.&lt;/p&gt;
&lt;ol start="4"&gt;
&lt;li&gt;Streamed read&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;fn read(ops: impl Iterator&amp;lt;Item = Read&amp;gt;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It's the same here, except you don't know how many elements there are, you may or may not choose to split it up according to hard limits of underlying implementation. But the idea is, that there is an arbitrary number of reads.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Axiom 1:&lt;/strong&gt; each layer of complexity can be implemented using a different layer.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Axiom 2:&lt;/strong&gt; higher layer of complexity, implemented using lower layer, requires multiple simpler operations to perform.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Axiom 3:&lt;/strong&gt; multiple invokations of a lower layer can be abstracted by fewer invokations of a higher layer.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Claim:&lt;/strong&gt; each layer of complexity adds some constant computation overhead, but it is insignificant, compared to reduction in computational complexity achieved (note that layers 3 and 4 are somewhat equivalent, but I chose to split them apart to make the transition not seem as large. From now on, I will only display streamed reads).&lt;/p&gt;
&lt;h3&gt;Fallible operation&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Basic single-byte read&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;fn read(address: usize) -&amp;gt; Result&amp;lt;u8&amp;gt;;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The read will either succeed or fail with specific error. There is nothing to improve here.&lt;/p&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Buffer read&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;fn read(address: usize, buffer: &amp;amp;mut [u8]) -&amp;gt; Result&amp;lt;()&amp;gt;;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The read will either succeed fully or fail in undefined way. Buffer may need to be fragmented, we can improve by returning partial successes and failures.&lt;/p&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Streamed read&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;fn read(ops: impl Iterator&amp;lt;Item = Read&amp;gt;, out: impl Fn(Result&amp;lt;()&amp;gt;));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The main problem with this design is that there is no way for dealing with partial successes or data fragmentation&lt;/p&gt;
&lt;h3&gt;Fragmentable operation&lt;/h3&gt;
&lt;p&gt;Fragmentable operation implies data separation and reordering. We need to be able to distinguish what the output of which part of each read is, hence we attach some metadata.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Basic single-byte read&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;fn read(address: usize) -&amp;gt; Result&amp;lt;u8&amp;gt;;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;No change.&lt;/p&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Buffer read&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;struct Output&amp;lt;'a&amp;gt; {
    address: usize,
    buffer: &amp;amp;mut [u8],
    error: Option&amp;lt;Error&amp;gt;
}

fn read(address: usize, buffer: &amp;amp;mut [u8], out: impl Fn(Output));
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Streamed read&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;struct Read&amp;lt;'a&amp;gt; {
    elem_idx: usize,
    address: usize,
    buffer: &amp;amp;'a mut [u8]
}

struct Output&amp;lt;'a&amp;gt; {
    elem_idx: usize,
    address: usize,
    buffer: &amp;amp;mut [u8],
    error: Option&amp;lt;Error&amp;gt;
}

fn read(ops: impl Iterator&amp;lt;Item = Read&amp;gt;, out: impl Fn(Output));
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Forward-redirectable operation&lt;/h3&gt;
&lt;p&gt;During fragmentation destination address of each buffer may change. To output results efficiently, source address or offset must be attached as additional metadata.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Basic single-byte read&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;fn read(address: usize) -&amp;gt; Result&amp;lt;u8&amp;gt;;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;No change&lt;/p&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Buffer read&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;struct Output&amp;lt;'a&amp;gt; {
    src_address: usize,
    dst_address: usize,
    buffer: &amp;amp;mut [u8],
    error: Option&amp;lt;Error&amp;gt;
}

fn read(src_address: usize, dst_address: usize, buffer: &amp;amp;mut [u8], out: impl Fn(Output));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;src_address&lt;/code&gt; may be the starting address of the read (at the root &lt;code&gt;src_address = dst_address&lt;/code&gt;), or &lt;code&gt;0&lt;/code&gt;, which would then yield output of the offset within the buffer.&lt;/p&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Streamed read&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;struct Read&amp;lt;'a&amp;gt; {
    elem_idx: usize,
    src_address: usize,
    dst_address: usize,
    buffer: &amp;amp;'a mut [u8]
}

struct Output&amp;lt;'a&amp;gt; {
    elem_idx: usize,
    address: usize,
    buffer: &amp;amp;mut [u8],
    error: Option&amp;lt;Error&amp;gt;
}

fn read(ops: impl Iterator&amp;lt;Item = Read&amp;gt;, out: impl Fn(Output));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is one thing we did not consider here - buffer availability. Sometimes you may want to allocate on-demand without overallocating regions that will fail during the process. This is especially important for networked transfer.&lt;/p&gt;
&lt;h3&gt;Buffer handle&lt;/h3&gt;
&lt;p&gt;This design implies buffers that exist only in an abstract sense. Different designs are possible, though, with different levels abstractness. Which is optimal? Let's find out.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Basic single-byte read&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;fn read(address: usize) -&amp;gt; Result&amp;lt;u8&amp;gt;;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;No change, because abstract buffers are not applicable.&lt;/p&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Buffer read&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;struct Buffer&amp;lt;'a&amp;gt; {
    data: &amp;amp;'a mut (),
    vtbl: &amp;amp;'static BufferVtbl,
}

struct BufferVtbl {
    get_mut: for&amp;lt;'a&amp;gt; fn(&amp;amp;'a mut self) -&amp;gt; &amp;amp;'a mut [u8],
    len: fn(&amp;amp;self) -&amp;gt; usize,
    split_at: fn(self, pos: usize) -&amp;gt; (Buffer, Buffer),
}

impl&amp;lt;'a&amp;gt; From&amp;lt;&amp;amp;'a mut [u8]&amp;gt; for Buffer&amp;lt;'a&amp;gt; { ... }

struct ByteAddressBuffer&amp;lt;'a&amp;gt; {
    data: &amp;amp;'a mut [u8],
    address: usize,
}

impl&amp;lt;'a&amp;gt; From&amp;lt;ByteAddressBuffer&amp;lt;'a&amp;gt;&amp;gt; for Buffer&amp;lt;'a&amp;gt; { ... }

struct Output&amp;lt;'a&amp;gt; {
    buffer: Buffer,
    error: Option&amp;lt;Error&amp;gt;
}

fn read(address: usize, buffer: Buffer, out: impl Fn(Output));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can actually drop &lt;code&gt;src_address&lt;/code&gt;! This data can be attached to the buffer and forwarded through. Alternatively, it can be swapped for something different for different tracking purposes.&lt;/p&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Streamed read&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;struct ByteIdxAddressBuffer&amp;lt;'a&amp;gt; {
    data: &amp;amp;'a mut [u8],
    address: usize,
    elem: usize,
}

impl&amp;lt;'a&amp;gt; From&amp;lt;ByteIdxAddressBuffer&amp;lt;'a&amp;gt;&amp;gt; for Buffer&amp;lt;'a&amp;gt; { ... }

fn read(ops: impl Iterator&amp;lt;Item = Read&amp;gt;, out: impl Fn(Output));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Should indexing on output be needed - just have the index stored within the buffer. Otherwise, the interface stays the same as single buffer read!&lt;/p&gt;
&lt;p&gt;Abstract buffers actually have a surprising improvement in simplicity! You pay for what you need as end user!&lt;/p&gt;
&lt;p&gt;Now, there is a bit of a flaw here. You put the data in the buffer, but how do you actually use it when it's returned out? The answer is opaquing the callback.&lt;/p&gt;
&lt;h3&gt;Opaquing the output callback&lt;/h3&gt;
&lt;p&gt;To access the buffer within output, the output callback must be attached to the input data. I.e. either every buffer must be of same type, or there needs to be a individual callback for each buffer. The callback could be attached to the buffer's vtable. Let's see this in action!&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Basic single-byte read&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;fn read(address: usize) -&amp;gt; Result&amp;lt;u8&amp;gt;;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;No change, because abstract buffers are not applicable.&lt;/p&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Buffer read&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;struct BufferVtbl {
    get_mut: for&amp;lt;'a&amp;gt; fn(&amp;amp;'a mut self) -&amp;gt; &amp;amp;'a mut [u8],
    len: fn(&amp;amp;self) -&amp;gt; usize,
    split_at: fn(self, pos: usize) -&amp;gt; (Buffer, Buffer),
    output: fn(&amp;amp;mut self, error: Option&amp;lt;Error&amp;gt;),
}

fn read(address: usize, buffer: Buffer);
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Streamed read&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;fn read(ops: impl Iterator&amp;lt;Item = Read&amp;gt;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are some end conditions to consider. One being success - write to buffer, call output(None). Second is failure - call output(err). Third is nothing. No output was called. What do we do then? Implicit error? Panic?&lt;/p&gt;
&lt;p&gt;Success usage may be simplified with a wrapper, but failure may still need to be handled.&lt;/p&gt;
&lt;p&gt;Final thing to consider is attaching vtable to each buffer. What if we always use the same data structure? It would be more efficient to have separate vtable and then passing pointers to buffers themselves. In addition, each buffer object is merely a pointer to underlying data store, which requires a heap allocation, or some clever stack ussage. Heap would prove prohibitavely expensive, perhaps an arena? Who knows, but this detail is making things tricky. Let's think about it.&lt;/p&gt;
&lt;h4&gt;Solving the alloc problem&lt;/h4&gt;
&lt;p&gt;We can avoid creating a mutated buffer upon splitting, by storing bounds of the said buffer on the abstract side, like so:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;struct BufferObj&amp;lt;'a&amp;gt; {
    buffer: *mut (),
    start: usize,
    end: usize,
    phantom: PhantomData&amp;lt;&amp;amp;'a mut u8&amp;gt;,
}

struct Buffer&amp;lt;'a&amp;gt; {
    buffer: BufferObj&amp;lt;'a&amp;gt;,
    vtbl: &amp;amp;'a BufferVtbl,
}

struct BufferVtbl {
    get_mut: for&amp;lt;'a&amp;gt; fn(&amp;amp;'a mut BufferObj) -&amp;gt; &amp;amp;'a mut [u8],
    output: fn(BufferObj, error: Option&amp;lt;Error&amp;gt;),
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;len&lt;/code&gt; and &lt;code&gt;split_at&lt;/code&gt; functions become part of the &lt;code&gt;impl Buffer&lt;/code&gt;, and it becomes responsibility of each implementation to ensure memory safety guarantees when accessing the data (notice how buffer is now a pointer, not a mut ref).&lt;/p&gt;
&lt;p&gt;Now, the buffer pointers still need to be created somehow, don't they? That is an unfortunate limitation of the approach, unless we created some sort of (metadata) stacking system.&lt;/p&gt;
&lt;h3&gt;Zero-access buffer&lt;/h3&gt;
&lt;p&gt;What if you never had access to the bytes of the buffer in the first place? Not requiring the buffer to have an underlying byte representation would be incredibly powerful - reading directly to network streams, using GPU objects, DMA, etc. That is powerful. Nothing really changes here, apart from the buffer vtable:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;struct BufferVtbl {
    get_mut: for&amp;lt;'a&amp;gt; fn(&amp;amp;'a mut BufferObj) -&amp;gt; &amp;amp;'a mut [u8],
    write: fn(&amp;amp;mut BufferObj, data: &amp;amp;[u8]),
    output: fn(BufferObj, error: Option&amp;lt;Error&amp;gt;),
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The idea is simple - if you already have a byte slice - call &lt;code&gt;write&lt;/code&gt;. It will pass your slice down to be copied without intermediate buffers. If it's a file, it will call &lt;code&gt;file.write(data)&lt;/code&gt;, if it's a network stream, something equivalent, if it's another byte buffer, well, then it will fall back to copy to a &lt;code&gt;get_mut&lt;/code&gt; result. But it will be branchless and efficient.&lt;/p&gt;
&lt;p&gt;If you don't have a byte buffer, and APIs called by you expect one, then you should use &lt;code&gt;get_mut&lt;/code&gt;, and pass the buffer over to API. If the underlying buffer is a byte buffer, it will cost you nothing! If it's a stream, then there will be an allocation, which can be optimized with arenas and what not.&lt;/p&gt;
&lt;p&gt;Can we reduce this even further? What if we could pass from stream to stream? Okay, that might actually be a bit too close to the sun. Let's stick with the last part.&lt;/p&gt;
&lt;h3&gt;Improving buffer states&lt;/h3&gt;
&lt;p&gt;There is a slight problem, however, with this approach. It would become key to call &lt;code&gt;output&lt;/code&gt; function to correctly synchronize temporary buffers. This is error prone.&lt;/p&gt;
&lt;p&gt;Let's rework the vtable so that it functions more as a state machine (and let's use fully valid syntax for a change):&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;struct BufferVtbl {
    get_mut: for&amp;lt;'a&amp;gt; fn(BufferObj&amp;lt;'a&amp;gt;) -&amp;gt; AllocedBufferObj&amp;lt;'a&amp;gt;,
    write: for&amp;lt;'a&amp;gt; for&amp;lt;'b&amp;gt; fn(BufferObj&amp;lt;'a&amp;gt;, data: &amp;amp;'b [u8]),
    error: for&amp;lt;'a&amp;gt; fn(BufferObj&amp;lt;'a&amp;gt;, error: Error),
}

struct AllocedBufferObj&amp;lt;'a&amp;gt; {
    alloced_buffer: &amp;amp;'a mut [u8],
    buffer: BufferObj&amp;lt;'a&amp;gt;,
    write: for&amp;lt;'a&amp;gt; fn(BufferObj&amp;lt;'a&amp;gt;, data: &amp;amp;'a mut [u8]),
    error: for&amp;lt;'a&amp;gt; fn(BufferObj&amp;lt;'a&amp;gt;, data: &amp;amp;'a mut [u8], error: Error),
}

impl&amp;lt;'a&amp;gt; Drop for AlloceBufferObj&amp;lt;'a&amp;gt; {
    fn drop(&amp;amp;mut self) {
        (self.write)(self.buffer, self.alloced_buffer);
    }
}

impl&amp;lt;'a&amp;gt; Deref for AlloceBufferObj&amp;lt;'a&amp;gt; {
    type Target = [u8];
    // ...
}

impl&amp;lt;'a&amp;gt; DerefMut for AlloceBufferObj&amp;lt;'a&amp;gt; {
    // ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now user may not invoke the same state twice. That is good, because then there is no need to check whether buffer was allocated or not before freeing, because that state is perfectly known ahead of time. In fact, the buffer within &lt;code&gt;AllocatedBufferObj&lt;/code&gt; may be a different type of buffer!&lt;/p&gt;
&lt;h2&gt;Async&lt;/h2&gt;
&lt;p&gt;I/O takes time, right? Can we somehow make this whole design asynchronous?&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;fn read(address: usize, buffer: Buffer) -&amp;gt; impl Stream&amp;lt;Item = Output&amp;gt;;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That's right, we don't need to deal with iterators of buffers anymore. Think about it, you can enqueue multiple reads before flushing them by calling &lt;code&gt;.await&lt;/code&gt;. In theory it works like magic and makes me look stupid for dealing with those iterators and callbacks all this time!&lt;/p&gt;
&lt;p&gt;To give a better idea, take the following psueodode:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;let fut1 = reader.read_val::&amp;lt;u64&amp;gt;(0xdeadbeef);
let fut2 = reader.read_val::&amp;lt;u32&amp;gt;(0xdeadbefe);
let mut buf = [0u8; 256];
let fut3 = reader.read(0xdeedfeeb, Buffer::new(&amp;amp;mut buf));

let (val64, val32, val_buf) = join!(fut1, fut2, fut3).await;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And here's a hypothetical reader impl:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;let mut ops = vec![];

loop {
    while let Some(op) = rx.next().await {
        // flush is sent whenever there is an await happening
        if op.is_flush() {
            break;
        }
        ops.push(op);
        if ops.len() &amp;gt;= 256 {
            break;
        }
    }
    perform_all_reads(&amp;amp;mut ops);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Automatic batching, this is literal magic!&lt;/p&gt;
&lt;p&gt;However, there still is a million dollar question that needs solving - how do you actually relate input Buffer with the output Stream?&lt;/p&gt;
&lt;p&gt;Note that you start with one buffer for the output stream (i.e. one buffer to output), but after fragmentation you end up with multiple buffers pointing to one stream. Completed buffers need to be stored &lt;em&gt;somewhere&lt;/em&gt;. We could allocate a reference counted list on the stream, but we need to remember that heap allocations are expensive, thus we need to reuse these lists as much as possible. One object that never moves throughout execution of the future is the reader - we can allocate the lists there, and while we're at it, let's allocate the streams themselves on the reader!&lt;/p&gt;
&lt;h3&gt;Initial design&lt;/h3&gt;
&lt;p&gt;Before we think too much about micro-optimizing data reuse, let's first write a very naive design:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;use flume::{Receiver, RecvStream, Sender, unbounded};

// For now this is just a Receiver::into_stream
type BufferStream = RecvStream&amp;lt;'static, Output&amp;gt;;

struct Reader {
    tx: Sender&amp;lt;(usize, Buffer)&amp;gt;,
    rx: Receiver&amp;lt;(usize, Buffer)&amp;gt;,
}

impl Reader {
    fn new() -&amp;gt; Self {
        let (tx, rx) = unbounded();
        Self { tx, rx }
    }

    async fn run(&amp;amp;self) {
        let mut stream = self.rx.stream();
        while let Some((addr, op)) = stream.next().await {
            // read impl
            op.error(Error::NotImplemented);
        }
    }

    fn read(&amp;amp;self, address: usize, buffer: Buffer) -&amp;gt; BufferStream {
        let (tx, rx) = unbounded();

        buffer.set_output(tx);
        self.tx.send(buffer).unwrap();

        rx.into_stream()
    }
}

struct BufferObj&amp;lt;'a&amp;gt; {
    buffer: *mut (),
    start: usize,
    end: usize,
    // new field:
    sender: Sender&amp;lt;Output&amp;gt;,
    phantom: PhantomData&amp;lt;&amp;amp;'a mut u8&amp;gt;,
}

struct Buffer&amp;lt;'a&amp;gt; {
    buffer: BufferObj&amp;lt;'a&amp;gt;,
    vtbl: &amp;amp;'a BufferVtbl,
}

impl&amp;lt;'a&amp;gt; Buffer&amp;lt;'a&amp;gt; {
    fn set_output(&amp;amp;mut self, sender: Sender&amp;lt;Output&amp;gt;) {
        self.buffer.sender = Some(sender);
    }
}

struct BufferVtbl {
    get_mut: for&amp;lt;'a&amp;gt; fn(BufferObj&amp;lt;'a&amp;gt;) -&amp;gt; AllocedBufferObj&amp;lt;'a&amp;gt;,
}

struct AllocedBufferObj&amp;lt;'a&amp;gt; {
    alloced_buffer: &amp;amp;'a mut [u8],
    buffer: BufferObj&amp;lt;'a&amp;gt;,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Buffer &lt;code&gt;write&lt;/code&gt; and &lt;code&gt;error&lt;/code&gt; are now implemented as part of &lt;code&gt;Buffer&lt;/code&gt; and &lt;code&gt;AllocatedBufferObj&lt;/code&gt; impls.&lt;/p&gt;
&lt;p&gt;It's pretty hard to fit everything in a very small space, but this is an end-to-end implementation. One thing we can improve is the fact that buffers need a stream attached only at the start of the chain:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;impl Reader {
    fn alloc_stream(&amp;amp;self, buffer: &amp;amp;mut Buffer) -&amp;gt; BufferStream {
        let (tx, rx) = unbounded();
        buffer.set_output(tx);
        rx.into_stream();
    }

    fn send_read(&amp;amp;self, address: usize, buffer: Buffer) {
        self.tx.send((address, buffer)).unwrap();
    }

    fn read(&amp;amp;self, address: usize, mut buffer: Buffer) -&amp;gt; BufferStream {
        let ret = self.alloc_stream(&amp;amp;mut buffer);
        self.send_read(address, buffer);
        ret
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, &lt;code&gt;read&lt;/code&gt; becomes the frontend method, while &lt;code&gt;alloc_stream&lt;/code&gt; with &lt;code&gt;send_stream&lt;/code&gt; become implementation details. To make this sufficiently performant we need to allocate a &lt;code&gt;BufferStream&lt;/code&gt; object on the &lt;code&gt;Reader&lt;/code&gt; and recycle it after use, hence why we have 2 functions for allocating a stream and sending the read.&lt;/p&gt;
&lt;p&gt;We could pass the stream as additional parameter during send, but I think it's very important to make it hard to missend the buffer down the wrong path at the API level.&lt;/p&gt;
&lt;h3&gt;Inserting the flush&lt;/h3&gt;
&lt;p&gt;Currently there is a risk of deadlock upon &lt;code&gt;.await&lt;/code&gt; operation, because the &lt;code&gt;Reader&lt;/code&gt; may choose to batch up an arbitrary number of reads into one large operation for performance reasons, and there is no way to explicitly tell when to stop. The final piece needed to make all of this work is implicit insertion of flushes whenever the user is awaiting for the result. This requires us to create a custom &lt;code&gt;BufferStream&lt;/code&gt; implementation, I will not write the whole thing down, but the gist of it is as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;#[pin_project::pin_project]
struct BufferStream {
    #[pin]
    inner: RecvStream&amp;lt;'static, Output&amp;gt;,
    flush: Arc&amp;lt;AtomicBool&amp;gt;,
}

impl Stream for BufferStream {
    type Item = Output;

    fn poll_next(self: Pin&amp;lt;&amp;amp;mut Self&amp;gt;, cx: &amp;amp;mut Context&amp;lt;'_&amp;gt;) -&amp;gt; Poll&amp;lt;Self::Item&amp;gt; {
        let this = self.project();
        match this.inner.poll_next(cx) {
            Poll::Pending =&amp;gt; {
                this.flush.store(true, Ordering::SeqCst);
                Poll::Pending
            }
            x =&amp;gt; x
        }
    }
}


struct Reader {
    tx: Sender&amp;lt;(usize, Buffer)&amp;gt;,
    rx: Receiver&amp;lt;(usize, Buffer)&amp;gt;,
    flush: Arc&amp;lt;AtomicBool&amp;gt;,
}

impl Reader {
    fn new() -&amp;gt; Self {
        let (tx, rx) = unbounded();
        Self { tx, rx, flush: Default::default() }
    }

    async fn run(&amp;amp;self) {
        let mut stream = self.rx.stream();
        loop {
            // wait for the next element or until self.flush becomes true
            let next = stream_next_or_wait_true(&amp;amp;mut stream, &amp;amp;self.flush);
            // if flush happened, try_read all elements without blocking and perform read
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That's close to it! The only thing left is allocation optimizations, but that doesn't change the core of it - reads end up processed in one long loop, as opposed to multiple individual function calls.&lt;/p&gt;
&lt;h3&gt;Achilles' heel&lt;/h3&gt;
&lt;p&gt;Some of you may have raised some eyebrows seeing multiple parallel thinking &amp;quot;surely this can't be efficient&amp;quot;. I, after some talking with ko1N, share the concern. So the question is, is it good enough?&lt;/p&gt;
&lt;p&gt;I rigged up a performance benchmark testing various scenarios, and I counted the number of calls performed per second. Here are the results:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Bare function                      | 59269249
Deque sequential                   | 57558620
Concurrent queue sequential        | 38223797
Single channel sequential          | 32853343
Single channel sequential async    | 24920480
Double channel sequential          | 22133939
Double channel sequential async    | 15569659
Bare function with dummy syscall   | 5354162
2 tasks 2 chan on threaded runtime | 2175128
2 async tasks and 2 channels       | 1046951
2 threads and 2 channels           | 377872
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bare function represents the theoretical maximum that can be achieved in direct memory copy scenarios, while dummy syscall is used as an anchor point for scenarios where kernel context switch occurs. As you can see, a single context switch lowers the performance by the factor of 10 (on arm64 macOS machine), so while the difference for everything in-between is large, it's worth to note that the methods are still &lt;em&gt;very fast&lt;/em&gt;. That being said, it's evident that any parallel processing done by the methods after the syscall is simply too slow, potentially due to the complexity involved in synchronizing the 2 sides. This poses a critical problem.&lt;/p&gt;
&lt;p&gt;First of all, is my hypothesis correct? I used &lt;code&gt;tokio-metrics&lt;/code&gt; to monitor execution of my async functions. It's as simple as surrounding my tasks with &lt;code&gt;TaskMonitor::instrument&lt;/code&gt; and then print the results afterwards.&lt;/p&gt;
&lt;p&gt;Here are the results:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Single channel sequential async:
24630635/s 0.00003ms
TaskMetrics {
    instrumented_count: 1,
    dropped_count: 1,
    first_poll_count: 1,
    total_first_poll_delay: 2.125¬µs,
    total_idled_count: 0,
    total_idle_duration: 0ns,
    total_scheduled_count: 0,
    total_scheduled_duration: 0ns,
    total_poll_count: 1,
    total_poll_duration: 1.000395041s,
    total_fast_poll_count: 0,
    total_fast_poll_duration: 0ns,
    total_slow_poll_count: 1,
    total_slow_poll_duration: 1.000395041s,
}
Double channel sequential:
22362663/s 0.00004ms
Double channel sequential async:
14271546/s 0.00006ms
TaskMetrics {
    instrumented_count: 1,
    dropped_count: 1,
    first_poll_count: 1,
    total_first_poll_delay: 1.833¬µs,
    total_idled_count: 0,
    total_idle_duration: 0ns,
    total_scheduled_count: 0,
    total_scheduled_duration: 0ns,
    total_poll_count: 1,
    total_poll_duration: 1.000018625s,
    total_fast_poll_count: 0,
    total_fast_poll_duration: 0ns,
    total_slow_poll_count: 1,
    total_slow_poll_duration: 1.000018625s,
}
2 jobs and channels (MT runtime):
2175128/s 0.000ms
TaskMetrics {
    instrumented_count: 2,
    dropped_count: 2,
    first_poll_count: 2,
    total_first_poll_delay: 13.416¬µs,
    total_idled_count: 4350256,
    total_idle_duration: 572.511412ms,
    total_scheduled_count: 4350257,
    total_scheduled_duration: 740.525747ms,
    total_poll_count: 4350259,
    total_poll_duration: 638.974828ms,
    total_fast_poll_count: 4350248,
    total_fast_poll_duration: 637.946415ms,
    total_slow_poll_count: 11,
    total_slow_poll_duration: 1.028413ms,
}
2 jobs and channels:
1187762/s 0.001ms
TaskMetrics {
    instrumented_count: 2,
    dropped_count: 2,
    first_poll_count: 2,
    total_first_poll_delay: 6.625¬µs,
    total_idled_count: 2375524,
    total_idle_duration: 807.33773ms,
    total_scheduled_count: 2375525,
    total_scheduled_duration: 852.521808ms,
    total_poll_count: 2375527,
    total_poll_duration: 311.660277ms,
    total_fast_poll_count: 2375525,
    total_fast_poll_duration: 311.499944ms,
    total_slow_poll_count: 2,
    total_slow_poll_duration: 160.333¬µs,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice how sequential tasks do not idle, while the 2 jobs on multi-threaded and single-threaded runtimes spent 57% and 81% of their time idling respectively. So the hypothesis is correct - a lot of time is wasted in synchronization. Simply eliminating those idles would make good enough for dummy syscall situations. In ideal scenario, the &lt;code&gt;total_poll_duration&lt;/code&gt; should approach 1 second. And if we take the single threaded case of &lt;code&gt;1187762&lt;/code&gt; calls per second, scale it up so that polling duration is 1 second, &lt;code&gt;1187762 / (1 - 0.3112) = 1724393&lt;/code&gt;, which makes it roughly in line with &lt;em&gt;Double channel sequential async&lt;/em&gt; - the maths check out.&lt;/p&gt;
&lt;p&gt;With careful design, I think there is a way to walk around this problem. The polling implementation of the client could directly poll on the server, completely bypassing the scheduler. Think about it - server waits for the channel to receive elements. Once we create a read stream and attach our buffer to it, we know with certainty that the server's channel is going to have elements available, we just need to poll on it.&lt;/p&gt;
&lt;p&gt;So, can we pull this off?&lt;/p&gt;
&lt;p&gt;We need to create a detached task that can be polled from multiple places. Something like &lt;code&gt;Mutex&amp;lt;Pin&amp;lt;Box&amp;lt;dyn Future&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt;. The poll implementation of every read stream then does &lt;code&gt;try_lock&lt;/code&gt; on the future and upon success, polls the internal future until &lt;code&gt;Poll::Pending&lt;/code&gt; state is reached. This, in theory, should eliminate the scheduling overhead, although there are still some details to work out when it comes to lock ownership (does the last future hold on to the lock or does it release it as soon as possible?). Let's just see how it fares. Here are the key snippets of code:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;#[pin_project::pin_project]
struct SharedFuture&amp;lt;F&amp;gt; {
    inner: Pin&amp;lt;Arc&amp;lt;SharedFutureContext&amp;lt;F&amp;gt;&amp;gt;&amp;gt;,
}

struct SharedFutureContext&amp;lt;F&amp;gt; {
    future: Mutex&amp;lt;Option&amp;lt;F&amp;gt;&amp;gt;,
    finished: AtomicBool,
}

#[derive(Debug, Clone, Copy)]
enum SharedFutureOutput&amp;lt;T&amp;gt; {
    AlreadyFinished,
    JustFinished(T),
    Running,
}

impl&amp;lt;F: Future&amp;gt; SharedFuture&amp;lt;F&amp;gt; {
    async fn run_till_finished(self) {
        loop {
            match self.clone().await {
                SharedFutureOutput::AlreadyFinished | SharedFutureOutput::JustFinished(_) =&amp;gt; break,
                _ =&amp;gt; {
                    tokio::task::yield_now().await
                }
            }
        }
    }

    async fn run_once(&amp;amp;self) {
        self.clone().await;
    }
}

impl&amp;lt;F: Future&amp;gt; Future for SharedFuture&amp;lt;F&amp;gt; {
    type Output = SharedFutureOutput&amp;lt;F::Output&amp;gt;;

    fn poll(self: Pin&amp;lt;&amp;amp;mut Self&amp;gt;, cx: &amp;amp;mut Context&amp;lt;'_&amp;gt;) -&amp;gt; Poll&amp;lt;Self::Output&amp;gt; {
        let this = self.project();
        let inner: Pin&amp;lt;_&amp;gt; = this.inner.as_ref();
        let inner = inner.get_ref();

        if let Ok(mut future_cont) = inner.future.try_lock() {
            if let Some(future) = future_cont.as_mut() {
                let pin = unsafe { Pin::new_unchecked(&amp;amp;mut *future) };
                match pin.poll(cx) {
                    Poll::Pending =&amp;gt; Poll::Ready(SharedFutureOutput::Running),
                    Poll::Ready(val) =&amp;gt; {
                        *future_cont = None;
                        inner.finished.store(true, Ordering::Relaxed);
                        Poll::Ready(SharedFutureOutput::JustFinished(val))
                    }
                }
            } else {
                Poll::Ready(SharedFutureOutput::AlreadyFinished)
            }
        } else if inner.finished.load(Ordering::Acquire) {
            Poll::Ready(SharedFutureOutput::AlreadyFinished)
        } else {
            Poll::Ready(SharedFutureOutput::Running)
        }
    }
}

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And here are different versions of the 2 job benchmark:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;// Original impl
async fn test_latency_async(monitor: TaskMonitor) {
    let (tx1, rx1) = flume::unbounded();
    let (tx2, rx2) = flume::unbounded();

    let handle = tokio::spawn(monitor.instrument(tokio::task::unconstrained(async move {
        while let Ok(item) = rx1.recv_async().await {
            tx2.send_async(item).await.ok();
        }
    })));

    let instant = Instant::now();

    let mut elapsed = 0f64;
    let mut calls = 0;

    let mut tick_cnt = 0;

    while instant.elapsed().as_f64() &amp;lt;= 1.0 {
        let s = tick_count();
        let start = Instant::now();
        tx1.send_async(start).await.unwrap();
        let end = rx2.recv_async().await.unwrap();
        calls += 1;
        elapsed += end.elapsed().as_f64();
        tick_cnt += tick_count() - s;
    }

    tick_cnt /= calls;

    println!(
        &amp;quot;{calls}/s {:.3}ms | {tick_cnt}&amp;quot;,
        elapsed / calls as f64 * 1000.0
    );

    std::mem::drop(tx1);
    std::mem::drop(rx2);

    handle.await.ok();
}

// Using SharedFuture
async fn test_latency_async_sfut(monitor: TaskMonitor) {
    let (tx1, rx1) = flume::unbounded();
    let (tx2, rx2) = flume::unbounded();

    let shared_fut = SharedFuture::from(async move {
        while let Ok(item) = rx1.recv_async().await {
            tx2.send_async(item).await.ok();
        }
    });

    let handle = tokio::spawn(monitor.instrument(tokio::task::unconstrained(
        shared_fut.clone().run_till_finished(),
    )));

    let instant = Instant::now();

    let mut elapsed = 0f64;
    let mut calls = 0;

    let mut tick_cnt = 0;

    while instant.elapsed().as_f64() &amp;lt;= 1.0 {
        let s = tick_count();
        let start = Instant::now();
        tx1.send_async(start).await.unwrap();
        shared_fut.run_once().await;
        let end = rx2.recv_async().await.unwrap();
        calls += 1;
        elapsed += end.elapsed().as_f64();
        tick_cnt += tick_count() - s;
    }

    tick_cnt /= calls;

    println!(
        &amp;quot;{calls}/s {:.3}ms | {tick_cnt}&amp;quot;,
        elapsed / calls as f64 * 1000.0
    );

    std::mem::drop(tx1);
    std::mem::drop(rx2);

    handle.await.ok();
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And here are the results:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Bare function                      | 58946058
Deque sequential                   | 57418653
Concurrent queue sequential        | 38125560
Single channel sequential          | 32886669
Single channel sequential async    | 24983136
Double channel sequential          | 22334788
Double channel sequential async    | 14839470
2 tasks 2 chan with SharedFuture   | 6062313
Bare function with dummy syscall   | 5475882
2 tasks 2 chan on threaded runtime | 2215541
2 async tasks and 2 channels       | 1214937
2 threads and 2 channels           | 406485
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That's a significant improvement already. However, there still is significant overhead of running shared future. To save the trouble, the overhead stems from redundant poll that occurs when the loop is polled. Simply adding an explicit point that returns &lt;code&gt;Poll::Pending&lt;/code&gt; once per loop iteration fixes the issue. I.e. changing the loop as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;// Before:
let shared_fut = SharedFuture::from(async move {
    while let Ok(item) = rx1.recv_async().await {
        tx2.send_async(item).await.ok();
    }
});
// After:
let shared_fut = SharedFuture::from(async move {
    let mut cnt = 0;
    while let Ok(item) = rx1.recv_async().await {
        tx2.send_async(item).await.ok();
        core::future::poll_fn(|_| {
            cnt += 1;
            if (cnt % 2) == 0 {
                Poll::Ready(())
            } else {
                Poll::Pending
            }
        }).await;
    }
});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Performance:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Before:            6062313
After:             13418794
Sequential target: 14839470
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we removed all the locking, we'd achieve performance equal to our target (Double channel sequential async). However, it is worth to note that this optimization is too specific to the situation at hand and in real-world it could very easily lead to rescheduling, something we want to avoid. A better path forward would be a more efficient data structure, which is the approach I'm going to take.&lt;/p&gt;
&lt;p&gt;In conclusion, &lt;code&gt;SharedFuture&lt;/code&gt; shows to be the perfect solution for job synchronization overhead and production implementation will use a variation of this method.&lt;/p&gt;
&lt;h2&gt;Final API&lt;/h2&gt;
&lt;p&gt;Due to this API change, there now isn't a strong need to require mutable reference to perform I/O - reads can be issued concurrently without much trouble. That is an incredible win for memflow. With this API shift, memflow can become an incredibly simple-to-use system with amazing scalability.&lt;/p&gt;
&lt;p&gt;API itself becomes much simpler - batching becomes automatic, and the smallest number of reads gets issued. Take a look at this example:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;let p1 = reader.read_val::&amp;lt;u64&amp;gt;(0xdeadfeeb);

let p2 = async {
    let p = reader.read_val::&amp;lt;usize&amp;gt;(0xdeadbeef).await;
    let p = reader.read_val::&amp;lt;usize&amp;gt;(p).await;
    reader.read_val::&amp;lt;u64&amp;gt;(p + 0x10).await
};

let p3 = async {
    let mut p = 0xfeedbeef;
    for _ in 0..5 {
        p = reader.read_val::&amp;lt;usize&amp;gt;(p).await;
    }
    p
}

let (p1, p2, p3) = join!(p1, p2, p3).await;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many batches of operations are necessary to perform these reads? 5 is the longest chain, and so is the number of batches needed! Forget the days of thinking about collecting multiple values into a vector of operations - just let the async executor do the job for you!&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This came a long way from a simple single-byte read. We iterated through multiple versions of sync API making it as efficient as it can be and then flipped the design on its head with a simpler async API. One could argue it is less efficient due to allocation of stream objects, but through smart object reuse there should not be significant penalty in production. Overall this leads to a monumental shift in memflow I/O making it scale much better.&lt;/p&gt;
</summary><category term="memflow"></category><category term="Design Choices"></category><category term="Rust"></category></entry><entry><title>"Permanent Record" Review</title><link href="https://blaz.is/blog/post/permanent-record-review" rel="alternate"></link><published>2022-07-17T17:01:23+00:00</published><updated>2022-07-17T17:01:23+00:00</updated><author><name>auri</name></author><id>https://blaz.is/blog/post/permanent-record-review</id><summary type="html">&lt;p&gt;Edward Snowden's autobiography is a story about young man's journey leading up to arguably one of the most important events of the second decade, a story about rejection of evil, a story about rejection of being complicit, a story about standing up, even though all the cards are stacked up against one.&lt;/p&gt;
&lt;p&gt;Welcome to the book club! This is the first installment of the series, far too long coming, but it's finally here! We are starting with &amp;quot;Permanent Record&amp;quot;, because Ed's story is one of the more influential ones to my values, one of the most polarizing and misrepresented ones. He worked for both CIA and NSA and climbed up the ranks. While doing so he realized the scale of unconstitutional and morally wrong actions of these organizations. Not being okay with it he collected and released documents proving these activities. Seeking shelter from the response of the United States government he tried to reach Ecuador for political asylum, however, along the way his passport was repealed and thus he was exiled in Russia. Articles have been coming left and right describing Ed as a traitor, as someone who betrayed his country, and while yes, he did betray the system of the United States of America, but he did not betray the underlying values the country was built upon. I read the first half of the book several months back, and read the second half three days ago, immediately following with another book, so I may be misremembering some things.&lt;/p&gt;
&lt;h2&gt;Beginnings&lt;/h2&gt;
&lt;p&gt;The story starts at his roots, the first realizations of people leaving marks in society, the ways Ed got fascinated with technology, free information sharing, the way he stood up for oppression and rebelled against authority. Ed had 2 different lives - a gifted kid with loads of potential, who put the bare minimum needed in studies, and a fascinated internaut, who would hang around message boards with anonymous people, who he later found out were decades older than him, a never ending source of information feeding his curiosity. It was an incredible time where targeted bullying was not as prevalent, because there was always a clear way out - just come back with a different identity. People would play games of identifying the same person using different usernames, and Ed was incredibly good at hiding himself. Ed's parents were upstanding citizens working for the government, living in a city where working directly for the government was normal, he himself would be free to hang out on a porch of NSA headquarters, Ed describes the time before 9/11 as a time where freedom prevails. Then, a tragedy would struck the country and force it to close down, to find enemies, and ultimately become less free. After 9/11 no regular person could sit on the NSA's porch, because the entire area would be behind tightly controlled fence. Snowden described this time as the beginning of the fall of democracy.&lt;/p&gt;
&lt;h2&gt;Strive for Justice&lt;/h2&gt;
&lt;p&gt;After the tragedy, Ed wanted to help the country, he himself wanted to fight the bad guys. He enlisted in the army, which he would ultimately have to leave due to his health conditions. He ultimately wanted to become a government worker and work in the intelligence community (IC), which serving in the army was supposed to help with. Along his way Ed would notice subtle ways the system was rigged, for instance, one could be honorably discharged from the army if they chose to forfeit any claims to army's liability for any health conditions they were responsible for. Eventually, Ed would end up becoming a CIA contractor, but he wasn't interested in money. He wanted to travel, go to dangerous places where he would have impact, where he would help fighting for freedom. There were many freaky things he would learn during his training, one thing which struck me was Van Eck phreaking. It's a technique of extracting image contents from a CRT display by purely measuring electromagnetic emissions. That is how technical CIA training was, and not going to lie, that's really cool. Through a really funny sequence of events, Ed would end up in Geneva - the only TISO trainee who wanted to go to Afghani desert and ended up in one, highly in demand, place he did not want to go. In there he had a privilege to work alongside NSA folks, where he was hinted at absolutely insane technological capabilities of the competing intelligence organization. He would eventually become a government worker for the NSA and move to Japan, where he'd end up coming up with some uncomfortable realizations.&lt;/p&gt;
&lt;h2&gt;The One Thing Keeping You Awake at Night&lt;/h2&gt;
&lt;p&gt;In Japan, Ed worked as a systems analyst, helping to link together NSAs infrastructure with CIAs. The primary difference between NSA and CIA was that NSA was far more technologically advanced, while CIA was far more careful with security. NSA did not compartmentalize access to information, nor encrypted it. There was no universal backup strategy either. Ed was instrumental in improving this situation, through his EPICSHELTER backup system. The fact that Ed worked on such important systems will prove instrumental to his ability to pull off the 2013 blow of the whistle. Throughout the time in Japan, Ed also had to make several reports on technological capabilities of US IC adversaries, such as China. Then he realized that &amp;quot;there was simply no way for America to have so much much information about what the Chinese were doing without having done some of the very same things itself&amp;quot;, and reading the report he could sense himself &amp;quot;looking at a mirror and seeing a reflection of America. What China was doing publicly to its own citizens, America might be - could be - doing secretly to the world.&amp;quot; First, Snowden did his best to ignore it, but then his curiosity took over.&lt;/p&gt;
&lt;p&gt;Several whistleblowers had disclosed that after 9/11 Bush had authorized warantless wiretapping as part of President's Surveillance Program (PSP), which later was deeper ingrained into law. Later, an Unclassified Report on the PSP was released, which was very non-descriptive. Ed tried to look for the classified version of the report, which he could not find. There was no record of it. Only much later would he find the report, filed in Exceptionally Controlled Information (ECI) compartment, which was an &amp;quot;extremely rare classification meant to be hidden even from those holding top secret clearence&amp;quot;. The designation of the document's classification was very strict - &amp;quot;pretty much only a few dozen people in the world are allowed to read this&amp;quot;. Ed had access to the report through mistake - a draft copy left on one of the lower security systems, that should not have been left there, and as a sysadmin he had a chance to investigate it. And what he found, was shocking. He found &amp;quot;a complete recording of the NSA's most secret surveillance programs, and the agency directives and Department of Justice policies that had been used to subvert American law and contravene the US Constitution&amp;quot;. The most profound thing was that the classified version of the report exposed that the publicly available unclassified version was a fabricated lie - the only common thing between the reports was their title. Ed found out that NSA felt it necessary to collect every passing communication and had reinterpreted the law to enable bulk collection of the world's data without warrants, that NSA's mission had been transformed from defence of America to its control, &amp;quot;by redefining citizens' private communications as potential signals of intelligence&amp;quot;. This information, I think, was the key event that defined Snowden's life. He could never fall asleep without letting the world know about it.&lt;/p&gt;
&lt;h2&gt;Exposition&lt;/h2&gt;
&lt;p&gt;The details of how and when he was going to do it are not exactly important in understanding the key idea of the book, but they are extremely fascinating nonetheless. Ed was in a very unique position of having expansive access to key pieces of the infrastructure, having climbed up the ranks at an extremely young age. He had developed systems, such as automated readboard platform that could be used to collect important documents of the entire IC, called Heartbeat, as well as using other NSA systems to exfiltrate them. For instance, system administrators were so used to Heartbeat pulling many documents that he was able to use its beating pattern to pull in more sensitive documents in the process. Ed was working in a heavily fortified NSA site in Hawaii, and smuggled the documents out through SD cards. He wrote his own encryption schemes, and kept everything under wraps. He was visibly distressed his entire time, but never told anything to his girlfriend Lindslay (yes, he was chad enough to keep up a relationship this entire time). As one of my friends said after hearing this - based. In fact, we later find out that Lindslay assumed an affair after Ed went to break the world in Hong Kong. Snowden would go wardriving and communicate to various journalists through Tor and open hotspots. He was careful to always cover his tracks, leave no traces behind, and be extremely smart in his processes. He did not choose the journalists to contact, instead, he used the information of IC to know the most truth-seeking ones out there. The journalists that would not be silenced with a large sum of money or tad bit of intimidation, and it paid off really well.&lt;/p&gt;
&lt;p&gt;After extracting final documents he would fly to Hong Kong through Tokyo, paying for everything in cash (to be fair, I am not sure that was helpful given air transport is by far the most thoroughly logged system out there). Everything was deliberate. The location was picked specifically, because it was not under US influence, but was free enough so that his story would not be immediately discredited, which is a point we will get back to in a bit. In there, he waited for days before the journalists Glenn, Ewen and Laura showed up. And then, he told the story. This is where his views and what was reported slightly diverged from one another. Ed was focused on the system, he wanted to expose the system that enabled this to happen, journalists focused on the details, on which data was collected, who exactly was to blame. But these divergences were still compatible with each other. What Ed has extracted out, made its way to the world. Then, he needed a way out.&lt;/p&gt;
&lt;h2&gt;Way Out&lt;/h2&gt;
&lt;p&gt;He had no plan, and the propaganda had kicked in. All the little details picked on by the state were amplified. &amp;quot;All of those people, whether they faced prison or not, encountered some sort of backlash, most often severe and derived from the very abuse that I'd just helped expose: surveillance. If ever they'd expressed anger in private communication, they were &amp;quot;disgruntled.&amp;quot; If they'd ever visited a psychiatrist or a psychologist, or just checked out books on related subjects from a library, they were &amp;quot;mentally unsound.&amp;quot; If they'd been drunk even once, they were said to be alcoholics. If they'd had even one extramarital affair, they were said to be sexual deviants. [...] for the IC, it's just a matter of consulting the files, amplifying the available evidence, and, where no evidence exists, simply fabricating it.&amp;quot; Ed was very careful to minimize every chance of backlash, but he still could not avoid it. Chaos erupted, and he had nowhere to go. Thankfully, there were people ready to help. Several attorneys helped, such as Robert Tibbo and Jonathan Man. An entire team looked where to go. Ecuador was a great candidate, which is where he wanted to travel. There was one catch, however - flying through any airspace that was friendly to the United States was risky as the aircraft could be diverted and searched. For instance, after Ed got to Morcow US government suspected that Ed was traveling on the plane of the Bolivia's preident, diverted it to Vienna airport, searched it, and only allowed to continue once no trace of him were found. And yes, Ed chose to fly through Russia.&lt;/p&gt;
&lt;p&gt;Sarah Harrison was the last person to help Ed. She's a journalist and editor for WikiLeaks, a publishing organization aimed solely at revealing the darkest classified activities of governments around the world. Ed was vary for Sarah's motives, as he thought of Julian Assange, founder of WikiLeaks, as a very selfish individual who would do anything to win a &amp;quot;historic battle for the public's right to know&amp;quot;. However, it seems like Sarah's motive was a more personal and she was one of few people to openly disagree with Assange. Sarah respected Ed and wanted him to have a better outcome than Chelsea Manning - previous WikiLeaks most famous source who had to unjustly serve nearly 7 years of prison. Sarah was with Ed throughout the flight to Moscow where Ed's passport was repealed. Ed was stuck in Moscow airport, with nowhere to go, and almost immediately FSB (Russian intelligence agency) was talking with him, trying to make a deal. See, Ed was, and still is, a diehard patriot, contrary to what what some articles want you to believe. He knew that even going into brief talks with the FSB could be used to discredit him, thus he immediately shut them down. They warned him to not make a terrible mistake, but he was set. Eventually, Ed was granted political asylum in Moscow, which was to be used to discredit his acts.&lt;/p&gt;
&lt;p&gt;There is a simple thing to realize - were Ed to fly back to the US, he would never stand a chance at a fair trial. IC has broken the legal system apart, any accusations get blocked by the magical phrase of &amp;quot;it's classified&amp;quot;, and should he go back there, he would be trialed in a private court - there is no room for justice there, everyone works for the IC. Should the US want to try him justly, they would be able to do the trial right here and now. Sure, they would have no power to actually imprison him, but why not get ahead of the curve? Why not sentence him? And then, why not sentence him again for evasion of sentences? Or maybe, America does not want to try him publicly, because that would uncover some dark truths about wrongness and unconstitutionality of the Intelligence Community? That's up to you to decide.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;All in all, this was an incredible read. Ed went into great detail about his personal life, as well as the technicalities of the system, as well the processes he went through in exposing it. He exposed the dangers of how media is used to misrepresent the facts, and today, it is one of the most important things we should take note of when reading news articles. If you are interested in the technical details, and Ed's careful thought process, I would highly recommend this book. And if you are about to expose your government for something terrible, this is a must-read!&lt;/p&gt;
</summary><category term="Book Club"></category></entry><entry><title>Getting Your Stuff Together #1</title><link href="https://blaz.is/blog/post/getting-your-stuff-together-1" rel="alternate"></link><published>2022-06-11T09:12:21+00:00</published><updated>2022-06-11T09:12:21+00:00</updated><author><name>auri</name></author><id>https://blaz.is/blog/post/getting-your-stuff-together-1</id><summary type="html">&lt;p&gt;If you've been following me, you probably noticed I have gone silent. I wish I was able to say it was a massive project that is done now and I'm ready to share it with the world, no, I simply didn't do anything. Until April I had my dissertation, and after that, I've been simply not living up to my standards. It's fun and all to float around, but one day you will have to pick yourself up and do everything you have put off, which for me, is far too many things. Why don't we simply start today?&lt;/p&gt;
&lt;h2&gt;Wall of Shame&lt;/h2&gt;
&lt;p&gt;Public humiliation is fun right? Let me start with everything I publicly promised to do, but did not follow through with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Several YouTube videos (blackjack math, the secret delicate video, more RISC-V, OS making, robotics vlog, more memflow, binary exploitation).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;memflow 0.2.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Book club (I started reading Permanent Record, but never finished it).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition, over the months there were several less public things I was enthusiastic about doing, signed up doing, but when it came to actually doing, I would not deliver. Charles Hoskinson said that to be a leader what you need to do is &amp;quot;Tell what you're going to do, do what you said you'd do, and then tell what you did&amp;quot;, I'm awfully great at the first part, am I not? The last part would come naturally I think, but the middle, is what I need to focus on. In other words - execution.&lt;/p&gt;
&lt;h2&gt;Rock Solid Base&lt;/h2&gt;
&lt;p&gt;All these things I set out to do are meant for the world, but in order to serve the world I think I first need to have my day-to-day in order - have routines that help me stay on top of things and prevent me from going mentally insane. I need it to be balanced, and I think I know the perfect person to show this for me.&lt;/p&gt;
&lt;p&gt;Meet 2019 Auri. He's amazing - he is about to have his high school finals (I suppose A levels), one of the most important things at the time that can potentially make or break his life. Yet, while everyone around is stressing over the finals, Auri does not. While everyone is about to fall asleep during the first few lessons, Auri is zippy. Auri seemingly does not spend much time preparing for the exams, but he's about to be the first in school's history to max out 3 out of his 5 exams. What's his secret? His day to day is rock solid.&lt;/p&gt;
&lt;p&gt;Auri knows that everything starts with sleep, it's sacred. When it comes to any life decision where one option is keeping sleep, and the other is something incredibly attractive, sleep always wins. The mind needs to rest before the storm. Then, Auri knows that physical exercise is this seemingly magical thing that clears out brain fog, which is actually not magical, but a scientifically proven activity that helps with blood flow and feeds the brain with more oxygen. Knowing cardio is the most beneficial for the brain, he maxes it out with a 3-5 kilometer morning run. Immediately afterwards, he jumps into an ice cold shower to get the final shock needed to banish any sleep left away. As a finishing touch, Auri sits himself down to meditate, for about 10 minutes, because he knows that training attention will make him sharper over the long run. That's his morning routine. When it comes to evening, Auri journals so that he can make sense of everything that has happened throughout the day, make plans for the future and keep his mental state in check. And this is what he does every single day.&lt;/p&gt;
&lt;p&gt;That is quite a lot, yes. I want to go back into it. But how am I going to do it? And first of all, why did I drop this routine in the first place? That is thanks to the University - all the social activities have been too tempting and that's how sleep suffered. To be fair, my exercise routine immediately stopped, because I did not know where to run. I was left with cold showers, meditation, and journaling. I did not find meditation too useful and all the other things got me to gradually drop it off, while cold showers became a little too uncomfortable for my liking. That leaves us with journaling, which I slowly drifted away from given all the insane things that were happening around that time. And I can't say I couldn't execute afterwards, but my consistency dropped, and the only reason I could execute things was because I did not have many things to do. Okay, so how do we go back? Well, why not in order! Let's go through each thing one by one.&lt;/p&gt;
&lt;h3&gt;Sleep&lt;/h3&gt;
&lt;p&gt;This one is key, non-negotiable. 7-8 hours, which means give yourself 8, feel free to sleep less, but always give yourself 8. And this is done not by going to sleep at consistent time, but rather waking up at consistent time (and then going to sleep such that you have enough time). Your sleep cycle should eventually adjust itself to make you want sleep at the right time, and then naturally wake up at the time you're supposed to wake up. After a while it becomes like magic. Another key thing is that - when the alarm hits, do not, I repeat, do not go back to bed - you'd be sabotaging yourself, you will want to go back to sleep, and you will hate your every morning. Just wake up, stand up, and do your things. You can always leave the sleepy sleepy time in the evening. I personally attempt at doing this by engineering my environment - my phone is on the other end of the room, meaning, I need to stand up before I can turn it off, and then it becomes much easier to just not go back to bed.&lt;/p&gt;
&lt;p&gt;Now, last year I've done some sleep experiments. I once read up that humans used to split the sleep up into 2 parts. This sounds insane, but apparently it changed with industrialization when factories were expensive to power on, and it became cheaper to force people into working 8 hour shifts. And thus, it became our day to day to sleep once for 8 hours. But let me emphasize this fact - this is not an inherently natural state of reality that humans stuck with for hundreds of thousands of years, this is a relatively recent development done to maximize industrial output in jobs that require close to no mental effort. Personally, I noticed that my productivity plummets around 5 PM and the best thing I can do is watch YouTube in default mode network, and when it comes to programming I get productive well into the night. So why not try eliminating that evening period? Thus, I went to sleep twice - 4-8 AM and PM, and you know what, it was glorious. Even without the rest of the routine I was much more productive. Mainly, because it actually becomes easier to keep this schedule - not many social activities happen between 4-8 (only the practical ones do in the evening, and I used to not do any), and I can go partying without sacrificing sleep. This was so good, that I remember one night getting myself drunk, going to a night club, partying until 3 AM, coming back home sober and then spending one more hour coding CGlue before going to bed. I went out of this schedule mainly because I started going to dance practices that would lie around 6 PM, which was less than ideal. Sleep cycle is 1.5-2 hours, so in theory just 2 hours of sleep for one session should still be beneficial. I believe I might want to experiment with this, perhaps doing a 2/6 split instead of 4/4? What I know is that sleeping twice a day was beneficial to me, and what is key here is balancing out all the social activities with healthy amount of sleep.&lt;/p&gt;
&lt;h3&gt;Exercise&lt;/h3&gt;
&lt;p&gt;This one is pretty self explanatory - just do stuff that makes your heart beat quicker. Okay, probably there are way more important aspects to it, but I am no sports scientist. So take what I say with a pinch of salt, but the knowledge that is widely spread by scientists is that there is a direct link between cardio exercise and mental capacity increasing significantly for several hours after exercise. Jogging/running is not for everyone, although I believe people say it is not for them, because they tried it once and hated it. I sure as hell hated 1 km runs in school PE classes. I sure as hell hated the first 2 weeks going for at least 2 km every day. Both the fact that my legs hurt, but also during the run, it was exhausting, very, very, exhausting. Borderline suffering. No, actually suffering. In hindsight, it was probably not the brightest idea to do runs every day, because your body can simply give after too much physical stress, but it eventually got easier, my legs adapted, and I could get myself to the point of slowly increasing the distance to 5 km. What kept me going is this thing I heard that runners who force themselves through the suffering stage reach this &amp;quot;second breath&amp;quot; thing (I don't know what the term is in English) where it suddenly becomes easier to run and they can last longer. Which made me think that just because I'm suffering, doesn't mean I'm on the edge of collapsing, instead, I'm on the edge of switching gears. I never actually did that, but my first gear became more robust. Although, writing this I am wondering if that is a real thing or not. Heh, it served me well. So basically, running in the morning good. I will try doing it consistently.&lt;/p&gt;
&lt;p&gt;As for how, I used to engineer my environment by putting running clothes on top of other clothes in the evening, which helps remembering that I need to do it, and takes one step of action away from me in the morning.&lt;/p&gt;
&lt;h3&gt;Cold Showers&lt;/h3&gt;
&lt;p&gt;This is actually one the easiest uncomfortable thing to get yourself into. I started doing it before running. The reason this is easy is because it is passive! All you need to do is flick shower to the coldest side possible, and from a few moments to a few seconds the cold hits. It gives a shock, you want to leave, but try to keep yourself there, and move around to cover all sides of your body. In 2019 I would count to 60 seconds. Just like running you eventually get used to it. It makes you much sharper, and in addition, it actually helps with your skin. I would not use any hand creams or anything (being dumb), but cold showers made my skin softer and all the dry parts went away. From what I could gather (and after discussion with my skin expert friend (tm)) it is a very complex topic, but for dry skin hot showers may lead to dryer skin. Mind you, cold water may cause problems if your skin is sensitive and generally, lukewarm is the best, but cold showers were incredibly useful for me psychologically, and should your skin be fine with them I do recommend it out!&lt;/p&gt;
&lt;p&gt;Combining it with running is actually pretty marvelous. Getting that shock gives you a bit of a high, and it is a really good feeling when leaving the shower. 10/10 would recommend.&lt;/p&gt;
&lt;h3&gt;Journaling&lt;/h3&gt;
&lt;p&gt;This one is a pre-bed activity. I think it is way more beneficial to meditation. Brendon Burchard in &amp;quot;High Performance Habits&amp;quot; first introduces clarity as a habit (key thing in HPH is that the habits there are deliberate and conscious, not the typical &amp;quot;Oh let's get coffee in the morning&amp;quot; automatic habits). It is all about understanding what you've done and what you need to do. Journaling helps you keep track of things. Every night I would write general thoughts about the day, raise some questions, discuss immediate plans, and then fill in several check-up prompts that I took from the HPH book. And it is pretty difficult to put a finger on it, but whenever you have some ambiguities, it becomes easier to solve them, it becomes easier to track problems and overall it becomes easier to stay on top of things. Engineering your environment is a common theme, and here it is no different. If you write physically, keep the journal around visibly, and if you do it digitally, make sure it takes the least time to set up a new entry - have a template ready and make it so a single action gets you to writing.&lt;/p&gt;
&lt;h3&gt;Meditation&lt;/h3&gt;
&lt;p&gt;Not going to lie, this is more of a voodoo thing, and I could not attribute any benefits directly to meditation. Maybe, it is because I never really stuck with it religiously, which says a lot about my attention span. But since it is difficult in the first place, I think it means I have challenges to overcome. Especially given social media exploiting our psychology to keep us scrolling in the default mode, being alone with your thoughts becomes difficult, because there is always that temptation to do something (not healthy). Meditation would be a reminder of that, and I want to stick with it, but it probably won't be of highest priority.&lt;/p&gt;
&lt;p&gt;Overall, I want to incorporate all these mechanisms into my routine. However, it is key to realize that it is better to build things up slowly rather than be overly enthusiastic and unrealistic. I shall try starting out with sleep, journaling, exercise, cold showers, and only then meditation. I think it should be relatively easy to get back to the first 4, because I have done them in the past. I know I can stick with that. I know I can.&lt;/p&gt;
&lt;h2&gt;Consistency&lt;/h2&gt;
&lt;p&gt;The main problem I have is of consistency - I have far too many things to do, and no will to do any of them. Brendon Burchard said in his book &amp;quot;Humans are masterful jugglers. We can manage several projects at the same time, achieve many tasks concurrently, carry multiple levels of conversation - implicit and explicit - with several people at the dinner table. This strength serves all of us - up to a point. Then it destroys us.&amp;quot; What I noticed is that with all the things I want to do, I end up doing none. In addition to the promises I've made, I also have work, and I also have several other projects that are &lt;em&gt;hush hush&lt;/em&gt; for the time being. And yet, I'm doing nothing. Analysis paralysis? No, I don't even do analysis. Another thing he said was &amp;quot;Keep your main thing the main thing&amp;quot;. What is my main thing? Let's say YouTube is (well, it's not, but let's assume for the sake of it), then I would need to produce videos. And I need to go one by one. Simply, one, by, one. I need to make a plan, and stick to it, instead of scattering around like a firefly. That's my theory. And I will try it out. And while it is happening, I will have to say &amp;quot;no&amp;quot; to many things. Many, many things. My plate is already dripping from all sides.&lt;/p&gt;
&lt;h2&gt;Accountability&lt;/h2&gt;
&lt;p&gt;Back when I was preparing for the last Olympiad in informatics I was in, I found myself an accountability partner, for whom I gave 100 euros to safe-keep (it was a friend don't worry). If I didn't get through N practice challenges that week, he would get to keep the money (we then bent the rules to make it proportional, but whatever). While I couldn't get myself to do everything I set out to do, it did push me to get there. It was another environment engineering trick to get me to do what I consciously want to do, but have a hard time getting to do. In HPH, it is called raising necessity. I need some long-term accountability. Evidently, public one does not work on me - I have no real social status to lose, thus no fear. However, a friend of mine happens to be on a somewhat similar boat as me, thus we agreed to keep each other accountable. I do believe you can do everything on your own, but as social creatures, peer pressure is something that should make this journey a tad easier.&lt;/p&gt;
&lt;h2&gt;Final Thoughts&lt;/h2&gt;
&lt;p&gt;This post is more of a reminder to me of what I need to do to get where I want to be, but I also hope it may come in handy for you as well, wherever and whenever you are. I know I should build this all up one thing at a time, because it is important to realize that any initial motivation eventually burns out, and the more important thing is to keep on going instead of burning down with the loss of it. I'm not promising anything anymore, but should this go well, you should start seeing consistency in my content, and this deep hole should become a long and distant dream.&lt;/p&gt;
</summary><category term="Personal"></category></entry><entry><title>Reinventing memflow-cli</title><link href="https://blaz.is/blog/post/reinventing-memflow-cli" rel="alternate"></link><published>2022-01-28T22:55:42+00:00</published><updated>2022-01-28T22:55:42+00:00</updated><author><name>auri</name></author><id>https://blaz.is/blog/post/reinventing-memflow-cli</id><summary type="html">&lt;p&gt;Today I went for a casual walk around campus and for the next 2 hours started to think about memflow's CLI/daemon. We're essentially rewriting it for 0.2 with a much more flexible design. The question is how exactly the architecture is going to look like. Join me for some thoughts regarding that.&lt;/p&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;I am not going to talk much about the current design of the daemon - it's a hacked together mess with no sustainable road ahead. Instead, let's take a look at a reimagined architecture:&lt;/p&gt;
&lt;p&gt;&lt;img src="/media/memflow-diagram.drawio.png" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;This basically allows us to meet a number of important criteria:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Common structured interface for CLI/FUSE/Web/etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Scalable remote access with composition (we could also call the nodes as parachains for the lols).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;High performance FUSE interface with an option to run remotely.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Plugin system.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Allow user to load arbitrary plugins without compromising security of privileged daemon.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Basically this would serve as the ultimate &amp;quot;memflow framework&amp;quot; for applications and stuff alike.&lt;/p&gt;
&lt;h2&gt;Structure at its core&lt;/h2&gt;
&lt;p&gt;The diagram does not represent the relationships between various instances of connectors and OSs well enough, so let's take a look at some examples showing how it could look like.&lt;/p&gt;
&lt;h3&gt;1. Single daemon with FUSE&lt;/h3&gt;
&lt;p&gt;This is the simplest of cases - there is no interconnect and we basically have a single monolith executable handling everything. Inside the daemon we can create connectors and OSs. Say we have a Windows 7 coredump file with a QEMU windows 10 VM running. Performing nested instrospection we could have one (or both) of the 2 structures exposed by the daemon:&lt;/p&gt;
&lt;h5&gt;Candidate 1 - deeply nested structure&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;.
└── connector
    └── coredump
        └── win7-coredump
            └── qemu_win10
                └── win10-qemu
&lt;/code&gt;&lt;/pre&gt;
&lt;h5&gt;Candidate 2 - flattened structure&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;.
├── connector
│   ├── coredump
│   └── qemu_win10
└── os
    ├── win7-coredump
    └── win10-qemu
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I believe for simplicity (both from usability and manageability standpoints) option 2 makes way more sense. It may be harder to see connections this way, but we could also expose option 1 with a little bit of extra metadata and have best of both worlds.&lt;/p&gt;
&lt;p&gt;This is pretty simple, but what if we want to build an OS on a remote connector?&lt;/p&gt;
&lt;h3&gt;2. Two nodes&lt;/h3&gt;
&lt;p&gt;The problem when you have multiple nodes is possibility of duplicate names of connectors/OSs. One way around it would be to essentially introduce backend names into the mix and prefix remote connectors/OSs with the backend name. For instance, say on the backend we have the following structure:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;.
├── connector
│   ├── coredump
│   └── qemu_win10
└── os
    └── windows
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where &lt;code&gt;windows&lt;/code&gt; is an OS built on &lt;code&gt;coredump&lt;/code&gt; connector.&lt;/p&gt;
&lt;p&gt;Let's say we want to create a new &lt;code&gt;windows&lt;/code&gt; instance on the remote daemon's &lt;code&gt;qemu_win10&lt;/code&gt; connector. To avoid name duplication issues, we would perform some clever renaming and have the following structure:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;.
├── remote-daemon
│   ├── connector
│   │   ├── coredump
│   │   └── qemu_win10
│   └── os
│       └── windows
├── connector
│   ├── remote-daemon.coredump
│   └── remote-daemon.qemu_win10
└── os
    ├── remote-daemon.windows
    └── windows
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What's going on here? Why do we have duplication of connectors/OSs? Basically, we would love to allow user to work within the domain of a remote connection without involving local connection. Say you were to create a new OS on the local node, it would then exist only on it, but not the (parent) remote daemon. However, if we were to perform the operation on remote-daemon, it would then be available on both nodes, thus we would like to have an interface to deal with parent connections.&lt;/p&gt;
&lt;p&gt;So ultimately, we can identify an OS in the following way:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;backend1.backend2...backendN.os-name
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;NOTE: the following idea gets dropped later on due to atomicity reasons&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;# When performing any access operation, such as list or complete file read,
# we would need to traverse the structure. As you can imagine, it is very
# likely to be very slow to do with strings, thus let's introduce simple
# IDs for all connectors/OSs and backends. Thus, the core identification
# for an OS would look like this:

# backend1ID.backend2ID...backendNID.os-ID

# Where the original way would simply be an alias that gets translated
# locally on a high level operation (such as file open). In the end,
# tree traversal becomes incredibly simple - at every layer we just index
# into the ID and not have to perform any additional calculations. This
# starts to look awfully lot like an IP address or page table traversal,
# doesn't it?
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Plugins&lt;/h3&gt;
&lt;p&gt;The idea for plugins is that they would define an optional filtering function for all the places where they should be accessible in (for instance, LSASS monitoring would only need to work on lsass.exe processes) and then define read/write/rpc implementations that directly work on a filtered process or memory view. This way there would be as little boilerplate needed as possible and everything would stay structured.&lt;/p&gt;
&lt;p&gt;However, this is not the most abstract way to define a unit.&lt;/p&gt;
&lt;h3&gt;Directed Acyclic Graph&lt;/h3&gt;
&lt;p&gt;Boiling everything down, everything - all the directories to the backends, all the entries for OSs/connectors, all the processes within each OS and all the plugins that hook into different parts of the structure, is a node with edges to other nodes. Since it could be confusing to use the same term in 2 different contexts (framework nodes that get connect different daemons together), let's split this node up into 2 parts - a branch and a leaf. This way we can be sure we are in the clear that we are talking about internal structure of a single framework node.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;branch - does not allow for specific RW ops, but instead links up to other (branch/leaf) nodes. Equivalent to a directory in FS terms.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;leaf - does not direct itself to any other nodes, but instead provides actual functionality (RW or RPC). This is equivalent to a file in FS terms.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Whenever we traverse the structure to access anything from a physical memory file to a process minidump, we would end up using the following format:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;branch1.branch2...branchN.leaf
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;NOTE: same as previously&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;# Where each branch has an ID it is being accessed with and an alias. Same
# with the leafs. For simplicity in storage/traversal branches and leafs
# would be separated on each of the branches, thus could have duplicate
# IDs between the 2 node types.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You could say this forms a tree, which could be the simpler way about it, but if we have 2 different paths to the same branch (say os/remote-daemon.win10 and remote-daemon/os/win10), then it actually becomes a graph, specifically an acyclic one or a DAG.&lt;/p&gt;
&lt;h3&gt;State Tracking&lt;/h3&gt;
&lt;p&gt;The DAG gets traversed whenever something needs to be accessed for the first time. However, once the leaf is opened we do not want it from changing unpredictibly. Thus upon access a reference counted handle would be given. More often than not, multiple repeated accesses would yield the same leaf, but if something were to change, let's say a different process takes place, or the minidump becomes outdated, then the given handle becomes dangling on its own and further accesses to the same path would yield a different handle.&lt;/p&gt;
&lt;p&gt;Structurally each such handle could simply be another leaf that is in a somewhat flattened structure:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;backend1ID.backend2ID...backendNID.handles.handle
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;handles&lt;/code&gt; could be given a hardcoded value of 0, thus we would have:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;backend1ID.backend2ID...backendNID.0.handle
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another way about it would be a special handle traversal system. A handle may simply work as some sort of linked list - it would only contain the next backend and handle within. This way handles could be private information only shared with a single dependent node, path storage requirements become fixed and the traversal would look something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;local: handle -&amp;gt; backend1ID.handle1
backend1: handle1 -&amp;gt; backend2ID.handle2
...
backendN: handleN -&amp;gt; actual leaf
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Opening the handle directly naturally yields itself. Although, there may need to be a way to disable multiple accesses to the same handle in some cases. Whenever the handle has no references to it it will be removed completely and resources freed.&lt;/p&gt;
&lt;p&gt;There is an issue with reference counting across nodes, however. We must be able to handle cases whenever a node disconnects, the parent must know and subtract the right amount of from the count. This means the parent must keep track of how many references each downstream node has to each handle and this could end up rather messy.&lt;/p&gt;
&lt;h3&gt;DAG Syncing&lt;/h3&gt;
&lt;p&gt;Chances are entire DAG is not going to be synced - it would be too wasteful. DAG could be synced on-demand instead. Branch-by-branch. When we perform access from the user side we use actual names, not numerical IDs, because well, we don't know them! This needs to be worked out better, but I suppose we could either (open) access files by their names, or provide a name lookup. The problem with name lookup is that it's not really atomic, meaning sure, we get the IDs, but they could change inbetween calls! This is completely opposite from the handles - they will stay constant so long as they are open, thus we can easily use numeric IDs as we can rely on them staying constant.&lt;/p&gt;
&lt;p&gt;I guess a good middle ground would be to access backends by IDs only when a handle is being accessed, because then we can guarantee a backend will not change so long as the handle itself is valid (and the handle becomes invalid when the dependent backend gets disconnected). For anything else, first accesses by string values make most sense - including backend paths. This way we will ensure atomicity of the actions and performance whenever leafs are open.&lt;/p&gt;
&lt;h3&gt;Accessing OS/process/connector traits directly&lt;/h3&gt;
&lt;p&gt;This could actually be implemented rather easily as an RPC leaf. The difference between RW and RPC is that with RW you perform a single push/pull operation while RPC sends data and waits for a response. This could work rather nicely in async context whenever all the operations need to be forwarded across nodes. It's only at the 2 ends when we will need to perform potentially blocking operations. Naturally, some form of threading will be needed to allow for concurrency. The interface end (like FUSE) can implement threading anyhow they want - say new thread per file access or something like that, but on the backend side (specifically local backend that contains actual connector/OS objects) will require some pooling mechanism to allow for quick and concurrent access to objects - some sort of auto cloning mechanism, or a multithreaded queue.&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;All in all, this could certainly be an architecture that works. I could be missing something very obvious, however. And of course, things do change as implementation comes around. But this should serve as a really good starting point.&lt;/p&gt;
</summary><category term="memflow"></category><category term="Design Choices"></category></entry></feed>